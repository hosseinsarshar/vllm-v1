nohup: ignoring input
WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.
INFO 03-14 03:21:23 [__init__.py:256] Automatically detected platform tpu.
INFO 03-14 03:21:24 [api_server.py:912] vLLM API server version 0.7.4.dev339+ga21076ed.d20250312
INFO 03-14 03:21:24 [api_server.py:913] args: Namespace(subparser='serve', model_tag='meta-llama/Meta-Llama-3.1-8B', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='meta-llama/Meta-Llama-3.1-8B', task='auto', tokenizer=None, hf_config_path=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir='/dev/shm', load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=4096, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=None, swap_space=16.0, cpu_offload_gb=0, gpu_memory_utilization=0.5, num_gpu_blocks_override=None, max_num_batched_tokens=8192, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=128, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=True, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, use_tqdm_on_load=True, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', worker_extension_cls='', generation_config='auto', override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, enable_reasoning=False, reasoning_parser=None, disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function ServeSubcommand.cmd at 0x7bdfce7de5f0>)
INFO 03-14 03:21:30 [config.py:576] This model supports multiple tasks: {'classify', 'reward', 'generate', 'embed', 'score'}. Defaulting to 'generate'.
INFO 03-14 03:21:30 [config.py:1666] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 03-14 03:21:30 [tpu.py:76] [TPU] Forcing DYNAMO_ONCE compilation level
WARNING 03-14 03:21:30 [tpu.py:108] [V1][TPU] Disable prefix caching
INFO 03-14 03:21:35 [__init__.py:256] Automatically detected platform tpu.
INFO 03-14 03:21:35 [core.py:51] Initializing a V1 LLM engine (v0.7.4.dev339+ga21076ed.d20250312) with config: model='meta-llama/Meta-Llama-3.1-8B', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir='/dev/shm', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=None, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Meta-Llama-3.1-8B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":2,"backend":"openxla","splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
hosseins: init_worker() self.vllm_config.parallel_config.worker_cls='vllm.v1.worker.tpu_worker.TPUWorker'
hosseins: initialize_model_parallel() world_size=1
hosseins: initialize_model_parallel() all_ranks=tensor([[[0]]])
INFO 03-14 03:21:35 [parallel_state.py:950] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
WARNING 03-14 03:21:54 [tpu.py:116] Pin memory is not supported on TPU.
hosseins: get_model_loader() load_config.load_format=<LoadFormat.AUTO: 'auto'>
INFO 03-14 03:21:54 [tpu.py:39] Cannot use None backend on TPU.
INFO 03-14 03:21:54 [tpu.py:42] Using Pallas V1 backend.
WARNING 03-14 03:21:54 [topk_topp_sampler.py:46] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
INFO 03-14 03:21:55 [weight_utils.py:257] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  8.23it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:00<00:00,  5.60it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:00<00:00,  4.82it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:00<00:00,  5.28it/s]

INFO 03-14 03:21:56 [loader.py:429] Loading weights took 0.90 seconds
hosseins: block_size=16
hosseins: layer_name='model.layers.0.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.1.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.2.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.3.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.4.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.5.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.6.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.7.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.8.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.9.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.10.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.11.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.12.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.13.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.14.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.15.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.16.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.17.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.18.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.19.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.20.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.21.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.22.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.23.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.24.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.25.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.26.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.27.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.28.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.29.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.30.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.31.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: _initialize_kv_caches() self.model_executor=<vllm.v1.executor.abstract.UniProcExecutor object at 0x797b677dc160>
hosseins: _initialize_kv_caches() kv_cache_specs=[{'model.layers.0.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.1.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.2.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.3.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.4.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.5.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.6.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.7.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.8.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.9.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.10.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.11.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.12.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.13.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.14.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.15.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.16.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.17.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.18.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.19.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.20.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.21.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.22.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.23.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.24.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.25.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.26.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.27.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.28.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.29.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.30.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.31.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False)}]
hosseins: block_size=16
hosseins: layer_name='model.layers.0.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.1.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.2.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.3.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.4.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.5.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.6.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.7.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.8.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.9.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.10.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.11.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.12.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.13.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.14.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.15.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.16.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.17.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.18.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.19.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.20.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.21.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.22.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.23.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.24.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.25.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.26.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.27.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.28.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.29.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.30.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: layer_name='model.layers.31.self_attn.attn'
hosseins: block_size=16
hosseins: attn_module.num_kv_heads=8
hosseins: attn_module.head_size=128
hosseins: attn_module.dtype=torch.bfloat16
hosseins: ModelWrapperV1.forward()
hosseins: LlamaForCasualLM.forward()
hosseins: LlamaModel.forward()
hosseins: VocabParallelEmbedding.forward()
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([8192, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([8192, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([8192, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([8192, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([8192, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([8192, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([8192, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([8192, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([8192, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([8192, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([8192, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([8192, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([8192, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([8192, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([8192, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([8192, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([8192, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([8192, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([8192, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([8192, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([8192, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([8192, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([8192, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([8192, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([8192, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([8192, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([8192, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([8192, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([8192, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([8192, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([8192, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([8192, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([8192, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([8192, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([8192, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([8192, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([8192, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([8192, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([8192, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([8192, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([8192, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([8192, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([8192, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([8192, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([8192, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([8192, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([8192, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([8192, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([8192, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([8192, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([8192, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([8192, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([8192, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([8192, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([8192, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([8192, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([8192, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([8192, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([8192, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([8192, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([8192, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([8192, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([8192, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([8192, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([8192, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: _initialize_kv_caches() available_gpu_memory=16775087616
hosseins: get_kv_cache_configs() num_layers=32
hosseins: get_kv_cache_configs() len(kv_cache_specs)=1
hosseins: get_kv_cache_configs() kv_cache_specs=[{'model.layers.0.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.1.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.2.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.3.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.4.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.5.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.6.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.7.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.8.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.9.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.10.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.11.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.12.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.13.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.14.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.15.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.16.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.17.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.18.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.19.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.20.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.21.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.22.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.23.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.24.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.25.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.26.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.27.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.28.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.29.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.30.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.31.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False)}]
hosseins: page_sizes={65536}
hosseins: page_size=65536
hosseins: available_memory=16775087616
hosseins: num_blocks=7998
hosseins: num_blocks=7998
hosseins: vllm_config.cache_config.num_gpu_blocks_override=None
hosseins: vllm_config.cache_config.block_size=16
hosseins: num_tokens=127968
INFO 03-14 03:21:56 [kv_cache_utils.py:545] GPU KV cache size: 127,968 tokens
INFO 03-14 03:21:56 [kv_cache_utils.py:548] Maximum concurrency for 4,096 tokens per request: 31.24x
hosseins: _initialize_kv_caches() kv_cache_configs=[KVCacheConfig(num_blocks=7998, tensors={'model.layers.0.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.1.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.2.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.3.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.4.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.5.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.6.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.7.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.8.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.9.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.10.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.11.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.12.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.13.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.14.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.15.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.16.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.17.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.18.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.19.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.20.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.21.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.22.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.23.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.24.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.25.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.26.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.27.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.28.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.29.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.30.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.31.self_attn.attn': KVCacheTensor(size=524156928)}, groups=[['model.layers.0.self_attn.attn', 'model.layers.1.self_attn.attn', 'model.layers.2.self_attn.attn', 'model.layers.3.self_attn.attn', 'model.layers.4.self_attn.attn', 'model.layers.5.self_attn.attn', 'model.layers.6.self_attn.attn', 'model.layers.7.self_attn.attn', 'model.layers.8.self_attn.attn', 'model.layers.9.self_attn.attn', 'model.layers.10.self_attn.attn', 'model.layers.11.self_attn.attn', 'model.layers.12.self_attn.attn', 'model.layers.13.self_attn.attn', 'model.layers.14.self_attn.attn', 'model.layers.15.self_attn.attn', 'model.layers.16.self_attn.attn', 'model.layers.17.self_attn.attn', 'model.layers.18.self_attn.attn', 'model.layers.19.self_attn.attn', 'model.layers.20.self_attn.attn', 'model.layers.21.self_attn.attn', 'model.layers.22.self_attn.attn', 'model.layers.23.self_attn.attn', 'model.layers.24.self_attn.attn', 'model.layers.25.self_attn.attn', 'model.layers.26.self_attn.attn', 'model.layers.27.self_attn.attn', 'model.layers.28.self_attn.attn', 'model.layers.29.self_attn.attn', 'model.layers.30.self_attn.attn', 'model.layers.31.self_attn.attn']], kv_cache_spec={'model.layers.0.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.1.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.2.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.3.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.4.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.5.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.6.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.7.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.8.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.9.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.10.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.11.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.12.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.13.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.14.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.15.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.16.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.17.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.18.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.19.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.20.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.21.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.22.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.23.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.24.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.25.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.26.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.27.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.28.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.29.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.30.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.31.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False)})]
hosseins: _initialize_kv_caches() num_gpu_blocks_set={7998}
hosseins: _initialize_kv_caches() num_gpu_blocks=7998
hosseins: initialize_from_config() kv_cache_config=KVCacheConfig(num_blocks=7998, tensors={'model.layers.0.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.1.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.2.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.3.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.4.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.5.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.6.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.7.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.8.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.9.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.10.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.11.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.12.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.13.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.14.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.15.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.16.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.17.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.18.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.19.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.20.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.21.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.22.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.23.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.24.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.25.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.26.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.27.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.28.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.29.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.30.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.31.self_attn.attn': KVCacheTensor(size=524156928)}, groups=[['model.layers.0.self_attn.attn', 'model.layers.1.self_attn.attn', 'model.layers.2.self_attn.attn', 'model.layers.3.self_attn.attn', 'model.layers.4.self_attn.attn', 'model.layers.5.self_attn.attn', 'model.layers.6.self_attn.attn', 'model.layers.7.self_attn.attn', 'model.layers.8.self_attn.attn', 'model.layers.9.self_attn.attn', 'model.layers.10.self_attn.attn', 'model.layers.11.self_attn.attn', 'model.layers.12.self_attn.attn', 'model.layers.13.self_attn.attn', 'model.layers.14.self_attn.attn', 'model.layers.15.self_attn.attn', 'model.layers.16.self_attn.attn', 'model.layers.17.self_attn.attn', 'model.layers.18.self_attn.attn', 'model.layers.19.self_attn.attn', 'model.layers.20.self_attn.attn', 'model.layers.21.self_attn.attn', 'model.layers.22.self_attn.attn', 'model.layers.23.self_attn.attn', 'model.layers.24.self_attn.attn', 'model.layers.25.self_attn.attn', 'model.layers.26.self_attn.attn', 'model.layers.27.self_attn.attn', 'model.layers.28.self_attn.attn', 'model.layers.29.self_attn.attn', 'model.layers.30.self_attn.attn', 'model.layers.31.self_attn.attn']], kv_cache_spec={'model.layers.0.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.1.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.2.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.3.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.4.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.5.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.6.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.7.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.8.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.9.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.10.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.11.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.12.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.13.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.14.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.15.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.16.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.17.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.18.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.19.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.20.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.21.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.22.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.23.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.24.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.25.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.26.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.27.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.28.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.29.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.30.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.31.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False)})
hosseins: initialize_kv_cache() kv_cache_config=KVCacheConfig(num_blocks=7998, tensors={'model.layers.0.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.1.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.2.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.3.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.4.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.5.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.6.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.7.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.8.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.9.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.10.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.11.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.12.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.13.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.14.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.15.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.16.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.17.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.18.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.19.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.20.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.21.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.22.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.23.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.24.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.25.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.26.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.27.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.28.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.29.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.30.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.31.self_attn.attn': KVCacheTensor(size=524156928)}, groups=[['model.layers.0.self_attn.attn', 'model.layers.1.self_attn.attn', 'model.layers.2.self_attn.attn', 'model.layers.3.self_attn.attn', 'model.layers.4.self_attn.attn', 'model.layers.5.self_attn.attn', 'model.layers.6.self_attn.attn', 'model.layers.7.self_attn.attn', 'model.layers.8.self_attn.attn', 'model.layers.9.self_attn.attn', 'model.layers.10.self_attn.attn', 'model.layers.11.self_attn.attn', 'model.layers.12.self_attn.attn', 'model.layers.13.self_attn.attn', 'model.layers.14.self_attn.attn', 'model.layers.15.self_attn.attn', 'model.layers.16.self_attn.attn', 'model.layers.17.self_attn.attn', 'model.layers.18.self_attn.attn', 'model.layers.19.self_attn.attn', 'model.layers.20.self_attn.attn', 'model.layers.21.self_attn.attn', 'model.layers.22.self_attn.attn', 'model.layers.23.self_attn.attn', 'model.layers.24.self_attn.attn', 'model.layers.25.self_attn.attn', 'model.layers.26.self_attn.attn', 'model.layers.27.self_attn.attn', 'model.layers.28.self_attn.attn', 'model.layers.29.self_attn.attn', 'model.layers.30.self_attn.attn', 'model.layers.31.self_attn.attn']], kv_cache_spec={'model.layers.0.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.1.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.2.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.3.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.4.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.5.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.6.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.7.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.8.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.9.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.10.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.11.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.12.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.13.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.14.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.15.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.16.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.17.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.18.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.19.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.20.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.21.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.22.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.23.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.24.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.25.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.26.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.27.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.28.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.29.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.30.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.31.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False)})
hosseins: len(kv_cache_config.kv_cache_spec)=32
hosseins: kv_cache_config.num_blocks=7998
hosseins: len(kv_cache_config.tensors)=32
hosseins: len(kv_cache_config.groups)=1
hosseins: len(kv_cache_config.kv_cache_spec.items())=32
hosseins: ============================= model.layers.0.self_attn.attn =============================
hosseins: num_blocks=7998
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: tpu_k_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: get_shard_spec(tpu_v_cache)=None
hosseins: get_shard_spec(tpu_k_cache)=None
hosseins: ============================= model.layers.1.self_attn.attn =============================
hosseins: num_blocks=7998
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: tpu_k_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: get_shard_spec(tpu_v_cache)=None
hosseins: get_shard_spec(tpu_k_cache)=None
hosseins: ============================= model.layers.2.self_attn.attn =============================
hosseins: num_blocks=7998
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: tpu_k_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: get_shard_spec(tpu_v_cache)=None
hosseins: get_shard_spec(tpu_k_cache)=None
hosseins: ============================= model.layers.3.self_attn.attn =============================
hosseins: num_blocks=7998
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: tpu_k_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: get_shard_spec(tpu_v_cache)=None
hosseins: get_shard_spec(tpu_k_cache)=None
hosseins: ============================= model.layers.4.self_attn.attn =============================
hosseins: num_blocks=7998
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: tpu_k_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: get_shard_spec(tpu_v_cache)=None
hosseins: get_shard_spec(tpu_k_cache)=None
hosseins: ============================= model.layers.5.self_attn.attn =============================
hosseins: num_blocks=7998
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: tpu_k_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: get_shard_spec(tpu_v_cache)=None
hosseins: get_shard_spec(tpu_k_cache)=None
hosseins: ============================= model.layers.6.self_attn.attn =============================
hosseins: num_blocks=7998
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: tpu_k_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: get_shard_spec(tpu_v_cache)=None
hosseins: get_shard_spec(tpu_k_cache)=None
hosseins: ============================= model.layers.7.self_attn.attn =============================
hosseins: num_blocks=7998
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: tpu_k_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: get_shard_spec(tpu_v_cache)=None
hosseins: get_shard_spec(tpu_k_cache)=None
hosseins: ============================= model.layers.8.self_attn.attn =============================
hosseins: num_blocks=7998
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: tpu_k_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: get_shard_spec(tpu_v_cache)=None
hosseins: get_shard_spec(tpu_k_cache)=None
hosseins: ============================= model.layers.9.self_attn.attn =============================
hosseins: num_blocks=7998
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: tpu_k_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: get_shard_spec(tpu_v_cache)=None
hosseins: get_shard_spec(tpu_k_cache)=None
hosseins: ============================= model.layers.10.self_attn.attn =============================
hosseins: num_blocks=7998
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: tpu_k_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: get_shard_spec(tpu_v_cache)=None
hosseins: get_shard_spec(tpu_k_cache)=None
hosseins: ============================= model.layers.11.self_attn.attn =============================
hosseins: num_blocks=7998
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: tpu_k_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: get_shard_spec(tpu_v_cache)=None
hosseins: get_shard_spec(tpu_k_cache)=None
hosseins: ============================= model.layers.12.self_attn.attn =============================
hosseins: num_blocks=7998
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: tpu_k_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: get_shard_spec(tpu_v_cache)=None
hosseins: get_shard_spec(tpu_k_cache)=None
hosseins: ============================= model.layers.13.self_attn.attn =============================
hosseins: num_blocks=7998
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: tpu_k_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: get_shard_spec(tpu_v_cache)=None
hosseins: get_shard_spec(tpu_k_cache)=None
hosseins: ============================= model.layers.14.self_attn.attn =============================
hosseins: num_blocks=7998
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: tpu_k_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: get_shard_spec(tpu_v_cache)=None
hosseins: get_shard_spec(tpu_k_cache)=None
hosseins: ============================= model.layers.15.self_attn.attn =============================
hosseins: num_blocks=7998
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: tpu_k_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: get_shard_spec(tpu_v_cache)=None
hosseins: get_shard_spec(tpu_k_cache)=None
hosseins: ============================= model.layers.16.self_attn.attn =============================
hosseins: num_blocks=7998
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: tpu_k_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: get_shard_spec(tpu_v_cache)=None
hosseins: get_shard_spec(tpu_k_cache)=None
hosseins: ============================= model.layers.17.self_attn.attn =============================
hosseins: num_blocks=7998
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: tpu_k_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: get_shard_spec(tpu_v_cache)=None
hosseins: get_shard_spec(tpu_k_cache)=None
hosseins: ============================= model.layers.18.self_attn.attn =============================
hosseins: num_blocks=7998
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: tpu_k_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: get_shard_spec(tpu_v_cache)=None
hosseins: get_shard_spec(tpu_k_cache)=None
hosseins: ============================= model.layers.19.self_attn.attn =============================
hosseins: num_blocks=7998
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: tpu_k_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: get_shard_spec(tpu_v_cache)=None
hosseins: get_shard_spec(tpu_k_cache)=None
hosseins: ============================= model.layers.20.self_attn.attn =============================
hosseins: num_blocks=7998
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: tpu_k_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: get_shard_spec(tpu_v_cache)=None
hosseins: get_shard_spec(tpu_k_cache)=None
hosseins: ============================= model.layers.21.self_attn.attn =============================
hosseins: num_blocks=7998
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: tpu_k_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: get_shard_spec(tpu_v_cache)=None
hosseins: get_shard_spec(tpu_k_cache)=None
hosseins: ============================= model.layers.22.self_attn.attn =============================
hosseins: num_blocks=7998
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: tpu_k_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: get_shard_spec(tpu_v_cache)=None
hosseins: get_shard_spec(tpu_k_cache)=None
hosseins: ============================= model.layers.23.self_attn.attn =============================
hosseins: num_blocks=7998
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: tpu_k_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: get_shard_spec(tpu_v_cache)=None
hosseins: get_shard_spec(tpu_k_cache)=None
hosseins: ============================= model.layers.24.self_attn.attn =============================
hosseins: num_blocks=7998
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: tpu_k_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: get_shard_spec(tpu_v_cache)=None
hosseins: get_shard_spec(tpu_k_cache)=None
hosseins: ============================= model.layers.25.self_attn.attn =============================
hosseins: num_blocks=7998
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: tpu_k_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: get_shard_spec(tpu_v_cache)=None
hosseins: get_shard_spec(tpu_k_cache)=None
hosseins: ============================= model.layers.26.self_attn.attn =============================
hosseins: num_blocks=7998
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: tpu_k_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: get_shard_spec(tpu_v_cache)=None
hosseins: get_shard_spec(tpu_k_cache)=None
hosseins: ============================= model.layers.27.self_attn.attn =============================
hosseins: num_blocks=7998
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: tpu_k_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: get_shard_spec(tpu_v_cache)=None
hosseins: get_shard_spec(tpu_k_cache)=None
hosseins: ============================= model.layers.28.self_attn.attn =============================
hosseins: num_blocks=7998
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: tpu_k_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: get_shard_spec(tpu_v_cache)=None
hosseins: get_shard_spec(tpu_k_cache)=None
hosseins: ============================= model.layers.29.self_attn.attn =============================
hosseins: num_blocks=7998
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: tpu_k_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: get_shard_spec(tpu_v_cache)=None
hosseins: get_shard_spec(tpu_k_cache)=None
hosseins: ============================= model.layers.30.self_attn.attn =============================
hosseins: num_blocks=7998
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: tpu_k_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: get_shard_spec(tpu_v_cache)=None
hosseins: get_shard_spec(tpu_k_cache)=None
hosseins: ============================= model.layers.31.self_attn.attn =============================
hosseins: num_blocks=7998
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: tpu_k_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: get_shard_spec(tpu_v_cache)=None
hosseins: get_shard_spec(tpu_k_cache)=None
hosseins: initialize_kv_cache() kv_cache_config=KVCacheConfig(num_blocks=7998, tensors={'model.layers.0.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.1.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.2.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.3.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.4.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.5.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.6.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.7.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.8.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.9.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.10.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.11.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.12.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.13.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.14.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.15.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.16.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.17.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.18.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.19.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.20.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.21.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.22.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.23.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.24.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.25.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.26.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.27.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.28.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.29.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.30.self_attn.attn': KVCacheTensor(size=524156928), 'model.layers.31.self_attn.attn': KVCacheTensor(size=524156928)}, groups=[['model.layers.0.self_attn.attn', 'model.layers.1.self_attn.attn', 'model.layers.2.self_attn.attn', 'model.layers.3.self_attn.attn', 'model.layers.4.self_attn.attn', 'model.layers.5.self_attn.attn', 'model.layers.6.self_attn.attn', 'model.layers.7.self_attn.attn', 'model.layers.8.self_attn.attn', 'model.layers.9.self_attn.attn', 'model.layers.10.self_attn.attn', 'model.layers.11.self_attn.attn', 'model.layers.12.self_attn.attn', 'model.layers.13.self_attn.attn', 'model.layers.14.self_attn.attn', 'model.layers.15.self_attn.attn', 'model.layers.16.self_attn.attn', 'model.layers.17.self_attn.attn', 'model.layers.18.self_attn.attn', 'model.layers.19.self_attn.attn', 'model.layers.20.self_attn.attn', 'model.layers.21.self_attn.attn', 'model.layers.22.self_attn.attn', 'model.layers.23.self_attn.attn', 'model.layers.24.self_attn.attn', 'model.layers.25.self_attn.attn', 'model.layers.26.self_attn.attn', 'model.layers.27.self_attn.attn', 'model.layers.28.self_attn.attn', 'model.layers.29.self_attn.attn', 'model.layers.30.self_attn.attn', 'model.layers.31.self_attn.attn']], kv_cache_spec={'model.layers.0.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.1.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.2.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.3.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.4.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.5.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.6.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.7.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.8.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.9.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.10.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.11.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.12.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.13.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.14.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.15.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.16.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.17.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.18.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.19.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.20.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.21.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.22.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.23.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.24.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.25.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.26.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.27.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.28.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.29.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.30.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.31.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False)})
INFO 03-14 03:21:56 [core.py:128] init engine (profile, create kv cache, warmup model) took 0.06 seconds
INFO 03-14 03:21:57 [serving_chat.py:114] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
INFO 03-14 03:21:57 [serving_completion.py:60] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
INFO 03-14 03:21:57 [api_server.py:958] Starting vLLM API server on http://0.0.0.0:8000
INFO 03-14 03:21:57 [launcher.py:26] Available routes are:
INFO 03-14 03:21:57 [launcher.py:34] Route: /openapi.json, Methods: HEAD, GET
INFO 03-14 03:21:57 [launcher.py:34] Route: /docs, Methods: HEAD, GET
INFO 03-14 03:21:57 [launcher.py:34] Route: /docs/oauth2-redirect, Methods: HEAD, GET
INFO 03-14 03:21:57 [launcher.py:34] Route: /redoc, Methods: HEAD, GET
INFO 03-14 03:21:57 [launcher.py:34] Route: /health, Methods: GET
INFO 03-14 03:21:57 [launcher.py:34] Route: /ping, Methods: POST, GET
INFO 03-14 03:21:57 [launcher.py:34] Route: /tokenize, Methods: POST
INFO 03-14 03:21:57 [launcher.py:34] Route: /detokenize, Methods: POST
INFO 03-14 03:21:57 [launcher.py:34] Route: /v1/models, Methods: GET
INFO 03-14 03:21:57 [launcher.py:34] Route: /version, Methods: GET
INFO 03-14 03:21:57 [launcher.py:34] Route: /v1/chat/completions, Methods: POST
INFO 03-14 03:21:57 [launcher.py:34] Route: /v1/completions, Methods: POST
INFO 03-14 03:21:57 [launcher.py:34] Route: /v1/embeddings, Methods: POST
INFO 03-14 03:21:57 [launcher.py:34] Route: /pooling, Methods: POST
INFO 03-14 03:21:57 [launcher.py:34] Route: /score, Methods: POST
INFO 03-14 03:21:57 [launcher.py:34] Route: /v1/score, Methods: POST
INFO 03-14 03:21:57 [launcher.py:34] Route: /v1/audio/transcriptions, Methods: POST
INFO 03-14 03:21:57 [launcher.py:34] Route: /rerank, Methods: POST
INFO 03-14 03:21:57 [launcher.py:34] Route: /v1/rerank, Methods: POST
INFO 03-14 03:21:57 [launcher.py:34] Route: /v2/rerank, Methods: POST
INFO 03-14 03:21:57 [launcher.py:34] Route: /invocations, Methods: POST
INFO:     Started server process [969886]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO 03-14 03:22:07 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:22:17 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:22:27 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:22:37 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:22:47 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:22:57 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:23:07 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:23:17 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:23:27 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:23:37 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:23:47 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:23:57 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:24:07 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:24:17 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:24:27 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:24:37 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:24:47 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:24:57 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:25:07 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:25:17 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:25:27 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:25:37 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:25:47 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:25:57 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:26:07 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:26:17 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:26:27 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:26:37 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:26:47 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:26:57 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:27:07 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:27:17 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:27:27 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:27:37 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:27:47 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:27:57 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:28:07 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:28:17 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:28:27 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:28:37 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:28:47 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:28:57 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:29:07 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:29:17 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:29:27 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:29:37 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:29:47 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:29:57 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:30:07 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:30:17 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:30:27 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:30:37 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:30:47 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:30:57 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:31:07 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:31:17 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:31:27 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:31:37 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:31:47 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:31:57 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:32:07 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:32:17 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:32:27 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:32:37 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:32:47 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:32:57 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:33:07 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:33:17 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:33:27 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:33:37 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:33:47 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:33:57 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:34:07 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:34:17 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:34:27 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:34:37 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:34:47 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:34:57 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:35:07 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:35:17 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:35:27 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:35:37 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:35:47 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:35:57 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:36:07 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:36:17 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:36:27 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:36:37 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:36:47 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:36:57 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:37:07 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:37:17 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:37:27 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:37:37 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:37:47 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:37:57 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:38:07 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:38:17 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:38:27 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:38:37 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:38:47 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:38:57 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:39:07 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:39:17 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:39:27 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:39:37 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:39:47 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:39:57 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:40:07 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:40:17 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:40:27 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:40:37 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:40:47 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:40:57 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:41:07 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:41:17 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:41:27 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:41:37 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:41:47 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:41:57 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:42:07 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:42:17 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:42:27 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:42:37 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:42:47 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
/home/hosseins/miniconda3/envs/vllm-v1/lib/python3.10/site-packages/numpy/core/getlimits.py:549: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.
  setattr(self, word, getattr(machar, word).flat[0])
/home/hosseins/miniconda3/envs/vllm-v1/lib/python3.10/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.
  return self._float_to_str(self.smallest_subnormal)
hosseins: ModelWrapperV1.forward()
hosseins: LlamaForCasualLM.forward()
hosseins: LlamaModel.forward()
hosseins: VocabParallelEmbedding.forward()
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
INFO 03-14 03:42:57 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: ModelWrapperV1.forward()
hosseins: LlamaForCasualLM.forward()
hosseins: LlamaModel.forward()
hosseins: VocabParallelEmbedding.forward()
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: ModelWrapperV1.forward()
hosseins: LlamaForCasualLM.forward()
hosseins: LlamaModel.forward()
hosseins: VocabParallelEmbedding.forward()
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
INFO 03-14 03:43:07 [loggers.py:80] Avg prompt throughput: 0.5 tokens/s, Avg generation throughput: 0.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: ModelWrapperV1.forward()
hosseins: LlamaForCasualLM.forward()
hosseins: LlamaModel.forward()
hosseins: VocabParallelEmbedding.forward()
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
INFO 03-14 03:43:17 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%
INFO 03-14 03:43:27 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%
INFO 03-14 03:43:37 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%
INFO 03-14 03:43:47 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: ModelWrapperV1.forward()
hosseins: LlamaForCasualLM.forward()
hosseins: LlamaModel.forward()
hosseins: VocabParallelEmbedding.forward()
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
hosseins: PallasAttentionBackendImpl.forward()
hosseins: write_to_kv_cache() key.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() value.shape=torch.Size([16, 8, 128])
hosseins: write_to_kv_cache() key_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() value_cache.shape=torch.Size([7998, 16, 8, 128])
hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() output.shape=torch.Size([16, 4096])
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
hosseins: LlamaMLP.forward()
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 6144])
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
INFO 03-14 03:43:57 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%
INFO 03-14 03:44:07 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%
INFO 03-14 03:44:17 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%
INFO 03-14 03:44:27 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%
INFO:     127.0.0.1:48290 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 03-14 03:44:37 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:44:47 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:44:57 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:45:07 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:45:17 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:45:27 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-14 03:45:37 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
