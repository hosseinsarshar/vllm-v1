nohup: ignoring input
WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.
INFO 03-19 15:55:49 [__init__.py:256] Automatically detected platform tpu.
WARNING 03-19 15:55:50 [api_server.py:673] Torch Profiler is enabled in the API server. This should ONLY be used for local development!
INFO 03-19 15:55:50 [api_server.py:912] vLLM API server version 0.7.4.dev339+ga21076ed.d20250318
INFO 03-19 15:55:50 [api_server.py:913] args: Namespace(subparser='serve', model_tag='meta-llama/Meta-Llama-3.1-8B', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='meta-llama/Meta-Llama-3.1-8B', task='auto', tokenizer=None, hf_config_path=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir='/dev/shm', load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=512, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=None, swap_space=16.0, cpu_offload_gb=0, gpu_memory_utilization=0.5, num_gpu_blocks_override=None, max_num_batched_tokens=512, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=128, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=True, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, use_tqdm_on_load=True, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', worker_extension_cls='', generation_config='auto', override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, enable_reasoning=False, reasoning_parser=None, disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function ServeSubcommand.cmd at 0x7ef06a1dab90>)
INFO 03-19 15:55:56 [config.py:576] This model supports multiple tasks: {'reward', 'classify', 'score', 'embed', 'generate'}. Defaulting to 'generate'.
INFO 03-19 15:55:56 [config.py:1666] Chunked prefill is enabled with max_num_batched_tokens=512.
INFO 03-19 15:55:56 [tpu.py:76] [TPU] Forcing DYNAMO_ONCE compilation level
WARNING 03-19 15:55:56 [tpu.py:108] [V1][TPU] Disable prefix caching
INFO 03-19 15:56:00 [__init__.py:256] Automatically detected platform tpu.
INFO 03-19 15:56:01 [core.py:51] Initializing a V1 LLM engine (v0.7.4.dev339+ga21076ed.d20250318) with config: model='meta-llama/Meta-Llama-3.1-8B', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=512, download_dir='/dev/shm', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=None, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Meta-Llama-3.1-8B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":2,"backend":"openxla","splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
hosseins: init_worker() self.vllm_config.parallel_config.worker_cls='vllm.v1.worker.tpu_worker.TPUWorker'
INFO 03-19 15:56:19 [utils.py:364] Initializing SPMD engine with mesh=[{'device_ids': [0, 1, 2, 3], 'mesh_shape': (4,), 'axis_names': ('axis',)}]
hosseins: envs.VLLM_TORCH_PROFILER_DIR='/home/hosseins/prof-vllm-v1'
INFO 03-19 15:56:19 [tpu_worker.py:80] Profiling enabled. Traces will be saved to: /home/hosseins/prof-vllm-v1
Starting to trace for 1000000 ms. Remaining attempt(s): 2
2025-03-19 15:56:19.179134: W external/tsl/tsl/profiler/lib/profiler_session.cc:109] Profiling is late by 783050 nanoseconds and will start immediately.
hosseins: starting the profile at [/home/hosseins/prof-vllm-v1]
hosseins: initialize_model_parallel() world_size=1
hosseins: initialize_model_parallel() all_ranks=tensor([[[0]]])
INFO 03-19 15:56:19 [parallel_state.py:950] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
WARNING 03-19 15:56:19 [tpu.py:116] Pin memory is not supported on TPU.
hosseins: get_model_loader() load_config.load_format=<LoadFormat.AUTO: 'auto'>
hosseins: Attention. init()
INFO 03-19 15:56:19 [tpu.py:39] Cannot use None backend on TPU.
INFO 03-19 15:56:19 [tpu.py:42] Using Pallas V1 backend.
hosseins: Attention. init()
hosseins: Attention. init()
hosseins: Attention. init()
hosseins: Attention. init()
hosseins: Attention. init()
hosseins: Attention. init()
hosseins: Attention. init()
hosseins: Attention. init()
hosseins: Attention. init()
hosseins: Attention. init()
hosseins: Attention. init()
hosseins: Attention. init()
hosseins: Attention. init()
hosseins: Attention. init()
hosseins: Attention. init()
hosseins: Attention. init()
hosseins: Attention. init()
hosseins: Attention. init()
hosseins: Attention. init()
hosseins: Attention. init()
hosseins: Attention. init()
hosseins: Attention. init()
hosseins: Attention. init()
hosseins: Attention. init()
hosseins: Attention. init()
hosseins: Attention. init()
hosseins: Attention. init()
hosseins: Attention. init()
hosseins: Attention. init()
hosseins: Attention. init()
hosseins: Attention. init()
WARNING 03-19 15:56:19 [topk_topp_sampler.py:46] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
INFO 03-19 15:56:19 [weight_utils.py:258] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
hosseins: VocabParallelEmbedding -> weight_loader() [start_idx=0]
hosseins: VocabParallelEmbedding -> weight_loader() [shard_size=128256]
hosseins: DefaultModelLoader -> load_weights() name='layers.31.input_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.31.mlp.down_proj.weight'
INFO 03-19 15:56:20 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:20 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:20 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:20 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:20 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:20 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.57s/it]
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.31.post_attention_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='norm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.20.input_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.20.mlp.down_proj.weight'
INFO 03-19 15:56:21 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:21 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:21 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:21 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:21 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:21 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.20.mlp.up_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
INFO 03-19 15:56:21 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:21 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:21 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:21 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:21 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:21 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{replicated}'
hosseins: DefaultModelLoader -> load_weights() name='layers.20.post_attention_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.21.input_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.21.mlp.down_proj.weight'
INFO 03-19 15:56:21 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:21 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:21 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:21 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:21 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:21 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.21.mlp.gate_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
INFO 03-19 15:56:21 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:21 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:21 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:21 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:21 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:21 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{replicated}'
hosseins: DefaultModelLoader -> load_weights() name='layers.21.mlp.up_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)=''
INFO 03-19 15:56:21 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:21 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:21 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:21 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:21 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:21 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.21.post_attention_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.21.self_attn.k_proj.weight'
INFO 03-19 15:56:21 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:21 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:21 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:21 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:21 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:21 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.21.self_attn.o_proj.weight'
INFO 03-19 15:56:21 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:21 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:21 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:21 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:21 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:21 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.21.self_attn.q_proj.weight'
INFO 03-19 15:56:21 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:21 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:21 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:21 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:21 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:21 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.21.self_attn.v_proj.weight'
INFO 03-19 15:56:22 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:22 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:22 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:22 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.22.input_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.22.mlp.down_proj.weight'
INFO 03-19 15:56:22 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:22 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:22 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:22 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.22.mlp.gate_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
INFO 03-19 15:56:22 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:22 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:22 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:22 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{replicated}'
hosseins: DefaultModelLoader -> load_weights() name='layers.22.mlp.up_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)=''
INFO 03-19 15:56:22 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:22 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:22 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:22 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.22.post_attention_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.22.self_attn.k_proj.weight'
INFO 03-19 15:56:22 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:22 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:22 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:22 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.22.self_attn.o_proj.weight'
INFO 03-19 15:56:22 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:22 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:22 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:22 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.22.self_attn.q_proj.weight'
INFO 03-19 15:56:22 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:22 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:22 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:22 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.22.self_attn.v_proj.weight'
INFO 03-19 15:56:22 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:22 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:22 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:22 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.23.input_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.23.mlp.down_proj.weight'
INFO 03-19 15:56:22 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:22 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:22 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:22 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.23.mlp.gate_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
INFO 03-19 15:56:22 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:22 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:22 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:22 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{replicated}'
hosseins: DefaultModelLoader -> load_weights() name='layers.23.mlp.up_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)=''
INFO 03-19 15:56:22 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:22 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:22 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:22 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.23.post_attention_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.23.self_attn.k_proj.weight'
INFO 03-19 15:56:22 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:22 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:22 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:22 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.23.self_attn.o_proj.weight'
INFO 03-19 15:56:22 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:22 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:22 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:22 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.23.self_attn.q_proj.weight'
INFO 03-19 15:56:22 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:22 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:22 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:22 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.23.self_attn.v_proj.weight'
INFO 03-19 15:56:22 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:22 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:22 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:22 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.24.input_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.24.mlp.down_proj.weight'
INFO 03-19 15:56:22 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:22 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:22 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:22 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.24.mlp.gate_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
INFO 03-19 15:56:22 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:22 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:22 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:22 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{replicated}'
hosseins: DefaultModelLoader -> load_weights() name='layers.24.mlp.up_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)=''
INFO 03-19 15:56:22 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:22 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:22 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:22 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.24.post_attention_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.24.self_attn.k_proj.weight'
INFO 03-19 15:56:22 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:22 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:22 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:22 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.24.self_attn.o_proj.weight'
INFO 03-19 15:56:22 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:22 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:22 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:22 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.24.self_attn.q_proj.weight'
INFO 03-19 15:56:22 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:22 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:22 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:22 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.24.self_attn.v_proj.weight'
INFO 03-19 15:56:22 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:22 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:22 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:22 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.25.input_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.25.mlp.down_proj.weight'
INFO 03-19 15:56:22 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:22 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:22 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:22 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.25.mlp.gate_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
INFO 03-19 15:56:22 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:22 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:22 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:22 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{replicated}'
hosseins: DefaultModelLoader -> load_weights() name='layers.25.mlp.up_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)=''
INFO 03-19 15:56:22 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:22 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:22 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:22 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.25.post_attention_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.25.self_attn.k_proj.weight'
INFO 03-19 15:56:22 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:22 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:22 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:22 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.25.self_attn.o_proj.weight'
INFO 03-19 15:56:22 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:22 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:22 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:22 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.25.self_attn.q_proj.weight'
INFO 03-19 15:56:22 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:22 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:22 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:22 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.25.self_attn.v_proj.weight'
INFO 03-19 15:56:22 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:22 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:22 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:22 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.26.input_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.26.mlp.down_proj.weight'
INFO 03-19 15:56:22 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:22 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:22 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:22 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:22 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.26.mlp.gate_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
INFO 03-19 15:56:23 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:23 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:23 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:23 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{replicated}'
hosseins: DefaultModelLoader -> load_weights() name='layers.26.mlp.up_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)=''
INFO 03-19 15:56:23 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:23 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:23 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:23 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.26.post_attention_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.26.self_attn.k_proj.weight'
INFO 03-19 15:56:23 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:23 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:23 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:23 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.26.self_attn.o_proj.weight'
INFO 03-19 15:56:23 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:23 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:23 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:23 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.26.self_attn.q_proj.weight'
INFO 03-19 15:56:23 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:23 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:23 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:23 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.26.self_attn.v_proj.weight'
INFO 03-19 15:56:23 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:23 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:23 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:23 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.27.input_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.27.mlp.down_proj.weight'
INFO 03-19 15:56:23 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:23 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:23 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:23 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.27.mlp.gate_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
INFO 03-19 15:56:23 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:23 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:23 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:23 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{replicated}'
hosseins: DefaultModelLoader -> load_weights() name='layers.27.mlp.up_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)=''
INFO 03-19 15:56:23 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:23 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:23 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:23 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.27.post_attention_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.27.self_attn.k_proj.weight'
INFO 03-19 15:56:23 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:23 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:23 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:23 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.27.self_attn.o_proj.weight'
INFO 03-19 15:56:23 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:23 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:23 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:23 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.27.self_attn.q_proj.weight'
INFO 03-19 15:56:23 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:23 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:23 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:23 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.27.self_attn.v_proj.weight'
INFO 03-19 15:56:23 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:23 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:23 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:23 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.28.input_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.28.mlp.down_proj.weight'
INFO 03-19 15:56:23 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:23 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:23 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:23 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.28.mlp.gate_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
INFO 03-19 15:56:23 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:23 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:23 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:23 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{replicated}'
hosseins: DefaultModelLoader -> load_weights() name='layers.28.mlp.up_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)=''
INFO 03-19 15:56:23 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:23 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:23 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:23 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.28.post_attention_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.28.self_attn.k_proj.weight'
INFO 03-19 15:56:23 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:23 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:23 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:23 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.28.self_attn.o_proj.weight'
INFO 03-19 15:56:23 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:23 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:23 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:23 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.28.self_attn.q_proj.weight'
INFO 03-19 15:56:23 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:23 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:23 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:23 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.28.self_attn.v_proj.weight'
INFO 03-19 15:56:23 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:23 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:23 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:23 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.29.input_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.29.mlp.down_proj.weight'
INFO 03-19 15:56:23 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:23 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:23 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:23 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.29.mlp.gate_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
INFO 03-19 15:56:23 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:23 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:23 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:23 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{replicated}'
hosseins: DefaultModelLoader -> load_weights() name='layers.29.mlp.up_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)=''
INFO 03-19 15:56:23 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:23 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:23 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:23 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.29.post_attention_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.29.self_attn.k_proj.weight'
INFO 03-19 15:56:23 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:23 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:23 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:23 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.29.self_attn.o_proj.weight'
INFO 03-19 15:56:23 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:23 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:23 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:23 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.29.self_attn.q_proj.weight'
INFO 03-19 15:56:23 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:23 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:23 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:23 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.29.self_attn.v_proj.weight'
INFO 03-19 15:56:23 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:23 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:23 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:23 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.30.input_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.30.mlp.down_proj.weight'
INFO 03-19 15:56:23 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:23 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:23 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:23 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.30.mlp.gate_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
INFO 03-19 15:56:23 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:23 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:23 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:23 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{replicated}'
hosseins: DefaultModelLoader -> load_weights() name='layers.30.mlp.up_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)=''
INFO 03-19 15:56:23 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:23 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:23 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:23 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.30.post_attention_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.30.self_attn.k_proj.weight'
INFO 03-19 15:56:23 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:23 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:23 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:23 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.30.self_attn.o_proj.weight'
INFO 03-19 15:56:23 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:23 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:23 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:23 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.30.self_attn.q_proj.weight'
INFO 03-19 15:56:23 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:23 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:23 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:23 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:23 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.30.self_attn.v_proj.weight'
INFO 03-19 15:56:24 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:24 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:24 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:24 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:24 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:24 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.31.mlp.gate_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
INFO 03-19 15:56:24 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:24 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:24 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:24 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:24 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:24 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{replicated}'
hosseins: DefaultModelLoader -> load_weights() name='layers.31.mlp.up_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)=''
INFO 03-19 15:56:24 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:24 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:24 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:24 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:24 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:24 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.31.self_attn.k_proj.weight'
INFO 03-19 15:56:24 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:24 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:24 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:24 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:24 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:24 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.31.self_attn.o_proj.weight'
INFO 03-19 15:56:24 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:24 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:24 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:24 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:24 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:24 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.31.self_attn.q_proj.weight'
INFO 03-19 15:56:24 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:24 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:24 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:24 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:24 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:24 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.31.self_attn.v_proj.weight'
INFO 03-19 15:56:24 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:24 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:24 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:24 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:24 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:24 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:04<00:04,  2.17s/it]
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='embed_tokens.weight'
hosseins: VocabParallelEmbedding -> weight_loader() [start_idx=0]
hosseins: VocabParallelEmbedding -> weight_loader() [shard_size=128256]
hosseins: DefaultModelLoader -> load_weights() name='layers.0.input_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.0.mlp.down_proj.weight'
INFO 03-19 15:56:24 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:24 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:24 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:24 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:24 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:24 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.0.mlp.gate_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
INFO 03-19 15:56:25 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:25 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:25 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:25 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:25 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:25 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{replicated}'
hosseins: DefaultModelLoader -> load_weights() name='layers.0.mlp.up_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)=''
INFO 03-19 15:56:25 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:25 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:25 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:25 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:25 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:25 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.0.post_attention_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.0.self_attn.k_proj.weight'
INFO 03-19 15:56:25 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:25 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:25 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:25 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:25 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:25 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.0.self_attn.o_proj.weight'
INFO 03-19 15:56:25 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:25 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:25 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:25 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:25 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:25 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.0.self_attn.q_proj.weight'
INFO 03-19 15:56:25 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:25 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:25 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:25 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:25 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:25 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.0.self_attn.v_proj.weight'
INFO 03-19 15:56:25 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:25 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:25 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:25 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:25 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:25 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.1.input_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.1.mlp.down_proj.weight'
INFO 03-19 15:56:25 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:25 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:25 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:25 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:25 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:25 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.1.mlp.gate_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
INFO 03-19 15:56:25 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:25 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:25 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:25 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:25 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:25 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{replicated}'
hosseins: DefaultModelLoader -> load_weights() name='layers.1.mlp.up_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)=''
INFO 03-19 15:56:25 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:25 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:25 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:25 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:25 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:25 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.1.post_attention_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.1.self_attn.k_proj.weight'
INFO 03-19 15:56:25 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:25 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:25 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:25 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:25 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:25 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.1.self_attn.o_proj.weight'
INFO 03-19 15:56:26 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:26 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:26 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:26 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.1.self_attn.q_proj.weight'
INFO 03-19 15:56:26 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:26 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:26 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:26 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.1.self_attn.v_proj.weight'
INFO 03-19 15:56:26 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:26 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:26 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:26 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.2.input_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.2.mlp.down_proj.weight'
INFO 03-19 15:56:26 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:26 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:26 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:26 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.2.mlp.gate_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
INFO 03-19 15:56:26 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:26 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:26 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:26 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{replicated}'
hosseins: DefaultModelLoader -> load_weights() name='layers.2.mlp.up_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)=''
INFO 03-19 15:56:26 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:26 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:26 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:26 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.2.post_attention_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.2.self_attn.k_proj.weight'
INFO 03-19 15:56:26 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:26 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:26 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:26 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.2.self_attn.o_proj.weight'
INFO 03-19 15:56:26 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:26 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:26 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:26 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.2.self_attn.q_proj.weight'
INFO 03-19 15:56:26 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:26 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:26 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:26 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.2.self_attn.v_proj.weight'
INFO 03-19 15:56:26 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:26 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:26 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:26 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.3.input_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.3.mlp.down_proj.weight'
INFO 03-19 15:56:26 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:26 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:26 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:26 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.3.mlp.gate_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
INFO 03-19 15:56:26 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:26 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:26 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:26 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{replicated}'
hosseins: DefaultModelLoader -> load_weights() name='layers.3.mlp.up_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)=''
INFO 03-19 15:56:26 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:26 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:26 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:26 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.3.post_attention_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.3.self_attn.k_proj.weight'
INFO 03-19 15:56:26 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:26 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:26 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:26 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.3.self_attn.o_proj.weight'
INFO 03-19 15:56:26 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:26 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:26 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:26 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.3.self_attn.q_proj.weight'
INFO 03-19 15:56:26 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:26 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:26 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:26 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.3.self_attn.v_proj.weight'
INFO 03-19 15:56:26 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:26 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:26 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:26 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.4.input_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.4.mlp.down_proj.weight'
INFO 03-19 15:56:26 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:26 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:26 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:26 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.4.mlp.gate_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
INFO 03-19 15:56:26 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:26 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:26 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:26 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{replicated}'
hosseins: DefaultModelLoader -> load_weights() name='layers.4.mlp.up_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)=''
INFO 03-19 15:56:26 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:26 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:26 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:26 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.4.post_attention_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.4.self_attn.k_proj.weight'
INFO 03-19 15:56:26 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:26 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:26 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:26 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.4.self_attn.o_proj.weight'
INFO 03-19 15:56:26 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:26 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:26 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:26 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.4.self_attn.q_proj.weight'
INFO 03-19 15:56:26 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:26 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:26 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:26 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.4.self_attn.v_proj.weight'
INFO 03-19 15:56:26 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:26 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:26 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:26 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.5.input_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.5.mlp.down_proj.weight'
INFO 03-19 15:56:26 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:26 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:26 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:26 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.5.mlp.gate_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
INFO 03-19 15:56:26 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:26 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:26 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:26 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{replicated}'
hosseins: DefaultModelLoader -> load_weights() name='layers.5.mlp.up_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)=''
INFO 03-19 15:56:26 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:26 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:26 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:26 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.5.post_attention_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.5.self_attn.k_proj.weight'
INFO 03-19 15:56:26 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:26 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:26 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:26 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.5.self_attn.o_proj.weight'
INFO 03-19 15:56:26 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:26 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:26 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:26 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.5.self_attn.q_proj.weight'
INFO 03-19 15:56:26 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:26 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:26 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:26 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.5.self_attn.v_proj.weight'
INFO 03-19 15:56:26 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:26 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:26 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:26 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.6.input_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.6.mlp.down_proj.weight'
INFO 03-19 15:56:26 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:26 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:26 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:26 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:26 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.6.mlp.gate_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
INFO 03-19 15:56:27 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:27 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:27 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:27 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{replicated}'
hosseins: DefaultModelLoader -> load_weights() name='layers.6.mlp.up_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)=''
INFO 03-19 15:56:27 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:27 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:27 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:27 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.6.post_attention_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.6.self_attn.k_proj.weight'
INFO 03-19 15:56:27 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:27 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:27 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:27 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.6.self_attn.o_proj.weight'
INFO 03-19 15:56:27 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:27 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:27 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:27 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.6.self_attn.q_proj.weight'
INFO 03-19 15:56:27 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:27 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:27 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:27 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.6.self_attn.v_proj.weight'
INFO 03-19 15:56:27 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:27 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:27 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:27 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.7.input_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.7.mlp.down_proj.weight'
INFO 03-19 15:56:27 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:27 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:27 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:27 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.7.mlp.gate_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
INFO 03-19 15:56:27 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:27 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:27 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:27 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{replicated}'
hosseins: DefaultModelLoader -> load_weights() name='layers.7.mlp.up_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)=''
INFO 03-19 15:56:27 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:27 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:27 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:27 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.7.post_attention_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.7.self_attn.k_proj.weight'
INFO 03-19 15:56:27 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:27 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:27 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:27 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.7.self_attn.o_proj.weight'
INFO 03-19 15:56:27 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:27 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:27 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:27 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.7.self_attn.q_proj.weight'
INFO 03-19 15:56:27 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:27 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:27 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:27 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.7.self_attn.v_proj.weight'
INFO 03-19 15:56:27 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:27 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:27 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:27 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.8.input_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.8.mlp.down_proj.weight'
INFO 03-19 15:56:27 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:27 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:27 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:27 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.8.mlp.gate_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
INFO 03-19 15:56:27 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:27 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:27 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:27 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{replicated}'
hosseins: DefaultModelLoader -> load_weights() name='layers.8.mlp.up_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)=''
INFO 03-19 15:56:27 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:27 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:27 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:27 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.8.post_attention_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.8.self_attn.k_proj.weight'
INFO 03-19 15:56:27 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:27 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:27 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:27 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.8.self_attn.o_proj.weight'
INFO 03-19 15:56:27 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:27 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:27 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:27 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.8.self_attn.q_proj.weight'
INFO 03-19 15:56:27 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:27 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:27 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:27 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.8.self_attn.v_proj.weight'
INFO 03-19 15:56:27 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:27 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:27 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:27 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.75s/it]
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.10.input_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.10.mlp.down_proj.weight'
INFO 03-19 15:56:27 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:27 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:27 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:27 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.10.mlp.gate_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
INFO 03-19 15:56:27 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:27 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:27 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:27 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{replicated}'
hosseins: DefaultModelLoader -> load_weights() name='layers.10.mlp.up_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)=''
INFO 03-19 15:56:27 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:27 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:27 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:27 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.10.post_attention_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.10.self_attn.k_proj.weight'
INFO 03-19 15:56:27 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:27 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:27 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:27 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.10.self_attn.o_proj.weight'
INFO 03-19 15:56:27 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:27 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:27 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:27 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.10.self_attn.q_proj.weight'
INFO 03-19 15:56:27 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:27 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:27 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:27 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.10.self_attn.v_proj.weight'
INFO 03-19 15:56:27 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:27 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:27 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:27 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.11.input_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.11.mlp.down_proj.weight'
INFO 03-19 15:56:27 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:27 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:27 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:27 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:27 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.11.mlp.gate_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
INFO 03-19 15:56:28 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:28 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:28 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:28 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{replicated}'
hosseins: DefaultModelLoader -> load_weights() name='layers.11.mlp.up_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)=''
INFO 03-19 15:56:28 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:28 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:28 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:28 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.11.post_attention_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.11.self_attn.k_proj.weight'
INFO 03-19 15:56:28 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:28 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:28 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:28 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.11.self_attn.o_proj.weight'
INFO 03-19 15:56:28 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:28 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:28 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:28 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.11.self_attn.q_proj.weight'
INFO 03-19 15:56:28 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:28 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:28 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:28 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.11.self_attn.v_proj.weight'
INFO 03-19 15:56:28 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:28 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:28 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:28 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.12.input_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.12.mlp.down_proj.weight'
INFO 03-19 15:56:28 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:28 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:28 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:28 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.12.mlp.gate_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
INFO 03-19 15:56:28 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:28 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:28 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:28 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{replicated}'
hosseins: DefaultModelLoader -> load_weights() name='layers.12.mlp.up_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)=''
INFO 03-19 15:56:28 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:28 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:28 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:28 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.12.post_attention_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.12.self_attn.k_proj.weight'
INFO 03-19 15:56:28 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:28 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:28 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:28 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.12.self_attn.o_proj.weight'
INFO 03-19 15:56:28 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:28 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:28 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:28 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.12.self_attn.q_proj.weight'
INFO 03-19 15:56:28 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:28 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:28 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:28 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.12.self_attn.v_proj.weight'
INFO 03-19 15:56:28 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:28 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:28 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:28 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.13.input_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.13.mlp.down_proj.weight'
INFO 03-19 15:56:28 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:28 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:28 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:28 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.13.mlp.gate_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
INFO 03-19 15:56:28 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:28 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:28 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:28 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{replicated}'
hosseins: DefaultModelLoader -> load_weights() name='layers.13.mlp.up_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)=''
INFO 03-19 15:56:28 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:28 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:28 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:28 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.13.post_attention_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.13.self_attn.k_proj.weight'
INFO 03-19 15:56:28 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:28 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:28 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:28 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.13.self_attn.o_proj.weight'
INFO 03-19 15:56:28 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:28 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:28 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:28 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.13.self_attn.q_proj.weight'
INFO 03-19 15:56:28 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:28 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:28 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:28 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.13.self_attn.v_proj.weight'
INFO 03-19 15:56:28 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:28 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:28 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:28 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.14.input_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.14.mlp.down_proj.weight'
INFO 03-19 15:56:28 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:28 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:28 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:28 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.14.mlp.gate_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
INFO 03-19 15:56:28 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:28 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:28 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:28 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{replicated}'
hosseins: DefaultModelLoader -> load_weights() name='layers.14.mlp.up_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)=''
INFO 03-19 15:56:28 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:28 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:28 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:28 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.14.post_attention_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.14.self_attn.k_proj.weight'
INFO 03-19 15:56:28 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:28 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:28 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:28 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.14.self_attn.o_proj.weight'
INFO 03-19 15:56:28 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:28 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:28 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:28 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.14.self_attn.q_proj.weight'
INFO 03-19 15:56:28 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:28 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:28 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:28 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.14.self_attn.v_proj.weight'
INFO 03-19 15:56:28 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:28 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:28 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:28 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.15.input_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.15.mlp.down_proj.weight'
INFO 03-19 15:56:28 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:28 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:28 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:28 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:28 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.15.mlp.gate_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
INFO 03-19 15:56:29 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:29 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:29 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:29 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{replicated}'
hosseins: DefaultModelLoader -> load_weights() name='layers.15.mlp.up_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)=''
INFO 03-19 15:56:29 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:29 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:29 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:29 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.15.post_attention_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.15.self_attn.k_proj.weight'
INFO 03-19 15:56:29 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:29 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:29 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:29 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.15.self_attn.o_proj.weight'
INFO 03-19 15:56:29 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:29 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:29 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:29 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.15.self_attn.q_proj.weight'
INFO 03-19 15:56:29 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:29 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:29 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:29 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.15.self_attn.v_proj.weight'
INFO 03-19 15:56:29 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:29 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:29 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:29 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.16.input_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.16.mlp.down_proj.weight'
INFO 03-19 15:56:29 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:29 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:29 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:29 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.16.mlp.gate_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
INFO 03-19 15:56:29 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:29 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:29 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:29 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{replicated}'
hosseins: DefaultModelLoader -> load_weights() name='layers.16.mlp.up_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)=''
INFO 03-19 15:56:29 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:29 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:29 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:29 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.16.post_attention_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.16.self_attn.k_proj.weight'
INFO 03-19 15:56:29 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:29 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:29 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:29 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.16.self_attn.o_proj.weight'
INFO 03-19 15:56:29 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:29 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:29 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:29 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.16.self_attn.q_proj.weight'
INFO 03-19 15:56:29 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:29 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:29 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:29 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.16.self_attn.v_proj.weight'
INFO 03-19 15:56:29 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:29 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:29 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:29 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.17.input_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.17.mlp.down_proj.weight'
INFO 03-19 15:56:29 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:29 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:29 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:29 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.17.mlp.gate_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
INFO 03-19 15:56:29 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:29 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:29 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:29 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{replicated}'
hosseins: DefaultModelLoader -> load_weights() name='layers.17.mlp.up_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)=''
INFO 03-19 15:56:29 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:29 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:29 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:29 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.17.post_attention_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.17.self_attn.k_proj.weight'
INFO 03-19 15:56:29 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:29 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:29 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:29 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.17.self_attn.o_proj.weight'
INFO 03-19 15:56:29 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:29 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:29 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:29 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.17.self_attn.q_proj.weight'
INFO 03-19 15:56:29 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:29 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:29 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:29 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.17.self_attn.v_proj.weight'
INFO 03-19 15:56:29 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:29 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:29 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:29 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.18.input_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.18.mlp.down_proj.weight'
INFO 03-19 15:56:29 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:29 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:29 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:29 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.18.mlp.gate_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
INFO 03-19 15:56:29 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:29 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:29 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:29 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{replicated}'
hosseins: DefaultModelLoader -> load_weights() name='layers.18.mlp.up_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)=''
INFO 03-19 15:56:29 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:29 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:29 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:29 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.18.post_attention_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.18.self_attn.k_proj.weight'
INFO 03-19 15:56:29 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:29 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:29 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:29 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.18.self_attn.o_proj.weight'
INFO 03-19 15:56:29 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:29 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:29 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:29 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.18.self_attn.q_proj.weight'
INFO 03-19 15:56:29 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:29 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:29 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:29 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.18.self_attn.v_proj.weight'
INFO 03-19 15:56:29 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:29 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:29 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:29 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.19.input_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.19.mlp.down_proj.weight'
INFO 03-19 15:56:29 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:29 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:29 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:29 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.19.mlp.gate_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
INFO 03-19 15:56:29 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:29 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:29 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:29 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{replicated}'
hosseins: DefaultModelLoader -> load_weights() name='layers.19.mlp.up_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)=''
INFO 03-19 15:56:29 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:29 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:29 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:29 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.19.post_attention_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.19.self_attn.k_proj.weight'
INFO 03-19 15:56:29 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:29 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:29 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:29 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.19.self_attn.o_proj.weight'
INFO 03-19 15:56:29 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:29 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:29 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:29 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.19.self_attn.q_proj.weight'
INFO 03-19 15:56:29 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:29 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:29 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:29 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.19.self_attn.v_proj.weight'
INFO 03-19 15:56:29 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:29 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:29 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:29 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:29 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.20.mlp.gate_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)=''
INFO 03-19 15:56:30 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:30 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:30 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:30 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:30 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:30 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.20.self_attn.k_proj.weight'
INFO 03-19 15:56:30 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:30 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:30 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:30 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:30 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:30 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.20.self_attn.o_proj.weight'
INFO 03-19 15:56:30 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:30 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:30 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:30 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:30 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:30 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.20.self_attn.q_proj.weight'
INFO 03-19 15:56:30 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:30 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:30 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:30 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:30 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:30 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.20.self_attn.v_proj.weight'
INFO 03-19 15:56:30 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:30 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:30 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:30 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:30 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:30 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.9.input_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.9.mlp.down_proj.weight'
INFO 03-19 15:56:30 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:30 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:30 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:30 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:30 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 03-19 15:56:30 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.9.mlp.gate_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
INFO 03-19 15:56:30 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:30 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:30 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:30 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:30 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:30 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{replicated}'
hosseins: DefaultModelLoader -> load_weights() name='layers.9.mlp.up_proj.weight'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)=''
INFO 03-19 15:56:30 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:30 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:30 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 03-19 15:56:30 [linear.py:685] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:30 [linear.py:686] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 03-19 15:56:30 [linear.py:687] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.9.post_attention_layernorm.weight'
hosseins: DefaultModelLoader -> load_weights() name='layers.9.self_attn.k_proj.weight'
INFO 03-19 15:56:30 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:30 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:30 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:30 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:30 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:30 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.9.self_attn.o_proj.weight'
INFO 03-19 15:56:30 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:30 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:30 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:30 [linear.py:1254] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:30 [linear.py:1255] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:30 [linear.py:1256] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[1,4]0,1,2,3}'
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[1,4]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.9.self_attn.q_proj.weight'
INFO 03-19 15:56:30 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:30 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:30 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:30 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:30 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 03-19 15:56:30 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: DefaultModelLoader -> load_weights() name='layers.9.self_attn.v_proj.weight'
INFO 03-19 15:56:30 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 03-19 15:56:30 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 03-19 15:56:30 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 03-19 15:56:30 [linear.py:1120] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:30 [linear.py:1121] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 03-19 15:56:30 [linear.py:1122] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)='{devices=[4,1]0,1,2,3}'
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.72s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.57s/it]

hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() is completed
hosseins: load_weights() name='embed_tokens.weight'
hosseins: load_weights() param.shape=torch.Size([128256, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.0.self_attn.qkv_proj.weight'
hosseins: load_weights() param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.0.self_attn.o_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.0.mlp.gate_up_proj.weight'
hosseins: load_weights() param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.0.mlp.down_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.0.input_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.0.post_attention_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.1.self_attn.qkv_proj.weight'
hosseins: load_weights() param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.1.self_attn.o_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.1.mlp.gate_up_proj.weight'
hosseins: load_weights() param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.1.mlp.down_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.1.input_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.1.post_attention_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.2.self_attn.qkv_proj.weight'
hosseins: load_weights() param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.2.self_attn.o_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.2.mlp.gate_up_proj.weight'
hosseins: load_weights() param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.2.mlp.down_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.2.input_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.2.post_attention_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.3.self_attn.qkv_proj.weight'
hosseins: load_weights() param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.3.self_attn.o_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.3.mlp.gate_up_proj.weight'
hosseins: load_weights() param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.3.mlp.down_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.3.input_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.3.post_attention_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.4.self_attn.qkv_proj.weight'
hosseins: load_weights() param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.4.self_attn.o_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.4.mlp.gate_up_proj.weight'
hosseins: load_weights() param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.4.mlp.down_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.4.input_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.4.post_attention_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.5.self_attn.qkv_proj.weight'
hosseins: load_weights() param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.5.self_attn.o_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.5.mlp.gate_up_proj.weight'
hosseins: load_weights() param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.5.mlp.down_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.5.input_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.5.post_attention_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.6.self_attn.qkv_proj.weight'
hosseins: load_weights() param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.6.self_attn.o_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.6.mlp.gate_up_proj.weight'
hosseins: load_weights() param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.6.mlp.down_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.6.input_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.6.post_attention_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.7.self_attn.qkv_proj.weight'
hosseins: load_weights() param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.7.self_attn.o_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.7.mlp.gate_up_proj.weight'
hosseins: load_weights() param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.7.mlp.down_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.7.input_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.7.post_attention_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.8.self_attn.qkv_proj.weight'
hosseins: load_weights() param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.8.self_attn.o_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.8.mlp.gate_up_proj.weight'
hosseins: load_weights() param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.8.mlp.down_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.8.input_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.8.post_attention_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.9.self_attn.qkv_proj.weight'
hosseins: load_weights() param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.9.self_attn.o_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.9.mlp.gate_up_proj.weight'
hosseins: load_weights() param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.9.mlp.down_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.9.input_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.9.post_attention_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.10.self_attn.qkv_proj.weight'
hosseins: load_weights() param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.10.self_attn.o_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.10.mlp.gate_up_proj.weight'
hosseins: load_weights() param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.10.mlp.down_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.10.input_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.10.post_attention_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.11.self_attn.qkv_proj.weight'
hosseins: load_weights() param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.11.self_attn.o_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.11.mlp.gate_up_proj.weight'
hosseins: load_weights() param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.11.mlp.down_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.11.input_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.11.post_attention_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.12.self_attn.qkv_proj.weight'
hosseins: load_weights() param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.12.self_attn.o_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.12.mlp.gate_up_proj.weight'
hosseins: load_weights() param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.12.mlp.down_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.12.input_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.12.post_attention_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.13.self_attn.qkv_proj.weight'
hosseins: load_weights() param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.13.self_attn.o_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.13.mlp.gate_up_proj.weight'
hosseins: load_weights() param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.13.mlp.down_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.13.input_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.13.post_attention_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.14.self_attn.qkv_proj.weight'
hosseins: load_weights() param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.14.self_attn.o_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.14.mlp.gate_up_proj.weight'
hosseins: load_weights() param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.14.mlp.down_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.14.input_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.14.post_attention_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.15.self_attn.qkv_proj.weight'
hosseins: load_weights() param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.15.self_attn.o_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.15.mlp.gate_up_proj.weight'
hosseins: load_weights() param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.15.mlp.down_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.15.input_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.15.post_attention_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.16.self_attn.qkv_proj.weight'
hosseins: load_weights() param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.16.self_attn.o_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.16.mlp.gate_up_proj.weight'
hosseins: load_weights() param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.16.mlp.down_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.16.input_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.16.post_attention_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.17.self_attn.qkv_proj.weight'
hosseins: load_weights() param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.17.self_attn.o_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.17.mlp.gate_up_proj.weight'
hosseins: load_weights() param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.17.mlp.down_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.17.input_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.17.post_attention_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.18.self_attn.qkv_proj.weight'
hosseins: load_weights() param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.18.self_attn.o_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.18.mlp.gate_up_proj.weight'
hosseins: load_weights() param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.18.mlp.down_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.18.input_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.18.post_attention_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.19.self_attn.qkv_proj.weight'
hosseins: load_weights() param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.19.self_attn.o_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.19.mlp.gate_up_proj.weight'
hosseins: load_weights() param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.19.mlp.down_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.19.input_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.19.post_attention_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.20.self_attn.qkv_proj.weight'
hosseins: load_weights() param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.20.self_attn.o_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.20.mlp.gate_up_proj.weight'
hosseins: load_weights() param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.20.mlp.down_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.20.input_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.20.post_attention_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.21.self_attn.qkv_proj.weight'
hosseins: load_weights() param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.21.self_attn.o_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.21.mlp.gate_up_proj.weight'
hosseins: load_weights() param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.21.mlp.down_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.21.input_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.21.post_attention_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.22.self_attn.qkv_proj.weight'
hosseins: load_weights() param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.22.self_attn.o_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.22.mlp.gate_up_proj.weight'
hosseins: load_weights() param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.22.mlp.down_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.22.input_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.22.post_attention_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.23.self_attn.qkv_proj.weight'
hosseins: load_weights() param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.23.self_attn.o_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.23.mlp.gate_up_proj.weight'
hosseins: load_weights() param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.23.mlp.down_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.23.input_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.23.post_attention_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.24.self_attn.qkv_proj.weight'
hosseins: load_weights() param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.24.self_attn.o_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.24.mlp.gate_up_proj.weight'
hosseins: load_weights() param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.24.mlp.down_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.24.input_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.24.post_attention_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.25.self_attn.qkv_proj.weight'
hosseins: load_weights() param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.25.self_attn.o_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.25.mlp.gate_up_proj.weight'
hosseins: load_weights() param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.25.mlp.down_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.25.input_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.25.post_attention_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.26.self_attn.qkv_proj.weight'
hosseins: load_weights() param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.26.self_attn.o_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.26.mlp.gate_up_proj.weight'
hosseins: load_weights() param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.26.mlp.down_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.26.input_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.26.post_attention_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.27.self_attn.qkv_proj.weight'
hosseins: load_weights() param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.27.self_attn.o_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.27.mlp.gate_up_proj.weight'
hosseins: load_weights() param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.27.mlp.down_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.27.input_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.27.post_attention_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.28.self_attn.qkv_proj.weight'
hosseins: load_weights() param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.28.self_attn.o_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.28.mlp.gate_up_proj.weight'
hosseins: load_weights() param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.28.mlp.down_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.28.input_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.28.post_attention_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.29.self_attn.qkv_proj.weight'
hosseins: load_weights() param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.29.self_attn.o_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.29.mlp.gate_up_proj.weight'
hosseins: load_weights() param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.29.mlp.down_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.29.input_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.29.post_attention_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.30.self_attn.qkv_proj.weight'
hosseins: load_weights() param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.30.self_attn.o_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.30.mlp.gate_up_proj.weight'
hosseins: load_weights() param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.30.mlp.down_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.30.input_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.30.post_attention_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.31.self_attn.qkv_proj.weight'
hosseins: load_weights() param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.31.self_attn.o_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.31.mlp.gate_up_proj.weight'
hosseins: load_weights() param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: load_weights() name='layers.31.mlp.down_proj.weight'
hosseins: load_weights() param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: load_weights() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: load_weights() name='layers.31.input_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='layers.31.post_attention_layernorm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
hosseins: load_weights() name='norm.weight'
hosseins: load_weights() param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: load_weights() get_shard_spec(param)='{replicated}'
INFO 03-19 15:56:30 [loader.py:430] Loading weights took 10.55 seconds
hosseins: load_model() model is loaded
hosseins: _initialize_kv_caches() self.model_executor=<vllm.v1.executor.abstract.UniProcExecutor object at 0x716b66bd5450>
hosseins: _initialize_kv_caches() kv_cache_specs=[{'model.layers.0.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.1.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.2.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.3.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.4.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.5.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.6.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.7.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.8.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.9.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.10.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.11.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.12.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.13.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.14.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.15.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.16.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.17.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.18.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.19.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.20.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.21.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.22.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.23.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.24.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.25.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.26.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.27.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.28.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.29.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.30.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.31.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False)}]
hosseins: ModelWrapperV1.forward()
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ModelWrapperV1 -> forward() 1 get_shard_spec(input_ids)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ModelWrapperV1 -> forward() 1 get_shard_spec(positions)=''
hosseins: LlamaForCasualLM.forward()
hosseins: LlamaModel.forward()
hosseins: LlamaModel.forward() get_pp_group().is_first_rank=True
hosseins: VocabParallelEmbedding.forward()
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: VocabParallelEmbedding.forward() [get_shard_spec(masked_input)='']
hosseins: UnquantizedEmbeddingMethod -> embedding()
hosseins: UnquantizedEmbeddingMethod -> embedding() [layer.weight.shape=torch.Size([128256, 4096])]
hosseins: UnquantizedEmbeddingMethod -> embedding() [layer.weight.device=device(type='xla', index=0)]
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: VocabParallelEmbedding.forward() [get_shard_spec(output_parallel)='']
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: VocabParallelEmbedding.forward() [get_shard_spec(output)='']
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() 1 get_shard_spec(hidden_states)=''
hosseins: LlamaModel.forward() layer start
hosseins: LlamaModel.forward() ============= type(layer)=<class 'vllm.model_executor.models.llama.LlamaDecoderLayer'> =============
hosseins: LlamaModel.forward() Name: self_attn.qkv_proj.weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: self_attn.o_proj.weight, Shape: param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.gate_up_proj.weight, Shape: param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.down_proj.weight, Shape: param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: input_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaModel.forward() Name: post_attention_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() hidden_states.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaAttention.forward() self.qkv_proj=QKVParallelLinear(in_features=4096, output_features=6144, bias=False, tp_size=1, gather_output=False)
hosseins: LlamaAttention.forward() Name: weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaAttention.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.QKVParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([6144, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 6144]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([512, 6144])
hosseins: LlamaAttention.forward() qkv.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=''
hosseins: Attention. forward()
hosseins: Attention -> forward() key.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(key)='' value.shape=torch.Size([512, 1024])
hosseins: Attention -> forward() value.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(value)='' value.shape=torch.Size([512, 1024])
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 4096]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaAttention.forward() output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(residual)=''
hosseins: LlamaMLP.forward()
hosseins: LlamaMLP.forward() x.shape=torch.Size([512, 4096])
hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaMLP.forward() get_shard_spec(x)=''
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([28672, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 28672]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 14336]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 14336]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaModel.forward() ============= type(layer)=<class 'vllm.model_executor.models.llama.LlamaDecoderLayer'> =============
hosseins: LlamaModel.forward() Name: self_attn.qkv_proj.weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: self_attn.o_proj.weight, Shape: param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.gate_up_proj.weight, Shape: param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.down_proj.weight, Shape: param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: input_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaModel.forward() Name: post_attention_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() hidden_states.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaAttention.forward() self.qkv_proj=QKVParallelLinear(in_features=4096, output_features=6144, bias=False, tp_size=1, gather_output=False)
hosseins: LlamaAttention.forward() Name: weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaAttention.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.QKVParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([6144, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 6144]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([512, 6144])
hosseins: LlamaAttention.forward() qkv.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=''
hosseins: Attention. forward()
hosseins: Attention -> forward() key.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(key)='' value.shape=torch.Size([512, 1024])
hosseins: Attention -> forward() value.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(value)='' value.shape=torch.Size([512, 1024])
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 4096]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaAttention.forward() output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(residual)=''
hosseins: LlamaMLP.forward()
hosseins: LlamaMLP.forward() x.shape=torch.Size([512, 4096])
hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaMLP.forward() get_shard_spec(x)=''
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([28672, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 28672]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 14336]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 14336]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaModel.forward() ============= type(layer)=<class 'vllm.model_executor.models.llama.LlamaDecoderLayer'> =============
hosseins: LlamaModel.forward() Name: self_attn.qkv_proj.weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: self_attn.o_proj.weight, Shape: param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.gate_up_proj.weight, Shape: param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.down_proj.weight, Shape: param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: input_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaModel.forward() Name: post_attention_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() hidden_states.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaAttention.forward() self.qkv_proj=QKVParallelLinear(in_features=4096, output_features=6144, bias=False, tp_size=1, gather_output=False)
hosseins: LlamaAttention.forward() Name: weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaAttention.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.QKVParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([6144, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 6144]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([512, 6144])
hosseins: LlamaAttention.forward() qkv.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=''
hosseins: Attention. forward()
hosseins: Attention -> forward() key.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(key)='' value.shape=torch.Size([512, 1024])
hosseins: Attention -> forward() value.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(value)='' value.shape=torch.Size([512, 1024])
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 4096]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaAttention.forward() output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(residual)=''
hosseins: LlamaMLP.forward()
hosseins: LlamaMLP.forward() x.shape=torch.Size([512, 4096])
hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaMLP.forward() get_shard_spec(x)=''
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([28672, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 28672]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 14336]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 14336]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaModel.forward() ============= type(layer)=<class 'vllm.model_executor.models.llama.LlamaDecoderLayer'> =============
hosseins: LlamaModel.forward() Name: self_attn.qkv_proj.weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: self_attn.o_proj.weight, Shape: param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.gate_up_proj.weight, Shape: param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.down_proj.weight, Shape: param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: input_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaModel.forward() Name: post_attention_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() hidden_states.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaAttention.forward() self.qkv_proj=QKVParallelLinear(in_features=4096, output_features=6144, bias=False, tp_size=1, gather_output=False)
hosseins: LlamaAttention.forward() Name: weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaAttention.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.QKVParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([6144, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 6144]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([512, 6144])
hosseins: LlamaAttention.forward() qkv.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=''
hosseins: Attention. forward()
hosseins: Attention -> forward() key.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(key)='' value.shape=torch.Size([512, 1024])
hosseins: Attention -> forward() value.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(value)='' value.shape=torch.Size([512, 1024])
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 4096]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaAttention.forward() output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(residual)=''
hosseins: LlamaMLP.forward()
hosseins: LlamaMLP.forward() x.shape=torch.Size([512, 4096])
hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaMLP.forward() get_shard_spec(x)=''
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([28672, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 28672]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 14336]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 14336]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaModel.forward() ============= type(layer)=<class 'vllm.model_executor.models.llama.LlamaDecoderLayer'> =============
hosseins: LlamaModel.forward() Name: self_attn.qkv_proj.weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: self_attn.o_proj.weight, Shape: param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.gate_up_proj.weight, Shape: param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.down_proj.weight, Shape: param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: input_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaModel.forward() Name: post_attention_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() hidden_states.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaAttention.forward() self.qkv_proj=QKVParallelLinear(in_features=4096, output_features=6144, bias=False, tp_size=1, gather_output=False)
hosseins: LlamaAttention.forward() Name: weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaAttention.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.QKVParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([6144, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 6144]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([512, 6144])
hosseins: LlamaAttention.forward() qkv.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=''
hosseins: Attention. forward()
hosseins: Attention -> forward() key.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(key)='' value.shape=torch.Size([512, 1024])
hosseins: Attention -> forward() value.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(value)='' value.shape=torch.Size([512, 1024])
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 4096]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaAttention.forward() output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(residual)=''
hosseins: LlamaMLP.forward()
hosseins: LlamaMLP.forward() x.shape=torch.Size([512, 4096])
hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaMLP.forward() get_shard_spec(x)=''
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([28672, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 28672]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 14336]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 14336]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaModel.forward() ============= type(layer)=<class 'vllm.model_executor.models.llama.LlamaDecoderLayer'> =============
hosseins: LlamaModel.forward() Name: self_attn.qkv_proj.weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: self_attn.o_proj.weight, Shape: param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.gate_up_proj.weight, Shape: param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.down_proj.weight, Shape: param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: input_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaModel.forward() Name: post_attention_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() hidden_states.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaAttention.forward() self.qkv_proj=QKVParallelLinear(in_features=4096, output_features=6144, bias=False, tp_size=1, gather_output=False)
hosseins: LlamaAttention.forward() Name: weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaAttention.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.QKVParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([6144, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 6144]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([512, 6144])
hosseins: LlamaAttention.forward() qkv.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=''
hosseins: Attention. forward()
hosseins: Attention -> forward() key.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(key)='' value.shape=torch.Size([512, 1024])
hosseins: Attention -> forward() value.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(value)='' value.shape=torch.Size([512, 1024])
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 4096]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaAttention.forward() output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(residual)=''
hosseins: LlamaMLP.forward()
hosseins: LlamaMLP.forward() x.shape=torch.Size([512, 4096])
hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaMLP.forward() get_shard_spec(x)=''
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([28672, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 28672]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 14336]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 14336]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaModel.forward() ============= type(layer)=<class 'vllm.model_executor.models.llama.LlamaDecoderLayer'> =============
hosseins: LlamaModel.forward() Name: self_attn.qkv_proj.weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: self_attn.o_proj.weight, Shape: param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.gate_up_proj.weight, Shape: param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.down_proj.weight, Shape: param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: input_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaModel.forward() Name: post_attention_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() hidden_states.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaAttention.forward() self.qkv_proj=QKVParallelLinear(in_features=4096, output_features=6144, bias=False, tp_size=1, gather_output=False)
hosseins: LlamaAttention.forward() Name: weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaAttention.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.QKVParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([6144, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 6144]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([512, 6144])
hosseins: LlamaAttention.forward() qkv.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=''
hosseins: Attention. forward()
hosseins: Attention -> forward() key.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(key)='' value.shape=torch.Size([512, 1024])
hosseins: Attention -> forward() value.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(value)='' value.shape=torch.Size([512, 1024])
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 4096]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaAttention.forward() output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(residual)=''
hosseins: LlamaMLP.forward()
hosseins: LlamaMLP.forward() x.shape=torch.Size([512, 4096])
hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaMLP.forward() get_shard_spec(x)=''
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([28672, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 28672]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 14336]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 14336]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaModel.forward() ============= type(layer)=<class 'vllm.model_executor.models.llama.LlamaDecoderLayer'> =============
hosseins: LlamaModel.forward() Name: self_attn.qkv_proj.weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: self_attn.o_proj.weight, Shape: param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.gate_up_proj.weight, Shape: param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.down_proj.weight, Shape: param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: input_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaModel.forward() Name: post_attention_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() hidden_states.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaAttention.forward() self.qkv_proj=QKVParallelLinear(in_features=4096, output_features=6144, bias=False, tp_size=1, gather_output=False)
hosseins: LlamaAttention.forward() Name: weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaAttention.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.QKVParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([6144, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 6144]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([512, 6144])
hosseins: LlamaAttention.forward() qkv.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=''
hosseins: Attention. forward()
hosseins: Attention -> forward() key.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(key)='' value.shape=torch.Size([512, 1024])
hosseins: Attention -> forward() value.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(value)='' value.shape=torch.Size([512, 1024])
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 4096]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaAttention.forward() output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(residual)=''
hosseins: LlamaMLP.forward()
hosseins: LlamaMLP.forward() x.shape=torch.Size([512, 4096])
hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaMLP.forward() get_shard_spec(x)=''
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([28672, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 28672]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 14336]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 14336]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaModel.forward() ============= type(layer)=<class 'vllm.model_executor.models.llama.LlamaDecoderLayer'> =============
hosseins: LlamaModel.forward() Name: self_attn.qkv_proj.weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: self_attn.o_proj.weight, Shape: param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.gate_up_proj.weight, Shape: param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.down_proj.weight, Shape: param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: input_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaModel.forward() Name: post_attention_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() hidden_states.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaAttention.forward() self.qkv_proj=QKVParallelLinear(in_features=4096, output_features=6144, bias=False, tp_size=1, gather_output=False)
hosseins: LlamaAttention.forward() Name: weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaAttention.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.QKVParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([6144, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 6144]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([512, 6144])
hosseins: LlamaAttention.forward() qkv.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=''
hosseins: Attention. forward()
hosseins: Attention -> forward() key.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(key)='' value.shape=torch.Size([512, 1024])
hosseins: Attention -> forward() value.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(value)='' value.shape=torch.Size([512, 1024])
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 4096]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaAttention.forward() output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(residual)=''
hosseins: LlamaMLP.forward()
hosseins: LlamaMLP.forward() x.shape=torch.Size([512, 4096])
hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaMLP.forward() get_shard_spec(x)=''
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([28672, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 28672]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 14336]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 14336]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaModel.forward() ============= type(layer)=<class 'vllm.model_executor.models.llama.LlamaDecoderLayer'> =============
hosseins: LlamaModel.forward() Name: self_attn.qkv_proj.weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: self_attn.o_proj.weight, Shape: param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.gate_up_proj.weight, Shape: param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.down_proj.weight, Shape: param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: input_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaModel.forward() Name: post_attention_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() hidden_states.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaAttention.forward() self.qkv_proj=QKVParallelLinear(in_features=4096, output_features=6144, bias=False, tp_size=1, gather_output=False)
hosseins: LlamaAttention.forward() Name: weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaAttention.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.QKVParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([6144, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 6144]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([512, 6144])
hosseins: LlamaAttention.forward() qkv.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=''
hosseins: Attention. forward()
hosseins: Attention -> forward() key.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(key)='' value.shape=torch.Size([512, 1024])
hosseins: Attention -> forward() value.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(value)='' value.shape=torch.Size([512, 1024])
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 4096]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaAttention.forward() output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(residual)=''
hosseins: LlamaMLP.forward()
hosseins: LlamaMLP.forward() x.shape=torch.Size([512, 4096])
hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaMLP.forward() get_shard_spec(x)=''
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([28672, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 28672]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 14336]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 14336]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaModel.forward() ============= type(layer)=<class 'vllm.model_executor.models.llama.LlamaDecoderLayer'> =============
hosseins: LlamaModel.forward() Name: self_attn.qkv_proj.weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: self_attn.o_proj.weight, Shape: param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.gate_up_proj.weight, Shape: param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.down_proj.weight, Shape: param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: input_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaModel.forward() Name: post_attention_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() hidden_states.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaAttention.forward() self.qkv_proj=QKVParallelLinear(in_features=4096, output_features=6144, bias=False, tp_size=1, gather_output=False)
hosseins: LlamaAttention.forward() Name: weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaAttention.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.QKVParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([6144, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 6144]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([512, 6144])
hosseins: LlamaAttention.forward() qkv.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=''
hosseins: Attention. forward()
hosseins: Attention -> forward() key.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(key)='' value.shape=torch.Size([512, 1024])
hosseins: Attention -> forward() value.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(value)='' value.shape=torch.Size([512, 1024])
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 4096]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaAttention.forward() output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(residual)=''
hosseins: LlamaMLP.forward()
hosseins: LlamaMLP.forward() x.shape=torch.Size([512, 4096])
hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaMLP.forward() get_shard_spec(x)=''
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([28672, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 28672]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 14336]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 14336]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaModel.forward() ============= type(layer)=<class 'vllm.model_executor.models.llama.LlamaDecoderLayer'> =============
hosseins: LlamaModel.forward() Name: self_attn.qkv_proj.weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: self_attn.o_proj.weight, Shape: param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.gate_up_proj.weight, Shape: param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.down_proj.weight, Shape: param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: input_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaModel.forward() Name: post_attention_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() hidden_states.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaAttention.forward() self.qkv_proj=QKVParallelLinear(in_features=4096, output_features=6144, bias=False, tp_size=1, gather_output=False)
hosseins: LlamaAttention.forward() Name: weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaAttention.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.QKVParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([6144, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 6144]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([512, 6144])
hosseins: LlamaAttention.forward() qkv.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=''
hosseins: Attention. forward()
hosseins: Attention -> forward() key.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(key)='' value.shape=torch.Size([512, 1024])
hosseins: Attention -> forward() value.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(value)='' value.shape=torch.Size([512, 1024])
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 4096]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaAttention.forward() output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(residual)=''
hosseins: LlamaMLP.forward()
hosseins: LlamaMLP.forward() x.shape=torch.Size([512, 4096])
hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaMLP.forward() get_shard_spec(x)=''
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([28672, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 28672]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 14336]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 14336]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaModel.forward() ============= type(layer)=<class 'vllm.model_executor.models.llama.LlamaDecoderLayer'> =============
hosseins: LlamaModel.forward() Name: self_attn.qkv_proj.weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: self_attn.o_proj.weight, Shape: param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.gate_up_proj.weight, Shape: param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.down_proj.weight, Shape: param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: input_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaModel.forward() Name: post_attention_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() hidden_states.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaAttention.forward() self.qkv_proj=QKVParallelLinear(in_features=4096, output_features=6144, bias=False, tp_size=1, gather_output=False)
hosseins: LlamaAttention.forward() Name: weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaAttention.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.QKVParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([6144, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 6144]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([512, 6144])
hosseins: LlamaAttention.forward() qkv.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=''
hosseins: Attention. forward()
hosseins: Attention -> forward() key.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(key)='' value.shape=torch.Size([512, 1024])
hosseins: Attention -> forward() value.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(value)='' value.shape=torch.Size([512, 1024])
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 4096]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaAttention.forward() output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(residual)=''
hosseins: LlamaMLP.forward()
hosseins: LlamaMLP.forward() x.shape=torch.Size([512, 4096])
hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaMLP.forward() get_shard_spec(x)=''
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([28672, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 28672]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 14336]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 14336]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaModel.forward() ============= type(layer)=<class 'vllm.model_executor.models.llama.LlamaDecoderLayer'> =============
hosseins: LlamaModel.forward() Name: self_attn.qkv_proj.weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: self_attn.o_proj.weight, Shape: param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.gate_up_proj.weight, Shape: param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.down_proj.weight, Shape: param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: input_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaModel.forward() Name: post_attention_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() hidden_states.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaAttention.forward() self.qkv_proj=QKVParallelLinear(in_features=4096, output_features=6144, bias=False, tp_size=1, gather_output=False)
hosseins: LlamaAttention.forward() Name: weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaAttention.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.QKVParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([6144, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 6144]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([512, 6144])
hosseins: LlamaAttention.forward() qkv.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=''
hosseins: Attention. forward()
hosseins: Attention -> forward() key.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(key)='' value.shape=torch.Size([512, 1024])
hosseins: Attention -> forward() value.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(value)='' value.shape=torch.Size([512, 1024])
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 4096]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaAttention.forward() output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(residual)=''
hosseins: LlamaMLP.forward()
hosseins: LlamaMLP.forward() x.shape=torch.Size([512, 4096])
hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaMLP.forward() get_shard_spec(x)=''
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([28672, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 28672]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 14336]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 14336]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaModel.forward() ============= type(layer)=<class 'vllm.model_executor.models.llama.LlamaDecoderLayer'> =============
hosseins: LlamaModel.forward() Name: self_attn.qkv_proj.weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: self_attn.o_proj.weight, Shape: param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.gate_up_proj.weight, Shape: param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.down_proj.weight, Shape: param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: input_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaModel.forward() Name: post_attention_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() hidden_states.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaAttention.forward() self.qkv_proj=QKVParallelLinear(in_features=4096, output_features=6144, bias=False, tp_size=1, gather_output=False)
hosseins: LlamaAttention.forward() Name: weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaAttention.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.QKVParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([6144, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 6144]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([512, 6144])
hosseins: LlamaAttention.forward() qkv.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=''
hosseins: Attention. forward()
hosseins: Attention -> forward() key.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(key)='' value.shape=torch.Size([512, 1024])
hosseins: Attention -> forward() value.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(value)='' value.shape=torch.Size([512, 1024])
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 4096]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaAttention.forward() output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(residual)=''
hosseins: LlamaMLP.forward()
hosseins: LlamaMLP.forward() x.shape=torch.Size([512, 4096])
hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaMLP.forward() get_shard_spec(x)=''
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([28672, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 28672]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 14336]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 14336]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaModel.forward() ============= type(layer)=<class 'vllm.model_executor.models.llama.LlamaDecoderLayer'> =============
hosseins: LlamaModel.forward() Name: self_attn.qkv_proj.weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: self_attn.o_proj.weight, Shape: param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.gate_up_proj.weight, Shape: param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.down_proj.weight, Shape: param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: input_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaModel.forward() Name: post_attention_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() hidden_states.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaAttention.forward() self.qkv_proj=QKVParallelLinear(in_features=4096, output_features=6144, bias=False, tp_size=1, gather_output=False)
hosseins: LlamaAttention.forward() Name: weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaAttention.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.QKVParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([6144, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 6144]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([512, 6144])
hosseins: LlamaAttention.forward() qkv.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=''
hosseins: Attention. forward()
hosseins: Attention -> forward() key.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(key)='' value.shape=torch.Size([512, 1024])
hosseins: Attention -> forward() value.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(value)='' value.shape=torch.Size([512, 1024])
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 4096]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaAttention.forward() output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(residual)=''
hosseins: LlamaMLP.forward()
hosseins: LlamaMLP.forward() x.shape=torch.Size([512, 4096])
hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaMLP.forward() get_shard_spec(x)=''
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([28672, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 28672]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 14336]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 14336]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaModel.forward() ============= type(layer)=<class 'vllm.model_executor.models.llama.LlamaDecoderLayer'> =============
hosseins: LlamaModel.forward() Name: self_attn.qkv_proj.weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: self_attn.o_proj.weight, Shape: param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.gate_up_proj.weight, Shape: param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.down_proj.weight, Shape: param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: input_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaModel.forward() Name: post_attention_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() hidden_states.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaAttention.forward() self.qkv_proj=QKVParallelLinear(in_features=4096, output_features=6144, bias=False, tp_size=1, gather_output=False)
hosseins: LlamaAttention.forward() Name: weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaAttention.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.QKVParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([6144, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 6144]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([512, 6144])
hosseins: LlamaAttention.forward() qkv.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=''
hosseins: Attention. forward()
hosseins: Attention -> forward() key.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(key)='' value.shape=torch.Size([512, 1024])
hosseins: Attention -> forward() value.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(value)='' value.shape=torch.Size([512, 1024])
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 4096]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaAttention.forward() output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(residual)=''
hosseins: LlamaMLP.forward()
hosseins: LlamaMLP.forward() x.shape=torch.Size([512, 4096])
hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaMLP.forward() get_shard_spec(x)=''
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([28672, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 28672]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 14336]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 14336]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaModel.forward() ============= type(layer)=<class 'vllm.model_executor.models.llama.LlamaDecoderLayer'> =============
hosseins: LlamaModel.forward() Name: self_attn.qkv_proj.weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: self_attn.o_proj.weight, Shape: param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.gate_up_proj.weight, Shape: param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.down_proj.weight, Shape: param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: input_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaModel.forward() Name: post_attention_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() hidden_states.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaAttention.forward() self.qkv_proj=QKVParallelLinear(in_features=4096, output_features=6144, bias=False, tp_size=1, gather_output=False)
hosseins: LlamaAttention.forward() Name: weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaAttention.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.QKVParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([6144, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 6144]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([512, 6144])
hosseins: LlamaAttention.forward() qkv.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=''
hosseins: Attention. forward()
hosseins: Attention -> forward() key.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(key)='' value.shape=torch.Size([512, 1024])
hosseins: Attention -> forward() value.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(value)='' value.shape=torch.Size([512, 1024])
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 4096]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaAttention.forward() output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(residual)=''
hosseins: LlamaMLP.forward()
hosseins: LlamaMLP.forward() x.shape=torch.Size([512, 4096])
hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaMLP.forward() get_shard_spec(x)=''
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([28672, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 28672]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 14336]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 14336]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaModel.forward() ============= type(layer)=<class 'vllm.model_executor.models.llama.LlamaDecoderLayer'> =============
hosseins: LlamaModel.forward() Name: self_attn.qkv_proj.weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: self_attn.o_proj.weight, Shape: param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.gate_up_proj.weight, Shape: param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.down_proj.weight, Shape: param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: input_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaModel.forward() Name: post_attention_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() hidden_states.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaAttention.forward() self.qkv_proj=QKVParallelLinear(in_features=4096, output_features=6144, bias=False, tp_size=1, gather_output=False)
hosseins: LlamaAttention.forward() Name: weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaAttention.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.QKVParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([6144, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 6144]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([512, 6144])
hosseins: LlamaAttention.forward() qkv.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=''
hosseins: Attention. forward()
hosseins: Attention -> forward() key.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(key)='' value.shape=torch.Size([512, 1024])
hosseins: Attention -> forward() value.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(value)='' value.shape=torch.Size([512, 1024])
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 4096]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaAttention.forward() output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(residual)=''
hosseins: LlamaMLP.forward()
hosseins: LlamaMLP.forward() x.shape=torch.Size([512, 4096])
hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaMLP.forward() get_shard_spec(x)=''
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([28672, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 28672]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 14336]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 14336]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaModel.forward() ============= type(layer)=<class 'vllm.model_executor.models.llama.LlamaDecoderLayer'> =============
hosseins: LlamaModel.forward() Name: self_attn.qkv_proj.weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: self_attn.o_proj.weight, Shape: param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.gate_up_proj.weight, Shape: param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.down_proj.weight, Shape: param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: input_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaModel.forward() Name: post_attention_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() hidden_states.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaAttention.forward() self.qkv_proj=QKVParallelLinear(in_features=4096, output_features=6144, bias=False, tp_size=1, gather_output=False)
hosseins: LlamaAttention.forward() Name: weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaAttention.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.QKVParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([6144, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 6144]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([512, 6144])
hosseins: LlamaAttention.forward() qkv.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=''
hosseins: Attention. forward()
hosseins: Attention -> forward() key.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(key)='' value.shape=torch.Size([512, 1024])
hosseins: Attention -> forward() value.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(value)='' value.shape=torch.Size([512, 1024])
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 4096]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaAttention.forward() output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(residual)=''
hosseins: LlamaMLP.forward()
hosseins: LlamaMLP.forward() x.shape=torch.Size([512, 4096])
hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaMLP.forward() get_shard_spec(x)=''
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([28672, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 28672]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 14336]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 14336]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaModel.forward() ============= type(layer)=<class 'vllm.model_executor.models.llama.LlamaDecoderLayer'> =============
hosseins: LlamaModel.forward() Name: self_attn.qkv_proj.weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: self_attn.o_proj.weight, Shape: param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.gate_up_proj.weight, Shape: param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.down_proj.weight, Shape: param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: input_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaModel.forward() Name: post_attention_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() hidden_states.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaAttention.forward() self.qkv_proj=QKVParallelLinear(in_features=4096, output_features=6144, bias=False, tp_size=1, gather_output=False)
hosseins: LlamaAttention.forward() Name: weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaAttention.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.QKVParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([6144, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 6144]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([512, 6144])
hosseins: LlamaAttention.forward() qkv.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=''
hosseins: Attention. forward()
hosseins: Attention -> forward() key.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(key)='' value.shape=torch.Size([512, 1024])
hosseins: Attention -> forward() value.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(value)='' value.shape=torch.Size([512, 1024])
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 4096]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaAttention.forward() output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(residual)=''
hosseins: LlamaMLP.forward()
hosseins: LlamaMLP.forward() x.shape=torch.Size([512, 4096])
hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaMLP.forward() get_shard_spec(x)=''
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([28672, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 28672]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 14336]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 14336]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaModel.forward() ============= type(layer)=<class 'vllm.model_executor.models.llama.LlamaDecoderLayer'> =============
hosseins: LlamaModel.forward() Name: self_attn.qkv_proj.weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: self_attn.o_proj.weight, Shape: param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.gate_up_proj.weight, Shape: param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.down_proj.weight, Shape: param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: input_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaModel.forward() Name: post_attention_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() hidden_states.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaAttention.forward() self.qkv_proj=QKVParallelLinear(in_features=4096, output_features=6144, bias=False, tp_size=1, gather_output=False)
hosseins: LlamaAttention.forward() Name: weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaAttention.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.QKVParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([6144, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 6144]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([512, 6144])
hosseins: LlamaAttention.forward() qkv.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=''
hosseins: Attention. forward()
hosseins: Attention -> forward() key.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(key)='' value.shape=torch.Size([512, 1024])
hosseins: Attention -> forward() value.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(value)='' value.shape=torch.Size([512, 1024])
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 4096]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaAttention.forward() output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(residual)=''
hosseins: LlamaMLP.forward()
hosseins: LlamaMLP.forward() x.shape=torch.Size([512, 4096])
hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaMLP.forward() get_shard_spec(x)=''
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([28672, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 28672]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 14336]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 14336]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaModel.forward() ============= type(layer)=<class 'vllm.model_executor.models.llama.LlamaDecoderLayer'> =============
hosseins: LlamaModel.forward() Name: self_attn.qkv_proj.weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: self_attn.o_proj.weight, Shape: param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.gate_up_proj.weight, Shape: param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.down_proj.weight, Shape: param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: input_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaModel.forward() Name: post_attention_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() hidden_states.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaAttention.forward() self.qkv_proj=QKVParallelLinear(in_features=4096, output_features=6144, bias=False, tp_size=1, gather_output=False)
hosseins: LlamaAttention.forward() Name: weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaAttention.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.QKVParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([6144, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 6144]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([512, 6144])
hosseins: LlamaAttention.forward() qkv.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=''
hosseins: Attention. forward()
hosseins: Attention -> forward() key.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(key)='' value.shape=torch.Size([512, 1024])
hosseins: Attention -> forward() value.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(value)='' value.shape=torch.Size([512, 1024])
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 4096]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaAttention.forward() output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(residual)=''
hosseins: LlamaMLP.forward()
hosseins: LlamaMLP.forward() x.shape=torch.Size([512, 4096])
hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaMLP.forward() get_shard_spec(x)=''
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([28672, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 28672]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 14336]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 14336]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaModel.forward() ============= type(layer)=<class 'vllm.model_executor.models.llama.LlamaDecoderLayer'> =============
hosseins: LlamaModel.forward() Name: self_attn.qkv_proj.weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: self_attn.o_proj.weight, Shape: param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.gate_up_proj.weight, Shape: param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.down_proj.weight, Shape: param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: input_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaModel.forward() Name: post_attention_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() hidden_states.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaAttention.forward() self.qkv_proj=QKVParallelLinear(in_features=4096, output_features=6144, bias=False, tp_size=1, gather_output=False)
hosseins: LlamaAttention.forward() Name: weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaAttention.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.QKVParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([6144, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 6144]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([512, 6144])
hosseins: LlamaAttention.forward() qkv.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=''
hosseins: Attention. forward()
hosseins: Attention -> forward() key.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(key)='' value.shape=torch.Size([512, 1024])
hosseins: Attention -> forward() value.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(value)='' value.shape=torch.Size([512, 1024])
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 4096]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaAttention.forward() output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(residual)=''
hosseins: LlamaMLP.forward()
hosseins: LlamaMLP.forward() x.shape=torch.Size([512, 4096])
hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaMLP.forward() get_shard_spec(x)=''
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([28672, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 28672]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 14336]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 14336]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaModel.forward() ============= type(layer)=<class 'vllm.model_executor.models.llama.LlamaDecoderLayer'> =============
hosseins: LlamaModel.forward() Name: self_attn.qkv_proj.weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: self_attn.o_proj.weight, Shape: param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.gate_up_proj.weight, Shape: param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.down_proj.weight, Shape: param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: input_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaModel.forward() Name: post_attention_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() hidden_states.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaAttention.forward() self.qkv_proj=QKVParallelLinear(in_features=4096, output_features=6144, bias=False, tp_size=1, gather_output=False)
hosseins: LlamaAttention.forward() Name: weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaAttention.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.QKVParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([6144, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 6144]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([512, 6144])
hosseins: LlamaAttention.forward() qkv.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=''
hosseins: Attention. forward()
hosseins: Attention -> forward() key.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(key)='' value.shape=torch.Size([512, 1024])
hosseins: Attention -> forward() value.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(value)='' value.shape=torch.Size([512, 1024])
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 4096]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaAttention.forward() output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(residual)=''
hosseins: LlamaMLP.forward()
hosseins: LlamaMLP.forward() x.shape=torch.Size([512, 4096])
hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaMLP.forward() get_shard_spec(x)=''
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([28672, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 28672]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 14336]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 14336]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaModel.forward() ============= type(layer)=<class 'vllm.model_executor.models.llama.LlamaDecoderLayer'> =============
hosseins: LlamaModel.forward() Name: self_attn.qkv_proj.weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: self_attn.o_proj.weight, Shape: param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.gate_up_proj.weight, Shape: param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.down_proj.weight, Shape: param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: input_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaModel.forward() Name: post_attention_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() hidden_states.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaAttention.forward() self.qkv_proj=QKVParallelLinear(in_features=4096, output_features=6144, bias=False, tp_size=1, gather_output=False)
hosseins: LlamaAttention.forward() Name: weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaAttention.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.QKVParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([6144, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 6144]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([512, 6144])
hosseins: LlamaAttention.forward() qkv.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=''
hosseins: Attention. forward()
hosseins: Attention -> forward() key.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(key)='' value.shape=torch.Size([512, 1024])
hosseins: Attention -> forward() value.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(value)='' value.shape=torch.Size([512, 1024])
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 4096]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaAttention.forward() output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(residual)=''
hosseins: LlamaMLP.forward()
hosseins: LlamaMLP.forward() x.shape=torch.Size([512, 4096])
hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaMLP.forward() get_shard_spec(x)=''
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([28672, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 28672]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 14336]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 14336]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaModel.forward() ============= type(layer)=<class 'vllm.model_executor.models.llama.LlamaDecoderLayer'> =============
hosseins: LlamaModel.forward() Name: self_attn.qkv_proj.weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: self_attn.o_proj.weight, Shape: param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.gate_up_proj.weight, Shape: param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.down_proj.weight, Shape: param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: input_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaModel.forward() Name: post_attention_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() hidden_states.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaAttention.forward() self.qkv_proj=QKVParallelLinear(in_features=4096, output_features=6144, bias=False, tp_size=1, gather_output=False)
hosseins: LlamaAttention.forward() Name: weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaAttention.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.QKVParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([6144, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 6144]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([512, 6144])
hosseins: LlamaAttention.forward() qkv.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=''
hosseins: Attention. forward()
hosseins: Attention -> forward() key.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(key)='' value.shape=torch.Size([512, 1024])
hosseins: Attention -> forward() value.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(value)='' value.shape=torch.Size([512, 1024])
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 4096]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaAttention.forward() output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(residual)=''
hosseins: LlamaMLP.forward()
hosseins: LlamaMLP.forward() x.shape=torch.Size([512, 4096])
hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaMLP.forward() get_shard_spec(x)=''
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([28672, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 28672]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 14336]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 14336]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaModel.forward() ============= type(layer)=<class 'vllm.model_executor.models.llama.LlamaDecoderLayer'> =============
hosseins: LlamaModel.forward() Name: self_attn.qkv_proj.weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: self_attn.o_proj.weight, Shape: param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.gate_up_proj.weight, Shape: param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.down_proj.weight, Shape: param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: input_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaModel.forward() Name: post_attention_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() hidden_states.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaAttention.forward() self.qkv_proj=QKVParallelLinear(in_features=4096, output_features=6144, bias=False, tp_size=1, gather_output=False)
hosseins: LlamaAttention.forward() Name: weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaAttention.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.QKVParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([6144, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 6144]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([512, 6144])
hosseins: LlamaAttention.forward() qkv.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=''
hosseins: Attention. forward()
hosseins: Attention -> forward() key.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(key)='' value.shape=torch.Size([512, 1024])
hosseins: Attention -> forward() value.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(value)='' value.shape=torch.Size([512, 1024])
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 4096]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaAttention.forward() output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(residual)=''
hosseins: LlamaMLP.forward()
hosseins: LlamaMLP.forward() x.shape=torch.Size([512, 4096])
hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaMLP.forward() get_shard_spec(x)=''
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([28672, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 28672]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 14336]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 14336]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaModel.forward() ============= type(layer)=<class 'vllm.model_executor.models.llama.LlamaDecoderLayer'> =============
hosseins: LlamaModel.forward() Name: self_attn.qkv_proj.weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: self_attn.o_proj.weight, Shape: param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.gate_up_proj.weight, Shape: param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.down_proj.weight, Shape: param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: input_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaModel.forward() Name: post_attention_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() hidden_states.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaAttention.forward() self.qkv_proj=QKVParallelLinear(in_features=4096, output_features=6144, bias=False, tp_size=1, gather_output=False)
hosseins: LlamaAttention.forward() Name: weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaAttention.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.QKVParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([6144, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 6144]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([512, 6144])
hosseins: LlamaAttention.forward() qkv.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=''
hosseins: Attention. forward()
hosseins: Attention -> forward() key.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(key)='' value.shape=torch.Size([512, 1024])
hosseins: Attention -> forward() value.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(value)='' value.shape=torch.Size([512, 1024])
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 4096]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaAttention.forward() output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(residual)=''
hosseins: LlamaMLP.forward()
hosseins: LlamaMLP.forward() x.shape=torch.Size([512, 4096])
hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaMLP.forward() get_shard_spec(x)=''
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([28672, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 28672]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 14336]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 14336]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaModel.forward() ============= type(layer)=<class 'vllm.model_executor.models.llama.LlamaDecoderLayer'> =============
hosseins: LlamaModel.forward() Name: self_attn.qkv_proj.weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: self_attn.o_proj.weight, Shape: param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.gate_up_proj.weight, Shape: param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.down_proj.weight, Shape: param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: input_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaModel.forward() Name: post_attention_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() hidden_states.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaAttention.forward() self.qkv_proj=QKVParallelLinear(in_features=4096, output_features=6144, bias=False, tp_size=1, gather_output=False)
hosseins: LlamaAttention.forward() Name: weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaAttention.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.QKVParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([6144, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 6144]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([512, 6144])
hosseins: LlamaAttention.forward() qkv.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=''
hosseins: Attention. forward()
hosseins: Attention -> forward() key.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(key)='' value.shape=torch.Size([512, 1024])
hosseins: Attention -> forward() value.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(value)='' value.shape=torch.Size([512, 1024])
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 4096]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaAttention.forward() output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(residual)=''
hosseins: LlamaMLP.forward()
hosseins: LlamaMLP.forward() x.shape=torch.Size([512, 4096])
hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaMLP.forward() get_shard_spec(x)=''
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([28672, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 28672]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 14336]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 14336]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaModel.forward() ============= type(layer)=<class 'vllm.model_executor.models.llama.LlamaDecoderLayer'> =============
hosseins: LlamaModel.forward() Name: self_attn.qkv_proj.weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: self_attn.o_proj.weight, Shape: param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.gate_up_proj.weight, Shape: param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.down_proj.weight, Shape: param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: input_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaModel.forward() Name: post_attention_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() hidden_states.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaAttention.forward() self.qkv_proj=QKVParallelLinear(in_features=4096, output_features=6144, bias=False, tp_size=1, gather_output=False)
hosseins: LlamaAttention.forward() Name: weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaAttention.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.QKVParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([6144, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 6144]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([512, 6144])
hosseins: LlamaAttention.forward() qkv.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=''
hosseins: Attention. forward()
hosseins: Attention -> forward() key.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(key)='' value.shape=torch.Size([512, 1024])
hosseins: Attention -> forward() value.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(value)='' value.shape=torch.Size([512, 1024])
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 4096]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaAttention.forward() output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(residual)=''
hosseins: LlamaMLP.forward()
hosseins: LlamaMLP.forward() x.shape=torch.Size([512, 4096])
hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaMLP.forward() get_shard_spec(x)=''
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([28672, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 28672]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 14336]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 14336]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaModel.forward() ============= type(layer)=<class 'vllm.model_executor.models.llama.LlamaDecoderLayer'> =============
hosseins: LlamaModel.forward() Name: self_attn.qkv_proj.weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: self_attn.o_proj.weight, Shape: param.shape=torch.Size([4096, 4096])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.gate_up_proj.weight, Shape: param.shape=torch.Size([28672, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: LlamaModel.forward() Name: mlp.down_proj.weight, Shape: param.shape=torch.Size([4096, 14336])
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{devices=[1,4]0,1,2,3}'
hosseins: LlamaModel.forward() Name: input_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaModel.forward() Name: post_attention_layernorm.weight, Shape: param.shape=torch.Size([4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaModel.forward() get_shard_spec(param)='{replicated}'
hosseins: LlamaDecoderLayer.forward()
hosseins: LlamaAttention.forward()
hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() hidden_states.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaAttention.forward() self.qkv_proj=QKVParallelLinear(in_features=4096, output_features=6144, bias=False, tp_size=1, gather_output=False)
hosseins: LlamaAttention.forward() Name: weight, Shape: param.shape=torch.Size([6144, 4096])
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: LlamaAttention.forward() get_shard_spec(param)='{devices=[4,1]0,1,2,3}'
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.QKVParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([6144, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 6144]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: LlamaAttention.forward() qkv.shape=torch.Size([512, 6144])
hosseins: LlamaAttention.forward() qkv.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(qkv)=''
hosseins: Attention. forward()
hosseins: Attention -> forward() key.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(key)='' value.shape=torch.Size([512, 1024])
hosseins: Attention -> forward() value.shape=torch.Size([512, 1024])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: Attention -> forward() get_shard_spec(value)='' value.shape=torch.Size([512, 1024])
hosseins: PallasAttentionBackendImpl.forward()
hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 4096]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaAttention.forward() output.shape=torch.Size([512, 4096])
hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(residual)=''
hosseins: LlamaMLP.forward()
hosseins: LlamaMLP.forward() x.shape=torch.Size([512, 4096])
hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaMLP.forward() get_shard_spec(x)=''
hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
└───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([28672, 4096]) get_shard_spec(layer.weight)='{devices=[4,1]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 28672]) get_shard_spec(out)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=''
hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
└───────┴───────┴───────┴───────┘
hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 14336]) get_shard_spec(layer.weight)='{devices=[1,4]0,1,2,3}'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 14336]) get_shard_spec(x)=''
hosseins: UnquantizedLinearMethod -> apply() name='weight'
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=''
hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=''
hosseins: LlamaModel.forward() layer end
hosseins: LlamaModel.forward() norm start
hosseins: LlamaModel.forward() norm end
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ModelWrapperV1 -> forward() 2 get_shard_spec(hidden_states)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ModelWrapperV1 -> forward() 2 get_shard_spec(input_ids)=''
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ModelWrapperV1 -> forward() 2 get_shard_spec(positions)=''
hosseins: ModelWrapperV1 -> compute_logits() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ModelWrapperV1 -> compute_logits() get_shard_spec(hidden_states)=''
hosseins: ModelWrapperV1 -> compute_logits() logits_indices.shape=torch.Size([64])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ModelWrapperV1 -> compute_logits() get_shard_spec(logits_indices)=''
hosseins: UnquantizedEmbeddingMethod -> apply()
hosseins: UnquantizedEmbeddingMethod -> apply() [layer.weight.shape=torch.Size([128256, 4096])]
hosseins: UnquantizedEmbeddingMethod -> apply() [layer.weight.device=device(type='xla', index=0)]
hosseins: ModelWrapperV1 -> compute_logits() hidden_states.shape=torch.Size([512, 4096])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ModelWrapperV1 -> compute_logits() get_shard_spec(hidden_states)=''
hosseins: ModelWrapperV1 -> compute_logits() logits_indices.shape=torch.Size([128])
┌──────────────────┐
│                  │
│ TPU [0, 1, 2, 3] │
│                  │
└──────────────────┘
hosseins: ModelWrapperV1 -> compute_logits() get_shard_spec(logits_indices)=''
hosseins: UnquantizedEmbeddingMethod -> apply()
hosseins: UnquantizedEmbeddingMethod -> apply() [layer.weight.shape=torch.Size([128256, 4096])]
hosseins: UnquantizedEmbeddingMethod -> apply() [layer.weight.device=device(type='xla', index=0)]
hosseins: _initialize_kv_caches() available_gpu_memory=9413680640
hosseins: get_kv_cache_configs() num_layers=32
hosseins: get_kv_cache_configs() len(kv_cache_specs)=1
hosseins: get_kv_cache_configs() kv_cache_specs=[{'model.layers.0.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.1.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.2.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.3.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.4.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.5.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.6.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.7.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.8.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.9.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.10.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.11.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.12.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.13.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.14.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.15.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.16.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.17.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.18.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.19.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.20.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.21.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.22.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.23.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.24.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.25.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.26.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.27.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.28.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.29.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.30.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.31.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False)}]
hosseins: page_sizes={65536}
hosseins: page_size=65536
hosseins: available_memory=9413680640
hosseins: num_blocks=4488
hosseins: num_blocks=4488
hosseins: vllm_config.cache_config.num_gpu_blocks_override=None
hosseins: vllm_config.cache_config.block_size=16
hosseins: num_tokens=71808
INFO 03-19 15:56:31 [kv_cache_utils.py:547] GPU KV cache size: 71,808 tokens
INFO 03-19 15:56:31 [kv_cache_utils.py:550] Maximum concurrency for 512 tokens per request: 140.25x
hosseins: _initialize_kv_caches() kv_cache_configs=[KVCacheConfig(num_blocks=4488, tensors={'model.layers.0.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.1.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.2.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.3.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.4.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.5.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.6.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.7.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.8.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.9.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.10.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.11.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.12.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.13.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.14.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.15.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.16.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.17.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.18.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.19.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.20.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.21.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.22.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.23.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.24.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.25.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.26.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.27.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.28.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.29.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.30.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.31.self_attn.attn': KVCacheTensor(size=294125568)}, groups=[['model.layers.0.self_attn.attn', 'model.layers.1.self_attn.attn', 'model.layers.2.self_attn.attn', 'model.layers.3.self_attn.attn', 'model.layers.4.self_attn.attn', 'model.layers.5.self_attn.attn', 'model.layers.6.self_attn.attn', 'model.layers.7.self_attn.attn', 'model.layers.8.self_attn.attn', 'model.layers.9.self_attn.attn', 'model.layers.10.self_attn.attn', 'model.layers.11.self_attn.attn', 'model.layers.12.self_attn.attn', 'model.layers.13.self_attn.attn', 'model.layers.14.self_attn.attn', 'model.layers.15.self_attn.attn', 'model.layers.16.self_attn.attn', 'model.layers.17.self_attn.attn', 'model.layers.18.self_attn.attn', 'model.layers.19.self_attn.attn', 'model.layers.20.self_attn.attn', 'model.layers.21.self_attn.attn', 'model.layers.22.self_attn.attn', 'model.layers.23.self_attn.attn', 'model.layers.24.self_attn.attn', 'model.layers.25.self_attn.attn', 'model.layers.26.self_attn.attn', 'model.layers.27.self_attn.attn', 'model.layers.28.self_attn.attn', 'model.layers.29.self_attn.attn', 'model.layers.30.self_attn.attn', 'model.layers.31.self_attn.attn']], kv_cache_spec={'model.layers.0.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.1.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.2.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.3.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.4.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.5.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.6.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.7.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.8.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.9.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.10.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.11.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.12.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.13.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.14.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.15.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.16.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.17.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.18.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.19.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.20.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.21.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.22.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.23.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.24.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.25.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.26.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.27.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.28.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.29.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.30.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.31.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False)})]
hosseins: _initialize_kv_caches() num_gpu_blocks_set={4488}
hosseins: _initialize_kv_caches() num_gpu_blocks=4488
hosseins: initialize_from_config() kv_cache_config=KVCacheConfig(num_blocks=4488, tensors={'model.layers.0.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.1.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.2.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.3.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.4.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.5.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.6.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.7.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.8.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.9.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.10.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.11.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.12.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.13.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.14.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.15.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.16.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.17.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.18.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.19.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.20.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.21.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.22.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.23.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.24.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.25.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.26.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.27.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.28.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.29.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.30.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.31.self_attn.attn': KVCacheTensor(size=294125568)}, groups=[['model.layers.0.self_attn.attn', 'model.layers.1.self_attn.attn', 'model.layers.2.self_attn.attn', 'model.layers.3.self_attn.attn', 'model.layers.4.self_attn.attn', 'model.layers.5.self_attn.attn', 'model.layers.6.self_attn.attn', 'model.layers.7.self_attn.attn', 'model.layers.8.self_attn.attn', 'model.layers.9.self_attn.attn', 'model.layers.10.self_attn.attn', 'model.layers.11.self_attn.attn', 'model.layers.12.self_attn.attn', 'model.layers.13.self_attn.attn', 'model.layers.14.self_attn.attn', 'model.layers.15.self_attn.attn', 'model.layers.16.self_attn.attn', 'model.layers.17.self_attn.attn', 'model.layers.18.self_attn.attn', 'model.layers.19.self_attn.attn', 'model.layers.20.self_attn.attn', 'model.layers.21.self_attn.attn', 'model.layers.22.self_attn.attn', 'model.layers.23.self_attn.attn', 'model.layers.24.self_attn.attn', 'model.layers.25.self_attn.attn', 'model.layers.26.self_attn.attn', 'model.layers.27.self_attn.attn', 'model.layers.28.self_attn.attn', 'model.layers.29.self_attn.attn', 'model.layers.30.self_attn.attn', 'model.layers.31.self_attn.attn']], kv_cache_spec={'model.layers.0.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.1.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.2.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.3.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.4.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.5.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.6.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.7.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.8.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.9.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.10.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.11.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.12.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.13.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.14.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.15.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.16.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.17.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.18.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.19.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.20.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.21.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.22.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.23.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.24.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.25.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.26.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.27.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.28.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.29.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.30.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.31.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False)})
hosseins: initialize_kv_cache() kv_cache_config=KVCacheConfig(num_blocks=4488, tensors={'model.layers.0.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.1.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.2.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.3.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.4.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.5.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.6.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.7.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.8.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.9.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.10.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.11.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.12.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.13.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.14.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.15.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.16.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.17.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.18.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.19.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.20.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.21.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.22.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.23.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.24.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.25.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.26.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.27.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.28.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.29.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.30.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.31.self_attn.attn': KVCacheTensor(size=294125568)}, groups=[['model.layers.0.self_attn.attn', 'model.layers.1.self_attn.attn', 'model.layers.2.self_attn.attn', 'model.layers.3.self_attn.attn', 'model.layers.4.self_attn.attn', 'model.layers.5.self_attn.attn', 'model.layers.6.self_attn.attn', 'model.layers.7.self_attn.attn', 'model.layers.8.self_attn.attn', 'model.layers.9.self_attn.attn', 'model.layers.10.self_attn.attn', 'model.layers.11.self_attn.attn', 'model.layers.12.self_attn.attn', 'model.layers.13.self_attn.attn', 'model.layers.14.self_attn.attn', 'model.layers.15.self_attn.attn', 'model.layers.16.self_attn.attn', 'model.layers.17.self_attn.attn', 'model.layers.18.self_attn.attn', 'model.layers.19.self_attn.attn', 'model.layers.20.self_attn.attn', 'model.layers.21.self_attn.attn', 'model.layers.22.self_attn.attn', 'model.layers.23.self_attn.attn', 'model.layers.24.self_attn.attn', 'model.layers.25.self_attn.attn', 'model.layers.26.self_attn.attn', 'model.layers.27.self_attn.attn', 'model.layers.28.self_attn.attn', 'model.layers.29.self_attn.attn', 'model.layers.30.self_attn.attn', 'model.layers.31.self_attn.attn']], kv_cache_spec={'model.layers.0.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.1.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.2.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.3.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.4.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.5.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.6.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.7.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.8.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.9.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.10.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.11.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.12.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.13.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.14.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.15.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.16.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.17.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.18.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.19.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.20.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.21.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.22.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.23.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.24.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.25.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.26.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.27.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.28.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.29.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.30.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.31.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False)})
hosseins: len(kv_cache_config.kv_cache_spec)=32
hosseins: kv_cache_config.num_blocks=4488
hosseins: len(kv_cache_config.tensors)=32
hosseins: len(kv_cache_config.groups)=1
hosseins: len(kv_cache_config.kv_cache_spec.items())=32
hosseins: ============================= model.layers.0.self_attn.attn =============================
hosseins: num_blocks=4488
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: dtype=torch.bfloat16
hosseins: tpu_k_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_k_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_k_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: tpu_v_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_v_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_v_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: ============================= model.layers.1.self_attn.attn =============================
hosseins: num_blocks=4488
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: dtype=torch.bfloat16
hosseins: tpu_k_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_k_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_k_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: tpu_v_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_v_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_v_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: ============================= model.layers.2.self_attn.attn =============================
hosseins: num_blocks=4488
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: dtype=torch.bfloat16
hosseins: tpu_k_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_k_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_k_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: tpu_v_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_v_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_v_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: ============================= model.layers.3.self_attn.attn =============================
hosseins: num_blocks=4488
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: dtype=torch.bfloat16
hosseins: tpu_k_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_k_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_k_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: tpu_v_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_v_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_v_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: ============================= model.layers.4.self_attn.attn =============================
hosseins: num_blocks=4488
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: dtype=torch.bfloat16
hosseins: tpu_k_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_k_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_k_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: tpu_v_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_v_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_v_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: ============================= model.layers.5.self_attn.attn =============================
hosseins: num_blocks=4488
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: dtype=torch.bfloat16
hosseins: tpu_k_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_k_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_k_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: tpu_v_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_v_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_v_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: ============================= model.layers.6.self_attn.attn =============================
hosseins: num_blocks=4488
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: dtype=torch.bfloat16
hosseins: tpu_k_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_k_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_k_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: tpu_v_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_v_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_v_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: ============================= model.layers.7.self_attn.attn =============================
hosseins: num_blocks=4488
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: dtype=torch.bfloat16
hosseins: tpu_k_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_k_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_k_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: tpu_v_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_v_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_v_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: ============================= model.layers.8.self_attn.attn =============================
hosseins: num_blocks=4488
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: dtype=torch.bfloat16
hosseins: tpu_k_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_k_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_k_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: tpu_v_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_v_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_v_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: ============================= model.layers.9.self_attn.attn =============================
hosseins: num_blocks=4488
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: dtype=torch.bfloat16
hosseins: tpu_k_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_k_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_k_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: tpu_v_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_v_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_v_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: ============================= model.layers.10.self_attn.attn =============================
hosseins: num_blocks=4488
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: dtype=torch.bfloat16
hosseins: tpu_k_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_k_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_k_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: tpu_v_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_v_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_v_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: ============================= model.layers.11.self_attn.attn =============================
hosseins: num_blocks=4488
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: dtype=torch.bfloat16
hosseins: tpu_k_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_k_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_k_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: tpu_v_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_v_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_v_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: ============================= model.layers.12.self_attn.attn =============================
hosseins: num_blocks=4488
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: dtype=torch.bfloat16
hosseins: tpu_k_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_k_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_k_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: tpu_v_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_v_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_v_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: ============================= model.layers.13.self_attn.attn =============================
hosseins: num_blocks=4488
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: dtype=torch.bfloat16
hosseins: tpu_k_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_k_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_k_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: tpu_v_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_v_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_v_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: ============================= model.layers.14.self_attn.attn =============================
hosseins: num_blocks=4488
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: dtype=torch.bfloat16
hosseins: tpu_k_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_k_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_k_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: tpu_v_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_v_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_v_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: ============================= model.layers.15.self_attn.attn =============================
hosseins: num_blocks=4488
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: dtype=torch.bfloat16
hosseins: tpu_k_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_k_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_k_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: tpu_v_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_v_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_v_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: ============================= model.layers.16.self_attn.attn =============================
hosseins: num_blocks=4488
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: dtype=torch.bfloat16
hosseins: tpu_k_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_k_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_k_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: tpu_v_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_v_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_v_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: ============================= model.layers.17.self_attn.attn =============================
hosseins: num_blocks=4488
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: dtype=torch.bfloat16
hosseins: tpu_k_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_k_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_k_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: tpu_v_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_v_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_v_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: ============================= model.layers.18.self_attn.attn =============================
hosseins: num_blocks=4488
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: dtype=torch.bfloat16
hosseins: tpu_k_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_k_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_k_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: tpu_v_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_v_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_v_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: ============================= model.layers.19.self_attn.attn =============================
hosseins: num_blocks=4488
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: dtype=torch.bfloat16
hosseins: tpu_k_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_k_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_k_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: tpu_v_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_v_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_v_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: ============================= model.layers.20.self_attn.attn =============================
hosseins: num_blocks=4488
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: dtype=torch.bfloat16
hosseins: tpu_k_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_k_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_k_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: tpu_v_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_v_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_v_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: ============================= model.layers.21.self_attn.attn =============================
hosseins: num_blocks=4488
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: dtype=torch.bfloat16
hosseins: tpu_k_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_k_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_k_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: tpu_v_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_v_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_v_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: ============================= model.layers.22.self_attn.attn =============================
hosseins: num_blocks=4488
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: dtype=torch.bfloat16
hosseins: tpu_k_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_k_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_k_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: tpu_v_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_v_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_v_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: ============================= model.layers.23.self_attn.attn =============================
hosseins: num_blocks=4488
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: dtype=torch.bfloat16
hosseins: tpu_k_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_k_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_k_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: tpu_v_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_v_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_v_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: ============================= model.layers.24.self_attn.attn =============================
hosseins: num_blocks=4488
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: dtype=torch.bfloat16
hosseins: tpu_k_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_k_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_k_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: tpu_v_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_v_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_v_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: ============================= model.layers.25.self_attn.attn =============================
hosseins: num_blocks=4488
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: dtype=torch.bfloat16
hosseins: tpu_k_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_k_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_k_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: tpu_v_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_v_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_v_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: ============================= model.layers.26.self_attn.attn =============================
hosseins: num_blocks=4488
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: dtype=torch.bfloat16
hosseins: tpu_k_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_k_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_k_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: tpu_v_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_v_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_v_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: ============================= model.layers.27.self_attn.attn =============================
hosseins: num_blocks=4488
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: dtype=torch.bfloat16
hosseins: tpu_k_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_k_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_k_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: tpu_v_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_v_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_v_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: ============================= model.layers.28.self_attn.attn =============================
hosseins: num_blocks=4488
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: dtype=torch.bfloat16
hosseins: tpu_k_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_k_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_k_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: tpu_v_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_v_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_v_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: ============================= model.layers.29.self_attn.attn =============================
hosseins: num_blocks=4488
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: dtype=torch.bfloat16
hosseins: tpu_k_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_k_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_k_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: tpu_v_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_v_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_v_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: ============================= model.layers.30.self_attn.attn =============================
hosseins: num_blocks=4488
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: dtype=torch.bfloat16
hosseins: tpu_k_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_k_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_k_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: tpu_v_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_v_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_v_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: ============================= model.layers.31.self_attn.attn =============================
hosseins: num_blocks=4488
hosseins: layer_spec.block_size=16
hosseins: layer_spec.num_kv_heads=8
hosseins: layer_spec.head_size=128
hosseins: dtype=torch.bfloat16
hosseins: tpu_k_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_k_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_k_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: tpu_v_cache.shape=torch.Size([4488, 16, 8, 128])
hosseins: tpu_v_cache.device=device(type='xla', index=0)
┌───────┐
│       │
│ TPU 0 │
│       │
└───────┘
hosseins: get_shard_spec(tpu_v_cache)='{devices=[1,1,4,1]0,1,2,3}'
hosseins: initialize_kv_cache() kv_cache_config=KVCacheConfig(num_blocks=4488, tensors={'model.layers.0.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.1.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.2.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.3.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.4.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.5.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.6.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.7.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.8.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.9.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.10.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.11.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.12.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.13.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.14.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.15.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.16.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.17.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.18.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.19.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.20.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.21.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.22.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.23.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.24.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.25.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.26.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.27.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.28.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.29.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.30.self_attn.attn': KVCacheTensor(size=294125568), 'model.layers.31.self_attn.attn': KVCacheTensor(size=294125568)}, groups=[['model.layers.0.self_attn.attn', 'model.layers.1.self_attn.attn', 'model.layers.2.self_attn.attn', 'model.layers.3.self_attn.attn', 'model.layers.4.self_attn.attn', 'model.layers.5.self_attn.attn', 'model.layers.6.self_attn.attn', 'model.layers.7.self_attn.attn', 'model.layers.8.self_attn.attn', 'model.layers.9.self_attn.attn', 'model.layers.10.self_attn.attn', 'model.layers.11.self_attn.attn', 'model.layers.12.self_attn.attn', 'model.layers.13.self_attn.attn', 'model.layers.14.self_attn.attn', 'model.layers.15.self_attn.attn', 'model.layers.16.self_attn.attn', 'model.layers.17.self_attn.attn', 'model.layers.18.self_attn.attn', 'model.layers.19.self_attn.attn', 'model.layers.20.self_attn.attn', 'model.layers.21.self_attn.attn', 'model.layers.22.self_attn.attn', 'model.layers.23.self_attn.attn', 'model.layers.24.self_attn.attn', 'model.layers.25.self_attn.attn', 'model.layers.26.self_attn.attn', 'model.layers.27.self_attn.attn', 'model.layers.28.self_attn.attn', 'model.layers.29.self_attn.attn', 'model.layers.30.self_attn.attn', 'model.layers.31.self_attn.attn']], kv_cache_spec={'model.layers.0.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.1.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.2.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.3.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.4.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.5.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.6.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.7.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.8.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.9.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.10.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.11.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.12.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.13.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.14.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.15.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.16.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.17.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.18.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.19.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.20.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.21.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.22.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.23.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.24.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.25.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.26.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.27.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.28.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.29.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.30.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.31.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=8, head_size=128, dtype=torch.bfloat16, use_mla=False)})
INFO 03-19 15:57:04 [core.py:128] init engine (profile, create kv cache, warmup model) took 33.91 seconds
INFO 03-19 15:57:05 [serving_chat.py:114] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
INFO 03-19 15:57:05 [serving_completion.py:60] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
INFO 03-19 15:57:05 [api_server.py:958] Starting vLLM API server on http://0.0.0.0:8000
INFO 03-19 15:57:05 [launcher.py:26] Available routes are:
INFO 03-19 15:57:05 [launcher.py:34] Route: /openapi.json, Methods: GET, HEAD
INFO 03-19 15:57:05 [launcher.py:34] Route: /docs, Methods: GET, HEAD
INFO 03-19 15:57:05 [launcher.py:34] Route: /docs/oauth2-redirect, Methods: GET, HEAD
INFO 03-19 15:57:05 [launcher.py:34] Route: /redoc, Methods: GET, HEAD
INFO 03-19 15:57:05 [launcher.py:34] Route: /health, Methods: GET
INFO 03-19 15:57:05 [launcher.py:34] Route: /ping, Methods: GET, POST
INFO 03-19 15:57:05 [launcher.py:34] Route: /tokenize, Methods: POST
INFO 03-19 15:57:05 [launcher.py:34] Route: /detokenize, Methods: POST
INFO 03-19 15:57:05 [launcher.py:34] Route: /v1/models, Methods: GET
INFO 03-19 15:57:05 [launcher.py:34] Route: /version, Methods: GET
INFO 03-19 15:57:05 [launcher.py:34] Route: /v1/chat/completions, Methods: POST
INFO 03-19 15:57:05 [launcher.py:34] Route: /v1/completions, Methods: POST
INFO 03-19 15:57:05 [launcher.py:34] Route: /v1/embeddings, Methods: POST
INFO 03-19 15:57:05 [launcher.py:34] Route: /pooling, Methods: POST
INFO 03-19 15:57:05 [launcher.py:34] Route: /score, Methods: POST
INFO 03-19 15:57:05 [launcher.py:34] Route: /v1/score, Methods: POST
INFO 03-19 15:57:05 [launcher.py:34] Route: /v1/audio/transcriptions, Methods: POST
INFO 03-19 15:57:05 [launcher.py:34] Route: /rerank, Methods: POST
INFO 03-19 15:57:05 [launcher.py:34] Route: /v1/rerank, Methods: POST
INFO 03-19 15:57:05 [launcher.py:34] Route: /v2/rerank, Methods: POST
INFO 03-19 15:57:05 [launcher.py:34] Route: /invocations, Methods: POST
INFO 03-19 15:57:05 [launcher.py:34] Route: /start_profile, Methods: POST
INFO 03-19 15:57:05 [launcher.py:34] Route: /stop_profile, Methods: POST
INFO:     Started server process [518320]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO 03-19 15:57:15 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 15:57:25 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 15:57:35 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 15:57:45 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 15:57:55 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 15:58:05 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 15:58:15 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 15:58:25 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 15:58:35 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 15:58:45 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 15:58:55 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 15:59:05 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 15:59:15 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 15:59:25 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 15:59:35 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 15:59:45 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 15:59:55 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:00:05 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:00:15 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:00:25 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:00:35 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:00:45 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:00:55 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:01:05 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:01:15 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:01:25 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:01:35 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:01:45 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:01:55 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:02:05 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:02:15 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:02:25 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:02:35 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:02:45 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:02:55 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:03:05 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:03:15 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:03:25 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:03:35 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:03:45 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:03:55 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:04:05 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:04:15 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:04:25 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:04:35 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:04:45 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:04:55 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:05:05 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:05:15 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:05:25 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:05:35 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:05:45 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:05:55 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:06:05 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:06:15 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:06:25 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:06:35 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:06:45 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:06:55 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:07:05 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:07:15 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:07:25 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:07:35 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:07:45 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:07:55 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:08:05 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:08:15 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:08:25 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:08:35 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:08:45 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:08:55 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:09:05 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:09:15 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:09:25 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:09:35 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:09:45 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:09:55 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:10:05 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:10:15 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:10:25 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:10:35 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:10:45 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:10:55 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:11:05 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:11:15 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:11:25 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:11:35 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:11:45 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:11:55 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:12:05 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:12:15 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:12:25 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:12:35 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:12:45 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:12:55 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:13:05 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:13:15 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:13:25 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:13:35 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:13:45 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:13:55 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:14:05 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:14:15 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:14:25 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:14:35 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:14:45 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:14:55 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:15:05 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:15:15 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:15:25 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:15:35 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:15:45 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:15:55 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:16:05 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:16:15 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:16:25 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:16:35 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:16:45 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:16:55 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:17:05 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:17:15 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:17:25 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:17:35 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:17:45 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:17:55 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:18:05 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:18:15 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:18:25 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:18:35 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:18:45 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:18:55 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:19:05 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:19:15 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:19:25 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:19:35 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:19:45 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:19:55 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:20:05 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:20:15 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:20:25 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:20:35 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:20:45 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:20:55 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:21:05 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:21:15 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:21:25 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:21:35 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:21:45 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:21:55 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:22:05 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:22:15 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:22:25 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:22:35 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:22:45 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:22:55 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:23:05 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:23:15 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:23:25 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:23:35 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:23:45 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:23:55 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:24:05 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:24:15 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:24:25 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:24:35 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:24:45 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:24:55 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:25:05 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:25:15 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:25:25 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:25:35 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:25:45 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:25:55 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:26:05 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:26:15 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:26:25 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:26:35 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:26:45 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:26:55 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:27:05 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:27:15 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:27:25 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:27:35 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:27:45 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:27:55 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:28:05 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:28:15 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:28:25 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:28:35 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:28:45 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:28:55 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:29:05 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:29:15 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:29:25 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:29:35 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:29:45 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:29:55 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:30:05 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:30:15 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:30:25 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:30:35 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:30:45 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:30:55 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:31:05 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:31:15 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:31:25 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:31:35 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:31:45 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:31:55 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:32:05 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:32:15 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:32:25 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:32:35 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:32:45 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:32:55 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:33:05 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:33:15 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:33:25 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:33:35 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:33:45 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:33:55 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:34:05 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:34:15 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:34:25 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:34:35 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:34:45 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:34:55 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:35:05 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:35:15 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:35:25 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:35:35 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:35:45 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:35:55 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:36:05 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:36:15 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:36:25 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:36:35 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:36:45 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:36:55 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:37:05 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:37:15 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:37:25 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:37:35 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:37:45 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:37:55 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:38:05 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:38:15 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:38:25 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:38:35 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:38:45 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:38:55 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:39:05 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:39:15 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:39:25 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:39:35 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:39:45 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:39:55 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:40:05 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:40:15 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:40:25 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:40:35 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:40:45 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:40:55 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:41:05 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:41:15 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:41:25 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:41:35 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:41:45 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:41:55 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 03-19 16:42:05 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
