nohup: ignoring input
WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.
INFO 03-18 16:19:31 [__init__.py:256] Automatically detected platform tpu.
INFO 03-18 16:19:32 [api_server.py:912] vLLM API server version 0.7.4.dev339+ga21076ed.d20250312
INFO 03-18 16:19:32 [api_server.py:913] args: Namespace(subparser='serve', model_tag='meta-llama/Meta-Llama-3.1-8B', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='meta-llama/Meta-Llama-3.1-8B', task='auto', tokenizer=None, hf_config_path=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir='/dev/shm', load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=512, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=4, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=None, swap_space=16.0, cpu_offload_gb=0, gpu_memory_utilization=0.5, num_gpu_blocks_override=None, max_num_batched_tokens=512, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=128, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, use_tqdm_on_load=True, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', worker_extension_cls='', generation_config='auto', override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, enable_reasoning=False, reasoning_parser=None, disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function ServeSubcommand.cmd at 0x7de07b5de830>)
INFO 03-18 16:19:38 [config.py:576] This model supports multiple tasks: {'generate', 'embed', 'reward', 'score', 'classify'}. Defaulting to 'generate'.
INFO 03-18 16:19:38 [config.py:1666] Chunked prefill is enabled with max_num_batched_tokens=512.
INFO 03-18 16:19:38 [tpu.py:76] [TPU] Forcing DYNAMO_ONCE compilation level
WARNING 03-18 16:19:38 [tpu.py:108] [V1][TPU] Disable prefix caching
INFO 03-18 16:19:44 [__init__.py:256] Automatically detected platform tpu.
INFO 03-18 16:19:44 [core.py:51] Initializing a V1 LLM engine (v0.7.4.dev339+ga21076ed.d20250312) with config: model='meta-llama/Meta-Llama-3.1-8B', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=512, download_dir='/dev/shm', load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=None, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Meta-Llama-3.1-8B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":2,"backend":"openxla","custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
2025-03-18 16:19:46,201	INFO worker.py:1832 -- Started a local Ray instance. View the dashboard at [1m[32m127.0.0.1:8265 [39m[22m
INFO 03-18 16:19:47 [ray_distributed_executor.py:176] use_ray_spmd_worker: True
hosseins: _init_workers_ray() len(bundle_indices)=4
[36m(pid=126665)[0m INFO 03-18 16:19:51 [__init__.py:256] Automatically detected platform tpu.
hosseins: _init_workers_ray() worker_ips=['10.130.0.78', '10.130.0.78', '10.130.0.78', '10.130.0.78']
hosseins: _init_workers_ray() worker_metadata=[RayWorkerMetaData(worker=Actor(RayWorkerWrapper, b035717b808d0ea8eef4946601000000), created_rank=0, adjusted_rank=-1, ip='10.130.0.78'), RayWorkerMetaData(worker=Actor(RayWorkerWrapper, e4b38f09ae5252940d5a08fe01000000), created_rank=1, adjusted_rank=-1, ip='10.130.0.78'), RayWorkerMetaData(worker=Actor(RayWorkerWrapper, ee6eb30bf20af0bbe247b09801000000), created_rank=2, adjusted_rank=-1, ip='10.130.0.78'), RayWorkerMetaData(worker=Actor(RayWorkerWrapper, 04a14f2ce4954479c2c2dd2901000000), created_rank=3, adjusted_rank=-1, ip='10.130.0.78')]
hosseins: _init_workers_ray() self.workers=[Actor(RayWorkerWrapper, b035717b808d0ea8eef4946601000000), Actor(RayWorkerWrapper, e4b38f09ae5252940d5a08fe01000000), Actor(RayWorkerWrapper, ee6eb30bf20af0bbe247b09801000000), Actor(RayWorkerWrapper, 04a14f2ce4954479c2c2dd2901000000)]
hosseins: _init_workers_ray() len(self.workers)=4
[36m(RayWorkerWrapper pid=126665)[0m hosseins: get_node_and_gpu_ids() device_key='TPU'
[36m(RayWorkerWrapper pid=126665)[0m hosseins: get_node_and_gpu_ids() node_id='3fc259e342d765ec430c594bc68018d41c5a653ce8e6c1404e770c99'
[36m(RayWorkerWrapper pid=126665)[0m hosseins: get_node_and_gpu_ids() gpu_ids=['0']
hosseins: _init_workers_ray() node_gpus=defaultdict(<class 'list'>, {'3fc259e342d765ec430c594bc68018d41c5a653ce8e6c1404e770c99': [0, 1, 2, 3]})
hosseins: _init_workers_ray() len(node_workers)=1
hosseins: _init_workers_ray() all_args_to_update_environment_variables=[{'TPU_VISIBLE_CHIPS': '0,1,2,3'}, {'TPU_VISIBLE_CHIPS': '0,1,2,3'}, {'TPU_VISIBLE_CHIPS': '0,1,2,3'}, {'TPU_VISIBLE_CHIPS': '0,1,2,3'}]
INFO 03-18 16:19:51 [ray_distributed_executor.py:358] non_carry_over_env_vars from config: set()
INFO 03-18 16:19:51 [ray_distributed_executor.py:360] Copying the following environment variables to workers: ['LD_LIBRARY_PATH', 'VLLM_USE_RAY_SPMD_WORKER', 'VLLM_USE_RAY_COMPILED_DAG', 'VLLM_USE_RAY_COMPILED_DAG_NCCL_CHANNEL', 'VLLM_WORKER_MULTIPROC_METHOD', 'VLLM_USE_V1']
INFO 03-18 16:19:51 [ray_distributed_executor.py:363] If certain env vars should NOT be copied to workers, add them to /home/hosseins/.config/vllm/ray_non_carry_over_env_vars.json file
[36m(RayWorkerWrapper pid=126665)[0m Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[36m(RayWorkerWrapper pid=126665)[0m WARNING 03-18 16:19:51 [utils.py:577] Overwriting environment variable TPU_VISIBLE_CHIPS from '0' to '0,1,2,3'
[36m(RayWorkerWrapper pid=126665)[0m WARNING 03-18 16:19:51 [utils.py:577] Overwriting environment variable LD_LIBRARY_PATH from '/home/hosseins/miniconda3/envs/vllm-v1/lib/python3.10/site-packages/cv2/../../lib64:/home/hosseins/miniconda3/envs/vllm-v1/lib/python3.10/site-packages/cv2/../../lib64:/home/hosseins/miniconda3/envs/vllm-v1/lib/python3.10/site-packages/cv2/../../lib64:' to '/home/hosseins/miniconda3/envs/vllm-v1/lib/python3.10/site-packages/cv2/../../lib64:/home/hosseins/miniconda3/envs/vllm-v1/lib/python3.10/site-packages/cv2/../../lib64:'
[36m(RayWorkerWrapper pid=126665)[0m hosseins: init_worker() self.vllm_config.parallel_config.worker_cls='vllm.v1.worker.tpu_worker.TPUWorker'
[36m(RayWorkerWrapper pid=126665)[0m hosseins: initialize_model_parallel() world_size=4
[36m(RayWorkerWrapper pid=126665)[0m hosseins: initialize_model_parallel() all_ranks=tensor([[[0, 1, 2, 3]]])
[36m(RayWorkerWrapper pid=126665)[0m INFO 03-18 16:19:59 [shm_broadcast.py:258] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_ea44b6bd'), local_subscribe_addr='ipc:///dev/shm/ray/960aadef-10c6-4be1-a2f2-ef5467b292b9', remote_subscribe_addr=None, remote_addr_ipv6=False)
[36m(pid=126653)[0m INFO 03-18 16:19:51 [__init__.py:256] Automatically detected platform tpu.[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(RayWorkerWrapper pid=126651)[0m hosseins: get_node_and_gpu_ids() device_key='TPU'[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=126651)[0m hosseins: get_node_and_gpu_ids() gpu_ids=['3'][32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m WARNING 03-18 16:19:51 [utils.py:577] Overwriting environment variable TPU_VISIBLE_CHIPS from '2' to '0,1,2,3'[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m WARNING 03-18 16:19:51 [utils.py:577] Overwriting environment variable LD_LIBRARY_PATH from '/home/hosseins/miniconda3/envs/vllm-v1/lib/python3.10/site-packages/cv2/../../lib64:/home/hosseins/miniconda3/envs/vllm-v1/lib/python3.10/site-packages/cv2/../../lib64:/home/hosseins/miniconda3/envs/vllm-v1/lib/python3.10/site-packages/cv2/../../lib64:' to '/home/hosseins/miniconda3/envs/vllm-v1/lib/python3.10/site-packages/cv2/../../lib64:/home/hosseins/miniconda3/envs/vllm-v1/lib/python3.10/site-packages/cv2/../../lib64:'[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: init_worker() self.vllm_config.parallel_config.worker_cls='vllm.v1.worker.tpu_worker.TPUWorker'[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: initialize_model_parallel() all_ranks=tensor([[[0, 1, 2, 3]]])[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m INFO 03-18 16:19:59 [parallel_state.py:950] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0
[36m(RayWorkerWrapper pid=126665)[0m WARNING 03-18 16:19:59 [tpu.py:116] Pin memory is not supported on TPU.
[36m(RayWorkerWrapper pid=126665)[0m hosseins: get_model_loader() load_config.load_format=<LoadFormat.AUTO: 'auto'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention. init()
[36m(RayWorkerWrapper pid=126665)[0m INFO 03-18 16:19:59 [tpu.py:39] Cannot use None backend on TPU.
[36m(RayWorkerWrapper pid=126665)[0m INFO 03-18 16:19:59 [tpu.py:42] Using Pallas V1 backend.
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention. init()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention. init()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention. init()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention. init()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention. init()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention. init()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention. init()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention. init()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention. init()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention. init()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention. init()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention. init()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention. init()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention. init()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention. init()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention. init()
[36m(RayWorkerWrapper pid=126653)[0m WARNING 03-18 16:20:00 [topk_topp_sampler.py:46] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[36m(RayWorkerWrapper pid=126665)[0m INFO 03-18 16:20:00 [weight_utils.py:258] Using model weights format ['*.safetensors']
[36m(RayWorkerWrapper pid=126653)[0m hosseins: VocabParallelEmbedding -> weight_loader() [start_idx=32064]
[36m(RayWorkerWrapper pid=126653)[0m hosseins: VocabParallelEmbedding -> weight_loader() [shard_size=32064]
[36m(RayWorkerWrapper pid=126653)[0m hosseins: DefaultModelLoader -> load_weights() name='layers.31.input_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: DefaultModelLoader -> load_weights() name='layers.31.mlp.down_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1248] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1249] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1250] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 3584])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 3584])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 3584])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 3584])]
[36m(RayWorkerWrapper pid=126653)[0m hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: DefaultModelLoader -> load_weights() name='layers.31.post_attention_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: DefaultModelLoader -> load_weights() name='norm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: DefaultModelLoader -> load_weights() name='layers.20.input_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: DefaultModelLoader -> load_weights() name='layers.20.mlp.down_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1248] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1249] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1250] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 3584])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 3584])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 3584])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 3584])]
[36m(RayWorkerWrapper pid=126653)[0m hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: DefaultModelLoader -> load_weights() name='layers.20.mlp.up_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)=None
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:679] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:680] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:681] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([7168, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([3584, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([3584, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([7168, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: DefaultModelLoader -> load_weights() name='layers.20.post_attention_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: DefaultModelLoader -> load_weights() name='layers.21.input_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: DefaultModelLoader -> load_weights() name='layers.21.mlp.down_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1248] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1249] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1250] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 3584])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 3584])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 3584])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 3584])]
[36m(RayWorkerWrapper pid=126653)[0m hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: DefaultModelLoader -> load_weights() name='layers.21.mlp.gate_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)=None
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:679] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:680] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:681] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([7168, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([3584, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([3584, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([7168, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: DefaultModelLoader -> load_weights() name='layers.21.mlp.up_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)=None
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:679] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:680] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:681] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([7168, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([3584, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([3584, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([7168, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: DefaultModelLoader -> load_weights() name='layers.21.post_attention_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: DefaultModelLoader -> load_weights() name='layers.21.self_attn.k_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1114] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1115] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1116] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([1536, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([256, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([256, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([1536, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: DefaultModelLoader -> load_weights() name='layers.21.self_attn.o_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1248] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1249] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1250] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 1024])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 1024])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 1024])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 1024])]
[36m(RayWorkerWrapper pid=126653)[0m hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: DefaultModelLoader -> load_weights() name='layers.21.self_attn.q_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1114] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1115] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1116] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([1536, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([1536, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: DefaultModelLoader -> load_weights() name='layers.21.self_attn.v_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1114] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1115] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1116] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([1536, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([256, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([256, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([1536, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: DefaultModelLoader -> load_weights() name='layers.22.input_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: DefaultModelLoader -> load_weights() name='layers.22.mlp.down_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1248] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1249] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1250] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 3584])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 3584])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 3584])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 3584])]
[36m(RayWorkerWrapper pid=126653)[0m hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: DefaultModelLoader -> load_weights() name='layers.22.mlp.gate_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)=None
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:679] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:680] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:681] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([7168, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([3584, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([3584, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([7168, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: DefaultModelLoader -> load_weights() name='layers.22.mlp.up_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)=None
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:679] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:680] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:681] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([7168, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([3584, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([3584, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([7168, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: DefaultModelLoader -> load_weights() name='layers.22.post_attention_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: DefaultModelLoader -> load_weights() name='layers.22.self_attn.k_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1114] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1115] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1116] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([1536, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([256, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([256, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:00 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([1536, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: DefaultModelLoader -> load_weights() name='layers.22.self_attn.o_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1248] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1249] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1250] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 1024])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 1024])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 1024])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 1024])]
[36m(RayWorkerWrapper pid=126653)[0m hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: DefaultModelLoader -> load_weights() name='layers.22.self_attn.q_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1114] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1115] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1116] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([1536, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([1536, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: DefaultModelLoader -> load_weights() name='layers.22.self_attn.v_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1114] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1115] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1116] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([1536, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([256, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([256, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([1536, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: DefaultModelLoader -> load_weights() name='layers.23.input_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: DefaultModelLoader -> load_weights() name='layers.23.mlp.down_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1248] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1249] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1250] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 3584])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 3584])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 3584])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 3584])]
[36m(RayWorkerWrapper pid=126653)[0m hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: DefaultModelLoader -> load_weights() name='layers.23.mlp.gate_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)=None
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:679] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:680] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:681] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([7168, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([3584, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([3584, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([7168, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: DefaultModelLoader -> load_weights() name='layers.23.mlp.up_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: MergedColumnParallelLinear -> weight_loader() 1 get_shard_spec(param_data.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: MergedColumnParallelLinear -> weight_loader() 2 get_shard_spec(param_data.data)=None
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:679] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:680] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:681] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([7168, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:682] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([3584, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:683] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([3584, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([7168, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: DefaultModelLoader -> load_weights() name='layers.23.post_attention_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: DefaultModelLoader -> load_weights() name='layers.23.self_attn.k_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1114] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1115] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1116] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([1536, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([256, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([256, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([1536, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: DefaultModelLoader -> load_weights() name='layers.23.self_attn.o_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1248] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1249] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1250] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 1024])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1251] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 1024])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1252] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 1024])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 1024])]
[36m(RayWorkerWrapper pid=126653)[0m hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: DefaultModelLoader -> load_weights() name='layers.23.self_attn.q_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1114] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1115] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1116] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([1536, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
[36m(RayWorkerWrapper pid=126665)[0m Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:00,  5.37it/s]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([1536, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: DefaultModelLoader -> load_weights() name='layers.23.self_attn.v_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1114] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1115] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1116] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([1536, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([256, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([256, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([1536, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: DefaultModelLoader -> load_weights() name='layers.24.input_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: DefaultModelLoader -> load_weights() name='layers.24.mlp.down_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1114] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1115] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1116] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([1536, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([256, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([256, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([1536, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)=None
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1114] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1115] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1116] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([1536, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([1536, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)=None
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1114] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1115] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1116] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([1536, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([256, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([256, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([1536, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)=None
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1114] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1115] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1116] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([1536, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([256, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([256, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([1536, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)=None
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1114] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1115] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1116] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([1536, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([1536, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)=None
[36m(RayWorkerWrapper pid=126665)[0m Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  2.09it/s]
[36m(RayWorkerWrapper pid=126665)[0m Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.76it/s]
[36m(RayWorkerWrapper pid=126653)[0m hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)=None
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1114] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1115] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1116] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([1536, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1117] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([256, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1118] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([256, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:01 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([1536, 4096])]
[36m(RayWorkerWrapper pid=126653)[0m hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: DefaultModelLoader -> load_weights() name='embed_tokens.weight'
[36m(RayWorkerWrapper pid=126651)[0m 
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() is completed
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='embed_tokens.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([32064, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.0.self_attn.qkv_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.0.self_attn.o_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 1024])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.0.mlp.gate_up_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([7168, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.0.mlp.down_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 3584])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.0.input_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.0.post_attention_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.1.self_attn.qkv_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.1.self_attn.o_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 1024])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.1.mlp.gate_up_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([7168, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.1.mlp.down_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 3584])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.1.input_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.1.post_attention_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.2.self_attn.qkv_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.2.self_attn.o_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 1024])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.2.mlp.gate_up_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([7168, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.2.mlp.down_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 3584])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.2.input_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.2.post_attention_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.3.self_attn.qkv_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.3.self_attn.o_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 1024])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.3.mlp.gate_up_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([7168, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.3.mlp.down_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 3584])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.3.input_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.3.post_attention_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.4.self_attn.qkv_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.4.self_attn.o_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 1024])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.4.mlp.gate_up_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([7168, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.4.mlp.down_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 3584])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.4.input_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.4.post_attention_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.5.self_attn.qkv_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.5.self_attn.o_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 1024])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.5.mlp.gate_up_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([7168, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.5.mlp.down_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 3584])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.5.input_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.5.post_attention_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.6.self_attn.qkv_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.6.self_attn.o_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 1024])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.6.mlp.gate_up_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([7168, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.6.mlp.down_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 3584])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.6.input_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.6.post_attention_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.7.self_attn.qkv_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.7.self_attn.o_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 1024])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.7.mlp.gate_up_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([7168, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.7.mlp.down_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 3584])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.7.input_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.7.post_attention_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.8.self_attn.qkv_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.8.self_attn.o_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 1024])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.8.mlp.gate_up_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([7168, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.8.mlp.down_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 3584])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.8.input_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.8.post_attention_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.9.self_attn.qkv_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.9.self_attn.o_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 1024])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.9.mlp.gate_up_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([7168, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.9.mlp.down_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 3584])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.9.input_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.9.post_attention_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.10.self_attn.qkv_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.10.self_attn.o_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 1024])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.10.mlp.gate_up_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([7168, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.10.mlp.down_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 3584])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.10.input_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.10.post_attention_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.11.self_attn.qkv_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.11.self_attn.o_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 1024])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.11.mlp.gate_up_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([7168, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.11.mlp.down_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 3584])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.11.input_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.11.post_attention_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.12.self_attn.qkv_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.12.self_attn.o_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 1024])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.12.mlp.gate_up_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([7168, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.12.mlp.down_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 3584])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.12.input_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.12.post_attention_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.13.self_attn.qkv_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.13.self_attn.o_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 1024])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.13.mlp.gate_up_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([7168, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.13.mlp.down_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 3584])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.13.input_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.13.post_attention_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.14.self_attn.qkv_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.14.self_attn.o_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 1024])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.14.mlp.gate_up_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([7168, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.14.mlp.down_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 3584])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.14.input_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.14.post_attention_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.15.self_attn.qkv_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.15.self_attn.o_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 1024])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.15.mlp.gate_up_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([7168, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.15.mlp.down_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 3584])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.15.input_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.15.post_attention_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.16.self_attn.qkv_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.16.self_attn.o_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 1024])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.16.mlp.gate_up_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([7168, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.16.mlp.down_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 3584])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.16.input_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.16.post_attention_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.17.self_attn.qkv_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.17.self_attn.o_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 1024])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.17.mlp.gate_up_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([7168, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.17.mlp.down_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 3584])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.17.input_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.17.post_attention_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.18.self_attn.qkv_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.18.self_attn.o_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 1024])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.18.mlp.gate_up_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([7168, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.18.mlp.down_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 3584])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.18.input_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.18.post_attention_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.19.self_attn.qkv_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.19.self_attn.o_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 1024])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.19.mlp.gate_up_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([7168, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.19.mlp.down_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 3584])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.19.input_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.19.post_attention_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.20.self_attn.qkv_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.20.self_attn.o_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 1024])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.20.mlp.gate_up_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([7168, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.20.mlp.down_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 3584])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.20.input_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.20.post_attention_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.21.self_attn.qkv_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.21.self_attn.o_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 1024])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.21.mlp.gate_up_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([7168, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.21.mlp.down_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 3584])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.21.input_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.21.post_attention_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.22.self_attn.qkv_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.22.self_attn.o_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 1024])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.22.mlp.gate_up_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([7168, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.22.mlp.down_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 3584])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.22.input_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.22.post_attention_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.23.self_attn.qkv_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.23.self_attn.o_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 1024])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.23.mlp.gate_up_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([7168, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.23.mlp.down_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 3584])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.23.input_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.23.post_attention_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.24.self_attn.qkv_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.24.self_attn.o_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 1024])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.24.mlp.gate_up_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([7168, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.24.mlp.down_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 3584])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.24.input_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.24.post_attention_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.25.self_attn.qkv_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.25.self_attn.o_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 1024])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.25.mlp.gate_up_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([7168, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.25.mlp.down_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 3584])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.25.input_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.25.post_attention_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.26.self_attn.qkv_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.26.self_attn.o_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 1024])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.26.mlp.gate_up_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([7168, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.26.mlp.down_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 3584])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.26.input_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.26.post_attention_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.27.self_attn.qkv_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.27.self_attn.o_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 1024])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.27.mlp.gate_up_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([7168, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.27.mlp.down_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 3584])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.27.input_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.27.post_attention_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.28.self_attn.qkv_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.28.self_attn.o_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 1024])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.28.mlp.gate_up_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([7168, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.28.mlp.down_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 3584])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.28.input_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.28.post_attention_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.29.self_attn.qkv_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.29.self_attn.o_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 1024])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.29.mlp.gate_up_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([7168, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.29.mlp.down_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 3584])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.29.input_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.29.post_attention_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.30.self_attn.qkv_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.30.self_attn.o_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 1024])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.30.mlp.gate_up_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([7168, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.30.mlp.down_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 3584])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.30.input_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.30.post_attention_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.31.self_attn.qkv_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.58it/s]
[36m(RayWorkerWrapper pid=126665)[0m Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.75it/s]
[36m(RayWorkerWrapper pid=126665)[0m 
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.31.self_attn.o_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 1024])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.31.mlp.gate_up_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([7168, 4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.31.mlp.down_proj.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096, 3584])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.31.input_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='layers.31.post_attention_layernorm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() name='norm.weight'
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_weights() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:02 [loader.py:430] Loading weights took 2.24 seconds
[36m(RayWorkerWrapper pid=126653)[0m hosseins: load_model() model is loaded
hosseins: _init_workers_ray() self.pp_tp_workers=[[Actor(RayWorkerWrapper, b035717b808d0ea8eef4946601000000), Actor(RayWorkerWrapper, e4b38f09ae5252940d5a08fe01000000), Actor(RayWorkerWrapper, ee6eb30bf20af0bbe247b09801000000), Actor(RayWorkerWrapper, 04a14f2ce4954479c2c2dd2901000000)]]
hosseins: _init_workers_ray() len(self.tp_driver_workers)=1
hosseins: _init_workers_ray() len(self.non_driver_workers)=3
hosseins: _initialize_kv_caches() self.model_executor=<vllm.v1.executor.ray_distributed_executor.RayDistributedExecutor object at 0x70a096fddb40>
hosseins: _initialize_kv_caches() kv_cache_specs=[{'model.layers.0.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.1.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.2.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.3.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.4.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.5.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.6.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.7.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.8.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.9.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.10.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.11.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.12.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.13.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.14.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.15.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.16.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.17.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.18.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.19.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.20.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.21.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.22.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.23.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.24.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.25.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.26.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.27.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.28.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.29.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.30.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.31.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False)}, {'model.layers.0.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.1.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.2.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.3.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.4.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.5.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.6.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.7.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.8.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.9.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.10.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.11.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.12.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.13.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.14.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.15.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.16.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.17.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.18.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.19.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.20.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.21.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.22.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.23.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.24.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.25.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.26.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.27.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.28.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.29.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.30.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.31.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False)}, {'model.layers.0.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.1.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.2.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.3.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.4.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.5.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.6.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.7.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.8.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.9.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.10.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.11.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.12.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.13.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.14.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.15.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.16.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.17.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.18.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.19.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.20.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.21.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.22.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.23.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.24.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.25.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.26.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.27.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.28.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.29.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.30.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.31.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False)}, {'model.layers.0.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.1.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.2.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.3.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.4.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.5.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.6.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.7.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.8.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.9.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.10.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.11.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.12.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.13.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.14.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.15.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.16.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.17.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.18.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.19.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.20.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.21.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.22.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.23.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.24.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.25.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.26.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.27.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.28.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.29.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.30.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.31.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False)}]
hosseins: _initialize_kv_caches() available_gpu_memory=16775087616
hosseins: get_kv_cache_configs() num_layers=32
hosseins: get_kv_cache_configs() len(kv_cache_specs)=4
hosseins: get_kv_cache_configs() kv_cache_specs=[{'model.layers.0.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.1.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.2.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.3.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.4.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.5.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.6.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.7.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.8.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.9.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.10.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.11.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.12.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.13.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.14.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.15.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.16.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.17.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.18.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.19.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.20.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.21.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.22.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.23.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.24.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.25.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.26.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.27.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.28.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.29.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.30.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.31.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False)}, {'model.layers.0.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.1.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.2.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.3.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.4.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.5.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.6.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.7.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.8.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.9.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.10.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.11.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.12.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.13.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.14.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.15.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.16.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.17.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.18.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.19.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.20.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.21.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.22.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.23.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.24.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.25.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.26.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.27.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.28.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.29.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.30.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.31.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False)}, {'model.layers.0.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.1.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.2.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.3.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.4.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.5.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.6.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.7.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.8.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.9.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.10.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.11.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.12.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.13.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.14.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.15.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.16.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.17.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.18.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.19.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.20.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.21.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.22.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.23.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.24.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.25.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.26.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.27.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.28.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.29.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.30.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.31.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False)}, {'model.layers.0.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.1.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.2.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.3.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.4.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.5.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.6.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.7.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.8.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.9.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.10.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.11.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.12.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.13.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.14.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.15.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.16.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.17.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.18.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.19.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.20.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.21.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.22.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.23.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.24.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.25.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.26.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.27.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.28.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.29.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.30.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.31.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False)}]
hosseins: page_sizes={16384}
hosseins: page_size=16384
hosseins: available_memory=16775087616
hosseins: num_blocks=31995
hosseins: num_blocks=31995
hosseins: vllm_config.cache_config.num_gpu_blocks_override=None
hosseins: vllm_config.cache_config.block_size=16
hosseins: num_tokens=511920
INFO 03-18 16:20:04 [kv_cache_utils.py:547] GPU KV cache size: 511,920 tokens
INFO 03-18 16:20:04 [kv_cache_utils.py:550] Maximum concurrency for 512 tokens per request: 999.84x
hosseins: page_sizes={16384}
hosseins: page_size=16384
hosseins: available_memory=16775087616
hosseins: num_blocks=31995
hosseins: num_blocks=31995
hosseins: vllm_config.cache_config.num_gpu_blocks_override=None
hosseins: vllm_config.cache_config.block_size=16
hosseins: num_tokens=511920
INFO 03-18 16:20:04 [kv_cache_utils.py:547] GPU KV cache size: 511,920 tokens
INFO 03-18 16:20:04 [kv_cache_utils.py:550] Maximum concurrency for 512 tokens per request: 999.84x
hosseins: page_sizes={16384}
hosseins: page_size=16384
hosseins: available_memory=16775087616
hosseins: num_blocks=31995
hosseins: num_blocks=31995
hosseins: vllm_config.cache_config.num_gpu_blocks_override=None
hosseins: vllm_config.cache_config.block_size=16
hosseins: num_tokens=511920
INFO 03-18 16:20:04 [kv_cache_utils.py:547] GPU KV cache size: 511,920 tokens
INFO 03-18 16:20:04 [kv_cache_utils.py:550] Maximum concurrency for 512 tokens per request: 999.84x
hosseins: page_sizes={16384}
hosseins: page_size=16384
hosseins: available_memory=16775087616
hosseins: num_blocks=31995
hosseins: num_blocks=31995
hosseins: vllm_config.cache_config.num_gpu_blocks_override=None
hosseins: vllm_config.cache_config.block_size=16
hosseins: num_tokens=511920
INFO 03-18 16:20:04 [kv_cache_utils.py:547] GPU KV cache size: 511,920 tokens
INFO 03-18 16:20:04 [kv_cache_utils.py:550] Maximum concurrency for 512 tokens per request: 999.84x
hosseins: _initialize_kv_caches() kv_cache_configs=[KVCacheConfig(num_blocks=31995, tensors={'model.layers.0.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.1.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.2.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.3.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.4.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.5.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.6.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.7.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.8.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.9.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.10.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.11.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.12.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.13.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.14.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.15.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.16.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.17.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.18.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.19.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.20.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.21.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.22.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.23.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.24.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.25.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.26.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.27.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.28.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.29.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.30.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.31.self_attn.attn': KVCacheTensor(size=524206080)}, groups=[['model.layers.0.self_attn.attn', 'model.layers.1.self_attn.attn', 'model.layers.2.self_attn.attn', 'model.layers.3.self_attn.attn', 'model.layers.4.self_attn.attn', 'model.layers.5.self_attn.attn', 'model.layers.6.self_attn.attn', 'model.layers.7.self_attn.attn', 'model.layers.8.self_attn.attn', 'model.layers.9.self_attn.attn', 'model.layers.10.self_attn.attn', 'model.layers.11.self_attn.attn', 'model.layers.12.self_attn.attn', 'model.layers.13.self_attn.attn', 'model.layers.14.self_attn.attn', 'model.layers.15.self_attn.attn', 'model.layers.16.self_attn.attn', 'model.layers.17.self_attn.attn', 'model.layers.18.self_attn.attn', 'model.layers.19.self_attn.attn', 'model.layers.20.self_attn.attn', 'model.layers.21.self_attn.attn', 'model.layers.22.self_attn.attn', 'model.layers.23.self_attn.attn', 'model.layers.24.self_attn.attn', 'model.layers.25.self_attn.attn', 'model.layers.26.self_attn.attn', 'model.layers.27.self_attn.attn', 'model.layers.28.self_attn.attn', 'model.layers.29.self_attn.attn', 'model.layers.30.self_attn.attn', 'model.layers.31.self_attn.attn']], kv_cache_spec={'model.layers.0.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.1.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.2.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.3.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.4.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.5.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.6.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.7.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.8.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.9.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.10.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.11.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.12.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.13.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.14.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.15.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.16.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.17.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.18.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.19.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.20.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.21.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.22.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.23.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.24.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.25.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.26.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.27.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.28.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.29.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.30.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.31.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False)}), KVCacheConfig(num_blocks=31995, tensors={'model.layers.0.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.1.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.2.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.3.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.4.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.5.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.6.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.7.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.8.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.9.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.10.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.11.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.12.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.13.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.14.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.15.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.16.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.17.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.18.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.19.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.20.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.21.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.22.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.23.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.24.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.25.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.26.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.27.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.28.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.29.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.30.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.31.self_attn.attn': KVCacheTensor(size=524206080)}, groups=[['model.layers.0.self_attn.attn', 'model.layers.1.self_attn.attn', 'model.layers.2.self_attn.attn', 'model.layers.3.self_attn.attn', 'model.layers.4.self_attn.attn', 'model.layers.5.self_attn.attn', 'model.layers.6.self_attn.attn', 'model.layers.7.self_attn.attn', 'model.layers.8.self_attn.attn', 'model.layers.9.self_attn.attn', 'model.layers.10.self_attn.attn', 'model.layers.11.self_attn.attn', 'model.layers.12.self_attn.attn', 'model.layers.13.self_attn.attn', 'model.layers.14.self_attn.attn', 'model.layers.15.self_attn.attn', 'model.layers.16.self_attn.attn', 'model.layers.17.self_attn.attn', 'model.layers.18.self_attn.attn', 'model.layers.19.self_attn.attn', 'model.layers.20.self_attn.attn', 'model.layers.21.self_attn.attn', 'model.layers.22.self_attn.attn', 'model.layers.23.self_attn.attn', 'model.layers.24.self_attn.attn', 'model.layers.25.self_attn.attn', 'model.layers.26.self_attn.attn', 'model.layers.27.self_attn.attn', 'model.layers.28.self_attn.attn', 'model.layers.29.self_attn.attn', 'model.layers.30.self_attn.attn', 'model.layers.31.self_attn.attn']], kv_cache_spec={'model.layers.0.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.1.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.2.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.3.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.4.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.5.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.6.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.7.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.8.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.9.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.10.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.11.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.12.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.13.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.14.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.15.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.16.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.17.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.18.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.19.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.20.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.21.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.22.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.23.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.24.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.25.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.26.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.27.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.28.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.29.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.30.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.31.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False)}), KVCacheConfig(num_blocks=31995, tensors={'model.layers.0.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.1.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.2.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.3.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.4.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.5.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.6.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.7.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.8.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.9.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.10.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.11.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.12.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.13.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.14.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.15.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.16.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.17.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.18.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.19.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.20.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.21.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.22.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.23.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.24.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.25.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.26.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.27.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.28.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.29.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.30.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.31.self_attn.attn': KVCacheTensor(size=524206080)}, groups=[['model.layers.0.self_attn.attn', 'model.layers.1.self_attn.attn', 'model.layers.2.self_attn.attn', 'model.layers.3.self_attn.attn', 'model.layers.4.self_attn.attn', 'model.layers.5.self_attn.attn', 'model.layers.6.self_attn.attn', 'model.layers.7.self_attn.attn', 'model.layers.8.self_attn.attn', 'model.layers.9.self_attn.attn', 'model.layers.10.self_attn.attn', 'model.layers.11.self_attn.attn', 'model.layers.12.self_attn.attn', 'model.layers.13.self_attn.attn', 'model.layers.14.self_attn.attn', 'model.layers.15.self_attn.attn', 'model.layers.16.self_attn.attn', 'model.layers.17.self_attn.attn', 'model.layers.18.self_attn.attn', 'model.layers.19.self_attn.attn', 'model.layers.20.self_attn.attn', 'model.layers.21.self_attn.attn', 'model.layers.22.self_attn.attn', 'model.layers.23.self_attn.attn', 'model.layers.24.self_attn.attn', 'model.layers.25.self_attn.attn', 'model.layers.26.self_attn.attn', 'model.layers.27.self_attn.attn', 'model.layers.28.self_attn.attn', 'model.layers.29.self_attn.attn', 'model.layers.30.self_attn.attn', 'model.layers.31.self_attn.attn']], kv_cache_spec={'model.layers.0.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.1.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.2.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.3.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.4.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.5.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.6.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.7.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.8.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.9.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.10.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.11.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.12.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.13.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.14.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.15.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.16.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.17.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.18.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.19.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.20.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.21.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.22.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.23.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.24.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.25.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.26.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.27.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.28.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.29.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.30.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.31.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False)}), KVCacheConfig(num_blocks=31995, tensors={'model.layers.0.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.1.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.2.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.3.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.4.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.5.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.6.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.7.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.8.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.9.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.10.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.11.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.12.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.13.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.14.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.15.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.16.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.17.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.18.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.19.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.20.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.21.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.22.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.23.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.24.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.25.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.26.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.27.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.28.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.29.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.30.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.31.self_attn.attn': KVCacheTensor(size=524206080)}, groups=[['model.layers.0.self_attn.attn', 'model.layers.1.self_attn.attn', 'model.layers.2.self_attn.attn', 'model.layers.3.self_attn.attn', 'model.layers.4.self_attn.attn', 'model.layers.5.self_attn.attn', 'model.layers.6.self_attn.attn', 'model.layers.7.self_attn.attn', 'model.layers.8.self_attn.attn', 'model.layers.9.self_attn.attn', 'model.layers.10.self_attn.attn', 'model.layers.11.self_attn.attn', 'model.layers.12.self_attn.attn', 'model.layers.13.self_attn.attn', 'model.layers.14.self_attn.attn', 'model.layers.15.self_attn.attn', 'model.layers.16.self_attn.attn', 'model.layers.17.self_attn.attn', 'model.layers.18.self_attn.attn', 'model.layers.19.self_attn.attn', 'model.layers.20.self_attn.attn', 'model.layers.21.self_attn.attn', 'model.layers.22.self_attn.attn', 'model.layers.23.self_attn.attn', 'model.layers.24.self_attn.attn', 'model.layers.25.self_attn.attn', 'model.layers.26.self_attn.attn', 'model.layers.27.self_attn.attn', 'model.layers.28.self_attn.attn', 'model.layers.29.self_attn.attn', 'model.layers.30.self_attn.attn', 'model.layers.31.self_attn.attn']], kv_cache_spec={'model.layers.0.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.1.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.2.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.3.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.4.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.5.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.6.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.7.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.8.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.9.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.10.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.11.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.12.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.13.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.14.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.15.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.16.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.17.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.18.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.19.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.20.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.21.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.22.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.23.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.24.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.25.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.26.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.27.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.28.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.29.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.30.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.31.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False)})]
hosseins: _initialize_kv_caches() num_gpu_blocks_set={31995}
hosseins: _initialize_kv_caches() num_gpu_blocks=31995
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ModelWrapperV1.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ModelWrapperV1 -> forward() 1 get_shard_spec(input_ids)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ModelWrapperV1 -> forward() 1 get_shard_spec(positions)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaForCasualLM.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_pp_group().is_first_rank=True
[36m(RayWorkerWrapper pid=126665)[0m hosseins: VocabParallelEmbedding.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: VocabParallelEmbedding.forward() [get_shard_spec(masked_input)=None]
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedEmbeddingMethod -> embedding()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedEmbeddingMethod -> embedding() [layer.weight.shape=torch.Size([32064, 4096])]
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedEmbeddingMethod -> embedding() [layer.weight.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126665)[0m hosseins: VocabParallelEmbedding.forward() [get_shard_spec(output_parallel)=None]
[36m(RayWorkerWrapper pid=126665)[0m hosseins: VocabParallelEmbedding.forward() [get_shard_spec(output)=None]
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() 1 get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() layer start
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() ============= type(layer)=<class 'vllm.model_executor.models.llama.LlamaDecoderLayer'> =============
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: self_attn.qkv_proj.weight, Shape: param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: self_attn.o_proj.weight, Shape: param.shape=torch.Size([4096, 1024])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: mlp.gate_up_proj.weight, Shape: param.shape=torch.Size([7168, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: mlp.down_proj.weight, Shape: param.shape=torch.Size([4096, 3584])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: input_layernorm.weight, Shape: param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: post_attention_layernorm.weight, Shape: param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() hidden_states.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() self.qkv_proj=QKVParallelLinear(in_features=4096, output_features=1536, bias=False, tp_size=4, gather_output=False)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() Name: weight, Shape: param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.QKVParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([1536, 4096]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 1536]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() qkv.shape=torch.Size([512, 1536])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() qkv.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention. forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() key.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() get_shard_spec(key)=None value.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() value.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() get_shard_spec(value)=None value.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: PallasAttentionBackendImpl.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([512, 1024])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 1024]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 1024]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() output.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([7168, 4096]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 7168]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 3584]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 3584]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() ============= type(layer)=<class 'vllm.model_executor.models.llama.LlamaDecoderLayer'> =============
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: self_attn.qkv_proj.weight, Shape: param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: self_attn.o_proj.weight, Shape: param.shape=torch.Size([4096, 1024])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: mlp.gate_up_proj.weight, Shape: param.shape=torch.Size([7168, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: mlp.down_proj.weight, Shape: param.shape=torch.Size([4096, 3584])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: input_layernorm.weight, Shape: param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: post_attention_layernorm.weight, Shape: param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() hidden_states.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() self.qkv_proj=QKVParallelLinear(in_features=4096, output_features=1536, bias=False, tp_size=4, gather_output=False)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() Name: weight, Shape: param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.QKVParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([1536, 4096]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 1536]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() qkv.shape=torch.Size([512, 1536])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() qkv.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention. forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() key.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() get_shard_spec(key)=None value.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() value.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() get_shard_spec(value)=None value.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: PallasAttentionBackendImpl.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([512, 1024])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 1024]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 1024]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() output.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([7168, 4096]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 7168]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 3584]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 3584]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() ============= type(layer)=<class 'vllm.model_executor.models.llama.LlamaDecoderLayer'> =============
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: self_attn.qkv_proj.weight, Shape: param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: self_attn.o_proj.weight, Shape: param.shape=torch.Size([4096, 1024])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: mlp.gate_up_proj.weight, Shape: param.shape=torch.Size([7168, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: mlp.down_proj.weight, Shape: param.shape=torch.Size([4096, 3584])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: input_layernorm.weight, Shape: param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: post_attention_layernorm.weight, Shape: param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() hidden_states.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() self.qkv_proj=QKVParallelLinear(in_features=4096, output_features=1536, bias=False, tp_size=4, gather_output=False)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() Name: weight, Shape: param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.QKVParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([1536, 4096]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 1536]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() qkv.shape=torch.Size([512, 1536])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() qkv.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention. forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() key.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() get_shard_spec(key)=None value.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() value.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() get_shard_spec(value)=None value.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: PallasAttentionBackendImpl.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([512, 1024])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 1024]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 1024]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() output.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([7168, 4096]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 7168]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 3584]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 3584]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() ============= type(layer)=<class 'vllm.model_executor.models.llama.LlamaDecoderLayer'> =============
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: self_attn.qkv_proj.weight, Shape: param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: self_attn.o_proj.weight, Shape: param.shape=torch.Size([4096, 1024])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: mlp.gate_up_proj.weight, Shape: param.shape=torch.Size([7168, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: mlp.down_proj.weight, Shape: param.shape=torch.Size([4096, 3584])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: input_layernorm.weight, Shape: param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: post_attention_layernorm.weight, Shape: param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() hidden_states.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() self.qkv_proj=QKVParallelLinear(in_features=4096, output_features=1536, bias=False, tp_size=4, gather_output=False)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() Name: weight, Shape: param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.QKVParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([1536, 4096]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 1536]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() qkv.shape=torch.Size([512, 1536])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() qkv.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention. forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() key.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() get_shard_spec(key)=None value.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() value.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() get_shard_spec(value)=None value.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: PallasAttentionBackendImpl.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([512, 1024])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 1024]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 1024]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() output.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([7168, 4096]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 7168]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 3584]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 3584]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() ============= type(layer)=<class 'vllm.model_executor.models.llama.LlamaDecoderLayer'> =============
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: self_attn.qkv_proj.weight, Shape: param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: self_attn.o_proj.weight, Shape: param.shape=torch.Size([4096, 1024])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: mlp.gate_up_proj.weight, Shape: param.shape=torch.Size([7168, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: mlp.down_proj.weight, Shape: param.shape=torch.Size([4096, 3584])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: input_layernorm.weight, Shape: param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: post_attention_layernorm.weight, Shape: param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() hidden_states.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() self.qkv_proj=QKVParallelLinear(in_features=4096, output_features=1536, bias=False, tp_size=4, gather_output=False)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() Name: weight, Shape: param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.QKVParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([1536, 4096]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 1536]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() qkv.shape=torch.Size([512, 1536])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() qkv.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention. forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() key.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() get_shard_spec(key)=None value.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() value.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() get_shard_spec(value)=None value.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: PallasAttentionBackendImpl.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([512, 1024])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 1024]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 1024]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() output.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([7168, 4096]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 7168]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 3584]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 3584]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() ============= type(layer)=<class 'vllm.model_executor.models.llama.LlamaDecoderLayer'> =============
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: self_attn.qkv_proj.weight, Shape: param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: self_attn.o_proj.weight, Shape: param.shape=torch.Size([4096, 1024])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: mlp.gate_up_proj.weight, Shape: param.shape=torch.Size([7168, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: mlp.down_proj.weight, Shape: param.shape=torch.Size([4096, 3584])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: input_layernorm.weight, Shape: param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: post_attention_layernorm.weight, Shape: param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() hidden_states.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() self.qkv_proj=QKVParallelLinear(in_features=4096, output_features=1536, bias=False, tp_size=4, gather_output=False)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() Name: weight, Shape: param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.QKVParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([1536, 4096]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 1536]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() qkv.shape=torch.Size([512, 1536])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() qkv.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention. forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() key.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() get_shard_spec(key)=None value.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() value.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() get_shard_spec(value)=None value.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: PallasAttentionBackendImpl.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([512, 1024])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 1024]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 1024]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() output.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([7168, 4096]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 7168]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 3584]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 3584]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() ============= type(layer)=<class 'vllm.model_executor.models.llama.LlamaDecoderLayer'> =============
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: self_attn.qkv_proj.weight, Shape: param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: self_attn.o_proj.weight, Shape: param.shape=torch.Size([4096, 1024])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: mlp.gate_up_proj.weight, Shape: param.shape=torch.Size([7168, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: mlp.down_proj.weight, Shape: param.shape=torch.Size([4096, 3584])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: input_layernorm.weight, Shape: param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: post_attention_layernorm.weight, Shape: param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() hidden_states.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() self.qkv_proj=QKVParallelLinear(in_features=4096, output_features=1536, bias=False, tp_size=4, gather_output=False)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() Name: weight, Shape: param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.QKVParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([1536, 4096]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 1536]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() qkv.shape=torch.Size([512, 1536])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() qkv.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention. forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() key.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() get_shard_spec(key)=None value.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() value.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() get_shard_spec(value)=None value.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: PallasAttentionBackendImpl.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([512, 1024])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 1024]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 1024]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() output.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([7168, 4096]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 7168]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 3584]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 3584]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() ============= type(layer)=<class 'vllm.model_executor.models.llama.LlamaDecoderLayer'> =============
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: self_attn.qkv_proj.weight, Shape: param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: self_attn.o_proj.weight, Shape: param.shape=torch.Size([4096, 1024])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: mlp.gate_up_proj.weight, Shape: param.shape=torch.Size([7168, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: mlp.down_proj.weight, Shape: param.shape=torch.Size([4096, 3584])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: input_layernorm.weight, Shape: param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: post_attention_layernorm.weight, Shape: param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() hidden_states.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() self.qkv_proj=QKVParallelLinear(in_features=4096, output_features=1536, bias=False, tp_size=4, gather_output=False)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() Name: weight, Shape: param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.QKVParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([1536, 4096]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 1536]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() qkv.shape=torch.Size([512, 1536])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() qkv.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention. forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() key.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() get_shard_spec(key)=None value.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() value.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() get_shard_spec(value)=None value.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: PallasAttentionBackendImpl.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([512, 1024])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 1024]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 1024]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() output.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([7168, 4096]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 7168]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 3584]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 3584]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() ============= type(layer)=<class 'vllm.model_executor.models.llama.LlamaDecoderLayer'> =============
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: self_attn.qkv_proj.weight, Shape: param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: self_attn.o_proj.weight, Shape: param.shape=torch.Size([4096, 1024])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: mlp.gate_up_proj.weight, Shape: param.shape=torch.Size([7168, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: mlp.down_proj.weight, Shape: param.shape=torch.Size([4096, 3584])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: input_layernorm.weight, Shape: param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: post_attention_layernorm.weight, Shape: param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() hidden_states.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() self.qkv_proj=QKVParallelLinear(in_features=4096, output_features=1536, bias=False, tp_size=4, gather_output=False)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() Name: weight, Shape: param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.QKVParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([1536, 4096]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 1536]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() qkv.shape=torch.Size([512, 1536])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() qkv.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention. forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() key.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() get_shard_spec(key)=None value.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() value.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() get_shard_spec(value)=None value.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: PallasAttentionBackendImpl.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([512, 1024])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 1024]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 1024]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() output.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([7168, 4096]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 7168]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 3584]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 3584]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() ============= type(layer)=<class 'vllm.model_executor.models.llama.LlamaDecoderLayer'> =============
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: self_attn.qkv_proj.weight, Shape: param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: self_attn.o_proj.weight, Shape: param.shape=torch.Size([4096, 1024])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: mlp.gate_up_proj.weight, Shape: param.shape=torch.Size([7168, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: mlp.down_proj.weight, Shape: param.shape=torch.Size([4096, 3584])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: input_layernorm.weight, Shape: param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: post_attention_layernorm.weight, Shape: param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() hidden_states.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() self.qkv_proj=QKVParallelLinear(in_features=4096, output_features=1536, bias=False, tp_size=4, gather_output=False)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() Name: weight, Shape: param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.QKVParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([1536, 4096]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 1536]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() qkv.shape=torch.Size([512, 1536])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() qkv.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention. forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() key.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() get_shard_spec(key)=None value.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() value.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() get_shard_spec(value)=None value.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: PallasAttentionBackendImpl.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([512, 1024])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 1024]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 1024]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() output.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([7168, 4096]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 7168]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 3584]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 3584]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() ============= type(layer)=<class 'vllm.model_executor.models.llama.LlamaDecoderLayer'> =============
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: self_attn.qkv_proj.weight, Shape: param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: self_attn.o_proj.weight, Shape: param.shape=torch.Size([4096, 1024])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: mlp.gate_up_proj.weight, Shape: param.shape=torch.Size([7168, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: mlp.down_proj.weight, Shape: param.shape=torch.Size([4096, 3584])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: input_layernorm.weight, Shape: param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: post_attention_layernorm.weight, Shape: param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() hidden_states.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() self.qkv_proj=QKVParallelLinear(in_features=4096, output_features=1536, bias=False, tp_size=4, gather_output=False)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() Name: weight, Shape: param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.QKVParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([1536, 4096]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 1536]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() qkv.shape=torch.Size([512, 1536])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() qkv.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention. forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() key.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() get_shard_spec(key)=None value.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() value.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() get_shard_spec(value)=None value.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: PallasAttentionBackendImpl.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([512, 1024])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 1024]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 1024]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() output.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([7168, 4096]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 7168]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 3584]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 3584]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() ============= type(layer)=<class 'vllm.model_executor.models.llama.LlamaDecoderLayer'> =============
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: self_attn.qkv_proj.weight, Shape: param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: self_attn.o_proj.weight, Shape: param.shape=torch.Size([4096, 1024])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: mlp.gate_up_proj.weight, Shape: param.shape=torch.Size([7168, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: mlp.down_proj.weight, Shape: param.shape=torch.Size([4096, 3584])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: input_layernorm.weight, Shape: param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: post_attention_layernorm.weight, Shape: param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() hidden_states.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() self.qkv_proj=QKVParallelLinear(in_features=4096, output_features=1536, bias=False, tp_size=4, gather_output=False)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() Name: weight, Shape: param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.QKVParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([1536, 4096]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 1536]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() qkv.shape=torch.Size([512, 1536])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() qkv.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention. forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() key.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() get_shard_spec(key)=None value.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() value.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() get_shard_spec(value)=None value.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: PallasAttentionBackendImpl.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([512, 1024])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 1024]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 1024]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() output.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([7168, 4096]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 7168]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 3584]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 3584]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() ============= type(layer)=<class 'vllm.model_executor.models.llama.LlamaDecoderLayer'> =============
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: self_attn.qkv_proj.weight, Shape: param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: self_attn.o_proj.weight, Shape: param.shape=torch.Size([4096, 1024])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: mlp.gate_up_proj.weight, Shape: param.shape=torch.Size([7168, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: mlp.down_proj.weight, Shape: param.shape=torch.Size([4096, 3584])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: input_layernorm.weight, Shape: param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: post_attention_layernorm.weight, Shape: param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() hidden_states.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() self.qkv_proj=QKVParallelLinear(in_features=4096, output_features=1536, bias=False, tp_size=4, gather_output=False)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() Name: weight, Shape: param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.QKVParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([1536, 4096]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 1536]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() qkv.shape=torch.Size([512, 1536])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() qkv.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention. forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() key.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() get_shard_spec(key)=None value.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() value.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() get_shard_spec(value)=None value.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: PallasAttentionBackendImpl.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([512, 1024])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 1024]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 1024]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() output.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([7168, 4096]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 7168]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 3584]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 3584]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() ============= type(layer)=<class 'vllm.model_executor.models.llama.LlamaDecoderLayer'> =============
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: self_attn.qkv_proj.weight, Shape: param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: self_attn.o_proj.weight, Shape: param.shape=torch.Size([4096, 1024])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: mlp.gate_up_proj.weight, Shape: param.shape=torch.Size([7168, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: mlp.down_proj.weight, Shape: param.shape=torch.Size([4096, 3584])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: input_layernorm.weight, Shape: param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: post_attention_layernorm.weight, Shape: param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() hidden_states.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() self.qkv_proj=QKVParallelLinear(in_features=4096, output_features=1536, bias=False, tp_size=4, gather_output=False)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() Name: weight, Shape: param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.QKVParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([1536, 4096]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 1536]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() qkv.shape=torch.Size([512, 1536])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() qkv.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention. forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() key.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() get_shard_spec(key)=None value.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() value.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() get_shard_spec(value)=None value.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: PallasAttentionBackendImpl.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([512, 1024])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 1024]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 1024]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() output.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([7168, 4096]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 7168]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 3584]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 3584]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() ============= type(layer)=<class 'vllm.model_executor.models.llama.LlamaDecoderLayer'> =============
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: self_attn.qkv_proj.weight, Shape: param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: self_attn.o_proj.weight, Shape: param.shape=torch.Size([4096, 1024])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: mlp.gate_up_proj.weight, Shape: param.shape=torch.Size([7168, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: mlp.down_proj.weight, Shape: param.shape=torch.Size([4096, 3584])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: input_layernorm.weight, Shape: param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: post_attention_layernorm.weight, Shape: param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() hidden_states.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() self.qkv_proj=QKVParallelLinear(in_features=4096, output_features=1536, bias=False, tp_size=4, gather_output=False)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() Name: weight, Shape: param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.QKVParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([1536, 4096]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 1536]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() qkv.shape=torch.Size([512, 1536])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() qkv.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention. forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() key.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() get_shard_spec(key)=None value.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() value.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() get_shard_spec(value)=None value.shape=torch.Size([512, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: PallasAttentionBackendImpl.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() attn_output.shape=torch.Size([512, 1024])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 1024]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 1024]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() output.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward() residual.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([7168, 4096]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 7168]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 3584]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 3584]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() ============= type(layer)=<class 'vllm.model_executor.models.llama.LlamaDecoderLayer'> =============
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: self_attn.qkv_proj.weight, Shape: param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: self_attn.o_proj.weight, Shape: param.shape=torch.Size([4096, 1024])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: mlp.gate_up_proj.weight, Shape: param.shape=torch.Size([7168, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: mlp.down_proj.weight, Shape: param.shape=torch.Size([4096, 3584])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: input_layernorm.weight, Shape: param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: post_attention_layernorm.weight, Shape: param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() hidden_states.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() self.qkv_proj=QKVParallelLinear(in_features=4096, output_features=1536, bias=False, tp_size=4, gather_output=False)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() Name: weight, Shape: param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.QKVParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([1536, 4096]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 4096]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() layer end
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() norm start
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() norm end
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ModelWrapperV1 -> forward() 2 get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ModelWrapperV1 -> compute_logits() hidden_states.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ModelWrapperV1 -> compute_logits() get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ModelWrapperV1 -> compute_logits() logits_indices.shape=torch.Size([64])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ModelWrapperV1 -> compute_logits() get_shard_spec(logits_indices)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedEmbeddingMethod -> apply()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedEmbeddingMethod -> apply() [layer.weight.shape=torch.Size([32064, 4096])]
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedEmbeddingMethod -> apply() [layer.weight.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ModelWrapperV1 -> compute_logits() hidden_states.shape=torch.Size([512, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ModelWrapperV1 -> compute_logits() get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ModelWrapperV1 -> compute_logits() logits_indices.shape=torch.Size([128])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ModelWrapperV1 -> compute_logits() get_shard_spec(logits_indices)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedEmbeddingMethod -> apply()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedEmbeddingMethod -> apply() [layer.weight.shape=torch.Size([32064, 4096])]
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedEmbeddingMethod -> apply() [layer.weight.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126665)[0m hosseins: initialize_from_config() kv_cache_config=KVCacheConfig(num_blocks=31995, tensors={'model.layers.0.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.1.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.2.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.3.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.4.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.5.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.6.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.7.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.8.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.9.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.10.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.11.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.12.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.13.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.14.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.15.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.16.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.17.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.18.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.19.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.20.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.21.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.22.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.23.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.24.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.25.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.26.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.27.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.28.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.29.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.30.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.31.self_attn.attn': KVCacheTensor(size=524206080)}, groups=[['model.layers.0.self_attn.attn', 'model.layers.1.self_attn.attn', 'model.layers.2.self_attn.attn', 'model.layers.3.self_attn.attn', 'model.layers.4.self_attn.attn', 'model.layers.5.self_attn.attn', 'model.layers.6.self_attn.attn', 'model.layers.7.self_attn.attn', 'model.layers.8.self_attn.attn', 'model.layers.9.self_attn.attn', 'model.layers.10.self_attn.attn', 'model.layers.11.self_attn.attn', 'model.layers.12.self_attn.attn', 'model.layers.13.self_attn.attn', 'model.layers.14.self_attn.attn', 'model.layers.15.self_attn.attn', 'model.layers.16.self_attn.attn', 'model.layers.17.self_attn.attn', 'model.layers.18.self_attn.attn', 'model.layers.19.self_attn.attn', 'model.layers.20.self_attn.attn', 'model.layers.21.self_attn.attn', 'model.layers.22.self_attn.attn', 'model.layers.23.self_attn.attn', 'model.layers.24.self_attn.attn', 'model.layers.25.self_attn.attn', 'model.layers.26.self_attn.attn', 'model.layers.27.self_attn.attn', 'model.layers.28.self_attn.attn', 'model.layers.29.self_attn.attn', 'model.layers.30.self_attn.attn', 'model.layers.31.self_attn.attn']], kv_cache_spec={'model.layers.0.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.1.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.2.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.3.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.4.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.5.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.6.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.7.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.8.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.9.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.10.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.11.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.12.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.13.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.14.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.15.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.16.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.17.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.18.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.19.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.20.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.21.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.22.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.23.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.24.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.25.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.26.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.27.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.28.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.29.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.30.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.31.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False)})
[36m(RayWorkerWrapper pid=126665)[0m hosseins: initialize_kv_cache() kv_cache_config=KVCacheConfig(num_blocks=31995, tensors={'model.layers.0.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.1.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.2.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.3.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.4.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.5.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.6.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.7.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.8.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.9.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.10.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.11.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.12.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.13.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.14.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.15.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.16.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.17.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.18.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.19.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.20.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.21.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.22.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.23.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.24.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.25.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.26.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.27.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.28.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.29.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.30.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.31.self_attn.attn': KVCacheTensor(size=524206080)}, groups=[['model.layers.0.self_attn.attn', 'model.layers.1.self_attn.attn', 'model.layers.2.self_attn.attn', 'model.layers.3.self_attn.attn', 'model.layers.4.self_attn.attn', 'model.layers.5.self_attn.attn', 'model.layers.6.self_attn.attn', 'model.layers.7.self_attn.attn', 'model.layers.8.self_attn.attn', 'model.layers.9.self_attn.attn', 'model.layers.10.self_attn.attn', 'model.layers.11.self_attn.attn', 'model.layers.12.self_attn.attn', 'model.layers.13.self_attn.attn', 'model.layers.14.self_attn.attn', 'model.layers.15.self_attn.attn', 'model.layers.16.self_attn.attn', 'model.layers.17.self_attn.attn', 'model.layers.18.self_attn.attn', 'model.layers.19.self_attn.attn', 'model.layers.20.self_attn.attn', 'model.layers.21.self_attn.attn', 'model.layers.22.self_attn.attn', 'model.layers.23.self_attn.attn', 'model.layers.24.self_attn.attn', 'model.layers.25.self_attn.attn', 'model.layers.26.self_attn.attn', 'model.layers.27.self_attn.attn', 'model.layers.28.self_attn.attn', 'model.layers.29.self_attn.attn', 'model.layers.30.self_attn.attn', 'model.layers.31.self_attn.attn']], kv_cache_spec={'model.layers.0.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.1.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.2.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.3.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.4.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.5.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.6.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.7.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.8.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.9.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.10.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.11.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.12.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.13.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.14.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.15.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.16.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.17.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.18.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.19.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.20.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.21.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.22.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.23.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.24.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.25.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.26.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.27.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.28.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.29.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.30.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.31.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False)})
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ============================= model.layers.0.self_attn.attn =============================
[36m(RayWorkerWrapper pid=126665)[0m hosseins: tpu_k_cache.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: get_shard_spec(tpu_k_cache)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: tpu_v_cache.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: get_shard_spec(tpu_v_cache)=None
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:19:59 [parallel_state.py:950] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m WARNING 03-18 16:19:59 [tpu.py:116] Pin memory is not supported on TPU.[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: get_model_loader() load_config.load_format=<LoadFormat.AUTO: 'auto'>[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=126667)[0m hosseins: Attention. init()[32m [repeated 111x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:19:59 [tpu.py:39] Cannot use None backend on TPU.[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:19:59 [tpu.py:42] Using Pallas V1 backend.[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=126667)[0m WARNING 03-18 16:20:00 [topk_topp_sampler.py:46] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=126667)[0m INFO 03-18 16:20:01 [weight_utils.py:258] Using model weights format ['*.safetensors'][32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=126667)[0m hosseins: VocabParallelEmbedding -> weight_loader() [shard_size=32064][32m [repeated 14x across cluster][0m
[36m(RayWorkerWrapper pid=126667)[0m hosseins: DefaultModelLoader -> load_weights() name='layers.9.self_attn.v_proj.weight'[32m [repeated 1116x across cluster][0m
[36m(RayWorkerWrapper pid=126667)[0m INFO 03-18 16:20:04 [linear.py:1248] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)][32m [repeated 248x across cluster][0m
[36m(RayWorkerWrapper pid=126667)[0m INFO 03-18 16:20:04 [linear.py:1249] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)][32m [repeated 248x across cluster][0m
[36m(RayWorkerWrapper pid=126667)[0m INFO 03-18 16:20:04 [linear.py:1253] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 1024])][32m [repeated 992x across cluster][0m
[36m(RayWorkerWrapper pid=126667)[0m hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param.data)=None[32m [repeated 248x across cluster][0m
[36m(RayWorkerWrapper pid=126667)[0m hosseins: RowParallelLinear -> weight_loader() get_shard_spec(param_data.data)=None[32m [repeated 248x across cluster][0m
[36m(RayWorkerWrapper pid=126667)[0m hosseins: DefaultModelLoader -> load_weights() name='norm.weight'[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=126667)[0m hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param_data.data)=None[32m [repeated 747x across cluster][0m
[36m(RayWorkerWrapper pid=126667)[0m INFO 03-18 16:20:04 [linear.py:679] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)][32m [repeated 249x across cluster][0m
[36m(RayWorkerWrapper pid=126667)[0m INFO 03-18 16:20:04 [linear.py:680] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)][32m [repeated 249x across cluster][0m
[36m(RayWorkerWrapper pid=126667)[0m INFO 03-18 16:20:04 [linear.py:684] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([7168, 4096])][32m [repeated 996x across cluster][0m
[36m(RayWorkerWrapper pid=126667)[0m hosseins: MergedColumnParallelLinear -> weight_loader() 3 get_shard_spec(param.data)=None[32m [repeated 249x across cluster][0m
[36m(RayWorkerWrapper pid=126667)[0m INFO 03-18 16:20:04 [linear.py:1114] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)][32m [repeated 369x across cluster][0m
[36m(RayWorkerWrapper pid=126667)[0m INFO 03-18 16:20:04 [linear.py:1115] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)][32m [repeated 369x across cluster][0m
[36m(RayWorkerWrapper pid=126667)[0m INFO 03-18 16:20:04 [linear.py:1119] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([1536, 4096])][32m [repeated 1476x across cluster][0m
[36m(RayWorkerWrapper pid=126667)[0m hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param.data)=None[32m [repeated 369x across cluster][0m
[36m(RayWorkerWrapper pid=126667)[0m hosseins: QKVParallelLinear -> weight_loader() get_shard_spec(param_data.data)=None[32m [repeated 369x across cluster][0m
[36m(RayWorkerWrapper pid=126667)[0m hosseins: DefaultModelLoader -> load_weights() name='embed_tokens.weight'[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=126667)[0m hosseins: load_weights() is completed[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=126667)[0m hosseins: load_weights() name='embed_tokens.weight'[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=126667)[0m hosseins: load_weights() param.shape=torch.Size([4096])[32m [repeated 1158x across cluster][0m
[36m(RayWorkerWrapper pid=126667)[0m hosseins: load_weights() get_shard_spec(param)=None[32m [repeated 582x across cluster][0m
[36m(RayWorkerWrapper pid=126667)[0m hosseins: load_weights() name='norm.weight'[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=126667)[0m INFO 03-18 16:20:04 [loader.py:430] Loading weights took 2.27 seconds[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=126667)[0m hosseins: load_model() model is loaded[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: tpu_k_cache.shape=torch.Size([31995, 16, 2, 128])[32m [repeated 166x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: ModelWrapperV1 -> forward() 2 get_shard_spec(input_ids)=None[32m [repeated 7x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: ModelWrapperV1 -> forward() 2 get_shard_spec(positions)=None[32m [repeated 7x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: LlamaForCasualLM.forward()[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: LlamaModel.forward()[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: LlamaModel.forward() get_pp_group().is_first_rank=True[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: VocabParallelEmbedding.forward()[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: VocabParallelEmbedding.forward() [get_shard_spec(masked_input)=None][32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: UnquantizedEmbeddingMethod -> embedding() [layer.weight.shape=torch.Size([32064, 4096])][32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: UnquantizedEmbeddingMethod -> embedding() [layer.weight.device=device(type='xla', index=0)][32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: VocabParallelEmbedding.forward() [get_shard_spec(output_parallel)=None][32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: VocabParallelEmbedding.forward() [get_shard_spec(output)=None][32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: LlamaModel.forward() 1 get_shard_spec(hidden_states)=None[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: LlamaModel.forward() layer start[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=126667)[0m hosseins: LlamaModel.forward() ============= type(layer)=<class 'vllm.model_executor.models.llama.LlamaDecoderLayer'> =============[32m [repeated 112x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: LlamaModel.forward() Name: self_attn.qkv_proj.weight, Shape: param.shape=torch.Size([1536, 4096])[32m [repeated 112x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None[32m [repeated 672x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: LlamaModel.forward() Name: self_attn.o_proj.weight, Shape: param.shape=torch.Size([4096, 1024])[32m [repeated 112x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: LlamaModel.forward() Name: mlp.gate_up_proj.weight, Shape: param.shape=torch.Size([7168, 4096])[32m [repeated 112x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: LlamaModel.forward() Name: mlp.down_proj.weight, Shape: param.shape=torch.Size([4096, 3584])[32m [repeated 112x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: LlamaModel.forward() Name: input_layernorm.weight, Shape: param.shape=torch.Size([4096])[32m [repeated 112x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: LlamaModel.forward() Name: post_attention_layernorm.weight, Shape: param.shape=torch.Size([4096])[32m [repeated 112x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([512, 4096])[32m [repeated 451x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: LlamaAttention.forward() output.shape=torch.Size([512, 4096])[32m [repeated 563x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: LlamaAttention.forward() hidden_states.device=device(type='xla', index=0)[32m [repeated 112x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None[32m [repeated 338x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: LlamaAttention.forward() self.qkv_proj=QKVParallelLinear(in_features=4096, output_features=1536, bias=False, tp_size=4, gather_output=False)[32m [repeated 112x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: LlamaAttention.forward() Name: weight, Shape: param.shape=torch.Size([1536, 4096])[32m [repeated 112x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: LlamaAttention.forward() get_shard_spec(param)=None[32m [repeated 112x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.QKVParallelLinear'>[32m [repeated 112x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>[32m [repeated 225x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 3584]) get_shard_spec(layer.weight)=None[32m [repeated 451x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([512, 3584]) get_shard_spec(x)=None[32m [repeated 451x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([512, 4096]) get_shard_spec(out)=None[32m [repeated 452x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=None[32m [repeated 226x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: LlamaAttention.forward() qkv.device=device(type='xla', index=0)[32m [repeated 113x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None[32m [repeated 113x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: Attention. forward()[32m [repeated 113x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: Attention -> forward() value.shape=torch.Size([512, 256])[32m [repeated 226x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: Attention -> forward() get_shard_spec(key)=None value.shape=torch.Size([512, 256])[32m [repeated 113x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: Attention -> forward() get_shard_spec(value)=None value.shape=torch.Size([512, 256])[32m [repeated 113x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: PallasAttentionBackendImpl.forward()[32m [repeated 113x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)[32m [repeated 113x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>[32m [repeated 226x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)[32m [repeated 113x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: LlamaAttention.forward() get_shard_spec(residual)=None[32m [repeated 113x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: LlamaMLP.forward() x.shape=torch.Size([512, 4096])[32m [repeated 226x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)[32m [repeated 113x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: LlamaMLP.forward() get_shard_spec(x)=None[32m [repeated 113x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>[32m [repeated 113x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: LlamaModel.forward() layer end[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: LlamaModel.forward() norm start[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: LlamaModel.forward() norm end[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: ModelWrapperV1 -> forward() 2 get_shard_spec(hidden_states)=None[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: ModelWrapperV1 -> compute_logits() logits_indices.shape=torch.Size([128])[32m [repeated 12x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: ModelWrapperV1 -> compute_logits() get_shard_spec(hidden_states)=None[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: ModelWrapperV1 -> compute_logits() get_shard_spec(logits_indices)=None[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: UnquantizedEmbeddingMethod -> apply() [layer.weight.shape=torch.Size([32064, 4096])][32m [repeated 12x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: UnquantizedEmbeddingMethod -> apply() [layer.weight.device=device(type='xla', index=0)][32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: initialize_from_config() kv_cache_config=KVCacheConfig(num_blocks=31995, tensors={'model.layers.0.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.1.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.2.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.3.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.4.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.5.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.6.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.7.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.8.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.9.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.10.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.11.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.12.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.13.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.14.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.15.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.16.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.17.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.18.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.19.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.20.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.21.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.22.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.23.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.24.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.25.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.26.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.27.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.28.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.29.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.30.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.31.self_attn.attn': KVCacheTensor(size=524206080)}, groups=[['model.layers.0.self_attn.attn', 'model.layers.1.self_attn.attn', 'model.layers.2.self_attn.attn', 'model.layers.3.self_attn.attn', 'model.layers.4.self_attn.attn', 'model.layers.5.self_attn.attn', 'model.layers.6.self_attn.attn', 'model.layers.7.self_attn.attn', 'model.layers.8.self_attn.attn', 'model.layers.9.self_attn.attn', 'model.layers.10.self_attn.attn', 'model.layers.11.self_attn.attn', 'model.layers.12.self_attn.attn', 'model.layers.13.self_attn.attn', 'model.layers.14.self_attn.attn', 'model.layers.15.self_attn.attn', 'model.layers.16.self_attn.attn', 'model.layers.17.self_attn.attn', 'model.layers.18.self_attn.attn', 'model.layers.19.self_attn.attn', 'model.layers.20.self_attn.attn', 'model.layers.21.self_attn.attn', 'model.layers.22.self_attn.attn', 'model.layers.23.self_attn.attn', 'model.layers.24.self_attn.attn', 'model.layers.25.self_attn.attn', 'model.layers.26.self_attn.attn', 'model.layers.27.self_attn.attn', 'model.layers.28.self_attn.attn', 'model.layers.29.self_attn.attn', 'model.layers.30.self_attn.attn', 'model.layers.31.self_attn.attn']], kv_cache_spec={'model.layers.0.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.1.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.2.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.3.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.4.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.5.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.6.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.7.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.8.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.9.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.10.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.11.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.12.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.13.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.14.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.15.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.16.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.17.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.18.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.19.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.20.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.21.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.22.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.23.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.24.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.25.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.26.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.27.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.28.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.29.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.30.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.31.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False)})[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: initialize_kv_cache() kv_cache_config=KVCacheConfig(num_blocks=31995, tensors={'model.layers.0.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.1.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.2.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.3.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.4.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.5.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.6.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.7.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.8.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.9.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.10.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.11.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.12.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.13.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.14.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.15.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.16.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.17.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.18.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.19.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.20.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.21.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.22.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.23.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.24.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.25.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.26.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.27.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.28.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.29.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.30.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.31.self_attn.attn': KVCacheTensor(size=524206080)}, groups=[['model.layers.0.self_attn.attn', 'model.layers.1.self_attn.attn', 'model.layers.2.self_attn.attn', 'model.layers.3.self_attn.attn', 'model.layers.4.self_attn.attn', 'model.layers.5.self_attn.attn', 'model.layers.6.self_attn.attn', 'model.layers.7.self_attn.attn', 'model.layers.8.self_attn.attn', 'model.layers.9.self_attn.attn', 'model.layers.10.self_attn.attn', 'model.layers.11.self_attn.attn', 'model.layers.12.self_attn.attn', 'model.layers.13.self_attn.attn', 'model.layers.14.self_attn.attn', 'model.layers.15.self_attn.attn', 'model.layers.16.self_attn.attn', 'model.layers.17.self_attn.attn', 'model.layers.18.self_attn.attn', 'model.layers.19.self_attn.attn', 'model.layers.20.self_attn.attn', 'model.layers.21.self_attn.attn', 'model.layers.22.self_attn.attn', 'model.layers.23.self_attn.attn', 'model.layers.24.self_attn.attn', 'model.layers.25.self_attn.attn', 'model.layers.26.self_attn.attn', 'model.layers.27.self_attn.attn', 'model.layers.28.self_attn.attn', 'model.layers.29.self_attn.attn', 'model.layers.30.self_attn.attn', 'model.layers.31.self_attn.attn']], kv_cache_spec={'model.layers.0.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.1.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.2.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.3.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.4.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.5.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.6.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.7.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.8.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.9.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.10.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.11.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.12.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.13.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.14.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.15.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.16.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.17.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.18.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.19.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.20.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.21.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.22.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.23.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.24.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.25.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.26.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.27.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.28.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.29.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.30.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.31.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False)})[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ============================= model.layers.5.self_attn.attn =============================[32m [repeated 20x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: tpu_k_cache.device=device(type='xla', index=0)[32m [repeated 20x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: get_shard_spec(tpu_k_cache)=None[32m [repeated 20x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: tpu_v_cache.device=device(type='xla', index=0)[32m [repeated 20x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: get_shard_spec(tpu_v_cache)=None[32m [repeated 20x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: tpu_k_cache.shape=torch.Size([31995, 16, 2, 128])[32m [repeated 140x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ============================= model.layers.10.self_attn.attn =============================[32m [repeated 20x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: tpu_k_cache.device=device(type='xla', index=0)[32m [repeated 20x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: get_shard_spec(tpu_k_cache)=None[32m [repeated 20x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: tpu_v_cache.device=device(type='xla', index=0)[32m [repeated 20x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: get_shard_spec(tpu_v_cache)=None[32m [repeated 20x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: tpu_k_cache.shape=torch.Size([31995, 16, 2, 128])[32m [repeated 140x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ============================= model.layers.15.self_attn.attn =============================[32m [repeated 20x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: tpu_k_cache.device=device(type='xla', index=0)[32m [repeated 20x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: get_shard_spec(tpu_k_cache)=None[32m [repeated 20x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: tpu_v_cache.device=device(type='xla', index=0)[32m [repeated 24x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: get_shard_spec(tpu_v_cache)=None[32m [repeated 24x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: tpu_k_cache.shape=torch.Size([31995, 16, 2, 128])[32m [repeated 140x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ============================= model.layers.20.self_attn.attn =============================[32m [repeated 20x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: tpu_k_cache.device=device(type='xla', index=0)[32m [repeated 20x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: get_shard_spec(tpu_k_cache)=None[32m [repeated 20x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: tpu_v_cache.device=device(type='xla', index=0)[32m [repeated 20x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: get_shard_spec(tpu_v_cache)=None[32m [repeated 20x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: tpu_k_cache.shape=torch.Size([31995, 16, 2, 128])[32m [repeated 168x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ============================= model.layers.26.self_attn.attn =============================[32m [repeated 24x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: tpu_k_cache.device=device(type='xla', index=0)[32m [repeated 24x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: get_shard_spec(tpu_k_cache)=None[32m [repeated 24x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: tpu_v_cache.device=device(type='xla', index=0)[32m [repeated 20x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: get_shard_spec(tpu_v_cache)=None[32m [repeated 20x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: tpu_k_cache.shape=torch.Size([31995, 16, 2, 128])[32m [repeated 140x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ============================= model.layers.31.self_attn.attn =============================[32m [repeated 20x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: tpu_k_cache.device=device(type='xla', index=0)[32m [repeated 20x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: get_shard_spec(tpu_k_cache)=None[32m [repeated 20x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: initialize_kv_cache() kv_cache_config=KVCacheConfig(num_blocks=31995, tensors={'model.layers.0.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.1.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.2.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.3.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.4.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.5.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.6.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.7.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.8.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.9.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.10.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.11.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.12.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.13.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.14.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.15.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.16.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.17.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.18.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.19.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.20.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.21.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.22.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.23.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.24.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.25.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.26.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.27.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.28.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.29.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.30.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.31.self_attn.attn': KVCacheTensor(size=524206080)}, groups=[['model.layers.0.self_attn.attn', 'model.layers.1.self_attn.attn', 'model.layers.2.self_attn.attn', 'model.layers.3.self_attn.attn', 'model.layers.4.self_attn.attn', 'model.layers.5.self_attn.attn', 'model.layers.6.self_attn.attn', 'model.layers.7.self_attn.attn', 'model.layers.8.self_attn.attn', 'model.layers.9.self_attn.attn', 'model.layers.10.self_attn.attn', 'model.layers.11.self_attn.attn', 'model.layers.12.self_attn.attn', 'model.layers.13.self_attn.attn', 'model.layers.14.self_attn.attn', 'model.layers.15.self_attn.attn', 'model.layers.16.self_attn.attn', 'model.layers.17.self_attn.attn', 'model.layers.18.self_attn.attn', 'model.layers.19.self_attn.attn', 'model.layers.20.self_attn.attn', 'model.layers.21.self_attn.attn', 'model.layers.22.self_attn.attn', 'model.layers.23.self_attn.attn', 'model.layers.24.self_attn.attn', 'model.layers.25.self_attn.attn', 'model.layers.26.self_attn.attn', 'model.layers.27.self_attn.attn', 'model.layers.28.self_attn.attn', 'model.layers.29.self_attn.attn', 'model.layers.30.self_attn.attn', 'model.layers.31.self_attn.attn']], kv_cache_spec={'model.layers.0.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.1.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.2.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.3.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.4.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.5.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.6.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.7.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.8.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.9.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.10.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.11.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.12.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.13.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.14.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.15.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.16.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.17.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.18.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.19.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.20.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.21.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.22.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.23.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.24.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.25.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.26.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.27.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.28.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.29.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.30.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.31.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False)})
[36m(RayWorkerWrapper pid=126665)[0m INFO 03-18 16:20:36 [tpu_model_runner.py:774] Compiling the model with different input shapes.
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ModelWrapperV1 -> forward() 1 get_shard_spec(input_ids)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ModelWrapperV1 -> forward() 1 get_shard_spec(positions)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaForCasualLM.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_pp_group().is_first_rank=True
[36m(RayWorkerWrapper pid=126665)[0m hosseins: VocabParallelEmbedding.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: VocabParallelEmbedding.forward() [get_shard_spec(masked_input)=None]
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedEmbeddingMethod -> embedding()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedEmbeddingMethod -> embedding() [layer.weight.shape=torch.Size([32064, 4096])]
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedEmbeddingMethod -> embedding() [layer.weight.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126665)[0m hosseins: VocabParallelEmbedding.forward() [get_shard_spec(output_parallel)=None]
[36m(RayWorkerWrapper pid=126665)[0m hosseins: VocabParallelEmbedding.forward() [get_shard_spec(output)=None]
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() 1 get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() layer start
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() ============= type(layer)=<class 'vllm.model_executor.models.llama.LlamaDecoderLayer'> =============
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: self_attn.qkv_proj.weight, Shape: param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: self_attn.o_proj.weight, Shape: param.shape=torch.Size([4096, 1024])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: mlp.gate_up_proj.weight, Shape: param.shape=torch.Size([7168, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: mlp.down_proj.weight, Shape: param.shape=torch.Size([4096, 3584])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: input_layernorm.weight, Shape: param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: post_attention_layernorm.weight, Shape: param.shape=torch.Size([4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() hidden_states.shape=torch.Size([16, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() hidden_states.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() self.qkv_proj=QKVParallelLinear(in_features=4096, output_features=1536, bias=False, tp_size=4, gather_output=False)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() Name: weight, Shape: param.shape=torch.Size([1536, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(param)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.QKVParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([1536, 4096]) get_shard_spec(layer.weight)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([16, 4096]) get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([16, 1536]) get_shard_spec(out)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() qkv.shape=torch.Size([16, 1536])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() qkv.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention. forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() key.shape=torch.Size([16, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() get_shard_spec(key)=None value.shape=torch.Size([16, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() value.shape=torch.Size([16, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() get_shard_spec(value)=None value.shape=torch.Size([16, 256])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: PallasAttentionBackendImpl.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: write_to_kv_cache() key.shape=torch.Size([16, 2, 128])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: write_to_kv_cache() get_shard_spec(key)=None value.shape=torch.Size([16, 2, 128])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: write_to_kv_cache() value.shape=torch.Size([16, 2, 128])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: write_to_kv_cache() get_shard_spec(value)=None value.shape=torch.Size([16, 2, 128])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: write_to_kv_cache() key_cache.shape=torch.Size([31995, 16, 2, 128])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: write_to_kv_cache() get_shard_spec(key_cache)=None key_cache.shape=torch.Size([31995, 16, 2, 128])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: write_to_kv_cache() value_cache.shape=torch.Size([31995, 16, 2, 128])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: write_to_kv_cache() get_shard_spec(value_cache)=None value_cache.shape=torch.Size([31995, 16, 2, 128])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([16])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: write_to_kv_cache() get_shard_spec(slot_mapping)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.shape=torch.Size([16, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.shape=torch.Size([16, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.shape=torch.Size([16, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.shape=torch.Size([16, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.shape=torch.Size([16, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.shape=torch.Size([16, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.shape=torch.Size([16, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.shape=torch.Size([16, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.shape=torch.Size([16, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.shape=torch.Size([16, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.shape=torch.Size([16, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.shape=torch.Size([16, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.shape=torch.Size([16, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() attn_output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() output.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(residual)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.shape=torch.Size([16, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() x.device=device(type='xla', index=0)
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaMLP.forward() get_shard_spec(x)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>
[36m(RayWorkerWrapper pid=126665)[0m hosseins: RowParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.RowParallelLinear'>
[36m(RayWorkerWrapper pid=126653)[0m hosseins: tpu_v_cache.device=device(type='xla', index=0)[32m [repeated 23x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: get_shard_spec(tpu_v_cache)=None[32m [repeated 23x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() layer end
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() norm start
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() norm end
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ModelWrapperV1 -> forward() 2 get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ModelWrapperV1 -> compute_logits() hidden_states.shape=torch.Size([16, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ModelWrapperV1 -> compute_logits() get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ModelWrapperV1 -> compute_logits() logits_indices.shape=torch.Size([64])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ModelWrapperV1 -> compute_logits() get_shard_spec(logits_indices)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedEmbeddingMethod -> apply()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedEmbeddingMethod -> apply() [layer.weight.shape=torch.Size([32064, 4096])]
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedEmbeddingMethod -> apply() [layer.weight.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ModelWrapperV1 -> compute_logits() hidden_states.shape=torch.Size([16, 4096])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ModelWrapperV1 -> compute_logits() get_shard_spec(hidden_states)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ModelWrapperV1 -> compute_logits() logits_indices.shape=torch.Size([128])
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ModelWrapperV1 -> compute_logits() get_shard_spec(logits_indices)=None
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedEmbeddingMethod -> apply()
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedEmbeddingMethod -> apply() [layer.weight.shape=torch.Size([32064, 4096])]
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedEmbeddingMethod -> apply() [layer.weight.device=device(type='xla', index=0)]
[36m(RayWorkerWrapper pid=126665)[0m INFO 03-18 16:20:39 [tpu_model_runner.py:780]   -- num_tokens: 16
[36m(RayWorkerWrapper pid=126653)[0m hosseins: ModelWrapperV1.forward()[32m [repeated 33x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: ============================= model.layers.31.self_attn.attn =============================[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: tpu_k_cache.device=device(type='xla', index=0)[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: get_shard_spec(tpu_k_cache)=None[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: initialize_kv_cache() kv_cache_config=KVCacheConfig(num_blocks=31995, tensors={'model.layers.0.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.1.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.2.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.3.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.4.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.5.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.6.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.7.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.8.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.9.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.10.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.11.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.12.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.13.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.14.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.15.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.16.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.17.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.18.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.19.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.20.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.21.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.22.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.23.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.24.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.25.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.26.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.27.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.28.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.29.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.30.self_attn.attn': KVCacheTensor(size=524206080), 'model.layers.31.self_attn.attn': KVCacheTensor(size=524206080)}, groups=[['model.layers.0.self_attn.attn', 'model.layers.1.self_attn.attn', 'model.layers.2.self_attn.attn', 'model.layers.3.self_attn.attn', 'model.layers.4.self_attn.attn', 'model.layers.5.self_attn.attn', 'model.layers.6.self_attn.attn', 'model.layers.7.self_attn.attn', 'model.layers.8.self_attn.attn', 'model.layers.9.self_attn.attn', 'model.layers.10.self_attn.attn', 'model.layers.11.self_attn.attn', 'model.layers.12.self_attn.attn', 'model.layers.13.self_attn.attn', 'model.layers.14.self_attn.attn', 'model.layers.15.self_attn.attn', 'model.layers.16.self_attn.attn', 'model.layers.17.self_attn.attn', 'model.layers.18.self_attn.attn', 'model.layers.19.self_attn.attn', 'model.layers.20.self_attn.attn', 'model.layers.21.self_attn.attn', 'model.layers.22.self_attn.attn', 'model.layers.23.self_attn.attn', 'model.layers.24.self_attn.attn', 'model.layers.25.self_attn.attn', 'model.layers.26.self_attn.attn', 'model.layers.27.self_attn.attn', 'model.layers.28.self_attn.attn', 'model.layers.29.self_attn.attn', 'model.layers.30.self_attn.attn', 'model.layers.31.self_attn.attn']], kv_cache_spec={'model.layers.0.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.1.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.2.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.3.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.4.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.5.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.6.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.7.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.8.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.9.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.10.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.11.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.12.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.13.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.14.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.15.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.16.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.17.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.18.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.19.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.20.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.21.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.22.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.23.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.24.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.25.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.26.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.27.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.28.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.29.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.30.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False), 'model.layers.31.self_attn.attn': FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, use_mla=False)})[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m INFO 03-18 16:20:36 [tpu_model_runner.py:774] Compiling the model with different input shapes.[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: ModelWrapperV1 -> forward() 1 get_shard_spec(input_ids)=None[32m [repeated 11x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: ModelWrapperV1 -> forward() 1 get_shard_spec(positions)=None[32m [repeated 11x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m 2025-03-18 16:20:42.836226: W torch_xla/csrc/runtime/pjrt_computation_client.cc:668] Failed to deserialize executable: INTERNAL: TfrtTpuExecutable proto deserialization failed while parsing core program!
[36m(RayWorkerWrapper pid=126651)[0m 2025-03-18 16:20:42.853687: W torch_xla/csrc/runtime/pjrt_computation_client.cc:668] Failed to deserialize executable: INTERNAL: TfrtTpuExecutable proto deserialization failed while parsing compiler metadata!
[36m(RayWorkerWrapper pid=126653)[0m hosseins: LlamaForCasualLM.forward()[32m [repeated 7x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: LlamaModel.forward()[32m [repeated 7x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: LlamaModel.forward() get_pp_group().is_first_rank=True[32m [repeated 7x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: VocabParallelEmbedding.forward()[32m [repeated 7x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: VocabParallelEmbedding.forward() [get_shard_spec(masked_input)=None][32m [repeated 7x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: UnquantizedEmbeddingMethod -> embedding() [layer.weight.shape=torch.Size([32064, 4096])][32m [repeated 14x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: UnquantizedEmbeddingMethod -> embedding() [layer.weight.device=device(type='xla', index=0)][32m [repeated 7x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: VocabParallelEmbedding.forward() [get_shard_spec(output_parallel)=None][32m [repeated 7x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: VocabParallelEmbedding.forward() [get_shard_spec(output)=None][32m [repeated 7x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: LlamaModel.forward() 1 get_shard_spec(hidden_states)=None[32m [repeated 7x across cluster][0m
[36m(RayWorkerWrapper pid=126653)[0m hosseins: LlamaModel.forward() layer start[32m [repeated 7x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() ============= type(layer)=<class 'vllm.model_executor.models.llama.LlamaDecoderLayer'> =============[32m [repeated 145x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: self_attn.qkv_proj.weight, Shape: param.shape=torch.Size([1536, 4096])[32m [repeated 145x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() get_shard_spec(param)=None[32m [repeated 868x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: self_attn.o_proj.weight, Shape: param.shape=torch.Size([4096, 1024])[32m [repeated 145x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: mlp.gate_up_proj.weight, Shape: param.shape=torch.Size([7168, 4096])[32m [repeated 145x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: mlp.down_proj.weight, Shape: param.shape=torch.Size([4096, 3584])[32m [repeated 145x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: input_layernorm.weight, Shape: param.shape=torch.Size([4096])[32m [repeated 145x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaModel.forward() Name: post_attention_layernorm.weight, Shape: param.shape=torch.Size([4096])[32m [repeated 144x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaDecoderLayer.forward() hidden_states.shape=torch.Size([32, 4096])[32m [repeated 570x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() output.shape=torch.Size([32, 4096])[32m [repeated 716x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() hidden_states.device=device(type='xla', index=0)[32m [repeated 144x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(hidden_states)=None[32m [repeated 428x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() self.qkv_proj=QKVParallelLinear(in_features=4096, output_features=1536, bias=False, tp_size=4, gather_output=False)[32m [repeated 144x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() Name: weight, Shape: param.shape=torch.Size([1536, 4096])[32m [repeated 144x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(param)=None[32m [repeated 144x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self)=<class 'vllm.model_executor.layers.linear.QKVParallelLinear'>[32m [repeated 144x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() type(self.quant_method)=<class 'vllm.model_executor.layers.linear.UnquantizedLinearMethod'>[32m [repeated 286x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() layer.weight.shape=torch.Size([4096, 3584]) get_shard_spec(layer.weight)=None[32m [repeated 570x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() x.shape=torch.Size([32, 3584]) get_shard_spec(x)=None[32m [repeated 570x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: UnquantizedLinearMethod -> apply() out.shape=torch.Size([32, 4096]) get_shard_spec(out)=None[32m [repeated 570x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: ColumnParallelLinear -> forward() get_shard_spec(output)=None[32m [repeated 286x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() qkv.device=device(type='xla', index=0)[32m [repeated 144x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: LlamaAttention.forward() get_shard_spec(qkv)=None[32m [repeated 144x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention. forward()[32m [repeated 144x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() value.shape=torch.Size([32, 256])[32m [repeated 288x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() get_shard_spec(key)=None value.shape=torch.Size([32, 256])[32m [repeated 144x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: Attention -> forward() get_shard_spec(value)=None value.shape=torch.Size([32, 256])[32m [repeated 144x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: PallasAttentionBackendImpl.forward()[32m [repeated 144x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: write_to_kv_cache() slot_mapping.shape=torch.Size([32])[32m [repeated 720x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: write_to_kv_cache() get_shard_spec(key)=None value.shape=torch.Size([32, 2, 128])[32m [repeated 144x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: write_to_kv_cache() get_shard_spec(value)=None value.shape=torch.Size([32, 2, 128])[32m [repeated 144x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: write_to_kv_cache() get_shard_spec(key_cache)=None key_cache.shape=torch.Size([31995, 16, 2, 128])[32m [repeated 144x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: write_to_kv_cache() get_shard_spec(value_cache)=None value_cache.shape=torch.Size([31995, 16, 2, 128])[32m [repeated 144x across cluster][0m
[36m(RayWorkerWrapper pid=126665)[0m hosseins: write_to_kv_cache() get_shard_spec(slot_mapping)=None[32m [repeated 144x across cluster][0m
ERROR 03-18 16:20:43 [core.py:332] EngineCore hit an exception: Traceback (most recent call last):
ERROR 03-18 16:20:43 [core.py:332]   File "/home/hosseins/vllm-v1/vllm/v1/engine/core.py", line 324, in run_engine_core
ERROR 03-18 16:20:43 [core.py:332]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 03-18 16:20:43 [core.py:332]   File "/home/hosseins/vllm-v1/vllm/v1/engine/core.py", line 279, in __init__
ERROR 03-18 16:20:43 [core.py:332]     super().__init__(vllm_config, executor_class, log_stats)
ERROR 03-18 16:20:43 [core.py:332]   File "/home/hosseins/vllm-v1/vllm/v1/engine/core.py", line 60, in __init__
ERROR 03-18 16:20:43 [core.py:332]     num_gpu_blocks, num_cpu_blocks = self._initialize_kv_caches(
ERROR 03-18 16:20:43 [core.py:332]   File "/home/hosseins/vllm-v1/vllm/v1/engine/core.py", line 125, in _initialize_kv_caches
ERROR 03-18 16:20:43 [core.py:332]     self.model_executor.initialize_from_config(kv_cache_configs)
ERROR 03-18 16:20:43 [core.py:332]   File "/home/hosseins/vllm-v1/vllm/v1/executor/abstract.py", line 63, in initialize_from_config
ERROR 03-18 16:20:43 [core.py:332]     self.collective_rpc("compile_or_warm_up_model")
ERROR 03-18 16:20:43 [core.py:332]   File "/home/hosseins/vllm-v1/vllm/executor/executor_base.py", line 316, in collective_rpc
ERROR 03-18 16:20:43 [core.py:332]     return self._run_workers(method, *args, **(kwargs or {}))
ERROR 03-18 16:20:43 [core.py:332]   File "/home/hosseins/vllm-v1/vllm/executor/ray_distributed_executor.py", line 532, in _run_workers
ERROR 03-18 16:20:43 [core.py:332]     ray_worker_outputs = ray.get(ray_worker_outputs)
ERROR 03-18 16:20:43 [core.py:332]   File "/home/hosseins/miniconda3/envs/vllm-v1/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
ERROR 03-18 16:20:43 [core.py:332]     return fn(*args, **kwargs)
ERROR 03-18 16:20:43 [core.py:332]   File "/home/hosseins/miniconda3/envs/vllm-v1/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
ERROR 03-18 16:20:43 [core.py:332]     return func(*args, **kwargs)
ERROR 03-18 16:20:43 [core.py:332]   File "/home/hosseins/miniconda3/envs/vllm-v1/lib/python3.10/site-packages/ray/_private/worker.py", line 2771, in get
ERROR 03-18 16:20:43 [core.py:332]     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
ERROR 03-18 16:20:43 [core.py:332]   File "/home/hosseins/miniconda3/envs/vllm-v1/lib/python3.10/site-packages/ray/_private/worker.py", line 919, in get_objects
ERROR 03-18 16:20:43 [core.py:332]     raise value.as_instanceof_cause()
ERROR 03-18 16:20:43 [core.py:332] ray.exceptions.RayTaskError(RuntimeError): [36mray::RayWorkerWrapper.execute_method()[39m (pid=126667, ip=10.130.0.78, actor_id=e4b38f09ae5252940d5a08fe01000000, repr=<vllm.executor.ray_utils.RayWorkerWrapper object at 0x710641f8da50>)
ERROR 03-18 16:20:43 [core.py:332]   File "/home/hosseins/vllm-v1/vllm/worker/worker_base.py", line 621, in execute_method
ERROR 03-18 16:20:43 [core.py:332]     raise e
ERROR 03-18 16:20:43 [core.py:332]   File "/home/hosseins/vllm-v1/vllm/worker/worker_base.py", line 612, in execute_method
ERROR 03-18 16:20:43 [core.py:332]     return run_method(self, method, args, kwargs)
ERROR 03-18 16:20:43 [core.py:332]   File "/home/hosseins/vllm-v1/vllm/utils.py", line 2241, in run_method
ERROR 03-18 16:20:43 [core.py:332]     return func(*args, **kwargs)
ERROR 03-18 16:20:43 [core.py:332]   File "/home/hosseins/vllm-v1/vllm/v1/worker/tpu_worker.py", line 188, in compile_or_warm_up_model
ERROR 03-18 16:20:43 [core.py:332]     self.model_runner.capture_model()
ERROR 03-18 16:20:43 [core.py:332]   File "/home/hosseins/vllm-v1/vllm/v1/worker/tpu_model_runner.py", line 782, in capture_model
ERROR 03-18 16:20:43 [core.py:332]     xm.wait_device_ops()
ERROR 03-18 16:20:43 [core.py:332]   File "/home/hosseins/miniconda3/envs/vllm-v1/lib/python3.10/site-packages/torch_xla/core/xla_model.py", line 1129, in wait_device_ops
ERROR 03-18 16:20:43 [core.py:332]     torch_xla._XLAC._xla_wait_device_ops(devices=devices)
ERROR 03-18 16:20:43 [core.py:332] RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: Error allocating device buffer: Attempting to allocate 249.96M. That was not possible. There are 138.76M free.; (0x0x0_HBM0)
ERROR 03-18 16:20:43 [core.py:332] 
INFO 03-18 16:20:43 [ray_distributed_executor.py:127] Shutting down Ray distributed executor. If you see error log from logging.cc regarding SIGTERM received, please ignore because this is the expected termination process in Ray.
CRITICAL 03-18 16:20:43 [core_client.py:260] Got fatal signal from worker processes, shutting down. See stack trace above for root cause issue.
2025-03-18 16:20:43,262	ERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::RayWorkerWrapper.execute_method()[39m (pid=126653, ip=10.130.0.78, actor_id=ee6eb30bf20af0bbe247b09801000000, repr=<vllm.executor.ray_utils.RayWorkerWrapper object at 0x70bebf185a80>)
  File "/home/hosseins/vllm-v1/vllm/worker/worker_base.py", line 621, in execute_method
    raise e
  File "/home/hosseins/vllm-v1/vllm/worker/worker_base.py", line 612, in execute_method
    return run_method(self, method, args, kwargs)
  File "/home/hosseins/vllm-v1/vllm/utils.py", line 2241, in run_method
    return func(*args, **kwargs)
  File "/home/hosseins/vllm-v1/vllm/v1/worker/tpu_worker.py", line 188, in compile_or_warm_up_model
    self.model_runner.capture_model()
  File "/home/hosseins/vllm-v1/vllm/v1/worker/tpu_model_runner.py", line 782, in capture_model
    xm.wait_device_ops()
  File "/home/hosseins/miniconda3/envs/vllm-v1/lib/python3.10/site-packages/torch_xla/core/xla_model.py", line 1129, in wait_device_ops
    torch_xla._XLAC._xla_wait_device_ops(devices=devices)
RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: Error allocating device buffer: Attempting to allocate 249.96M. That was not possible. There are 138.76M free.; (1x0x0_HBM0)
